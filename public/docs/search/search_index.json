{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"NWSL Advanced Analytics Intelligence","text":""},{"location":"#transforming-womens-soccer-analysis-through-data-science","title":"Transforming Women's Soccer Analysis Through Data Science","text":"<p>Welcome to the comprehensive documentation for the NWSL Advanced Analytics Intelligence platform - a sophisticated system that brings sabermetrics-inspired analysis to women's professional soccer.</p> <p>Platform Status</p> <p>13 complete seasons \u2022 99.38% data coverage \u2022 1,563 matches analyzed \u2022 42,572 player records</p>"},{"location":"#what-were-building","title":"What We're Building","text":"<p>The National Women's Soccer League deserves the same analytical sophistication that transformed baseball, basketball, and men's soccer. We're building the most comprehensive NWSL analytics platform ever created, moving far beyond basic statistics to provide meaningful insights that matter.</p>"},{"location":"#our-mission","title":"\ud83c\udfc6 Our Mission","text":"<p>To revolutionize how we understand and analyze women's professional soccer by applying advanced statistical methodologies that reveal the true story behind the game.</p>"},{"location":"#why-this-matters","title":"\ud83e\udde0 Why This Matters","text":"<p>Traditional soccer statistics tell an incomplete story. Goals and assists are important, but they miss the nuanced contributions that truly impact team success. Our platform introduces:</p> <ul> <li>Composite Performance Metrics: Like baseball's move from batting average to OPS, we combine multiple performance dimensions</li> <li>Context-Adjusted Analysis: Understanding performance relative to opposition strength and game situations  </li> <li>Predictive Indicators: Metrics that actually correlate with future success</li> <li>Tactical Intelligence: Automated identification of playing styles and tactical patterns</li> </ul>"},{"location":"#what-makes-us-different","title":"What Makes Us Different","text":""},{"location":"#sabermetrics-for-soccer","title":"\ud83d\udcca Sabermetrics for Soccer","text":"<p>We've adapted proven statistical methodologies from baseball's analytical revolution:</p> Traditional ApproachOur Advanced Approach <ul> <li>Goals scored</li> <li>Assists made</li> <li>Basic counting stats</li> <li>Surface-level analysis</li> </ul> <ul> <li>NWSL Impact Rating (NIR): Composite metric combining attacking, defensive, and progression impacts</li> <li>Context Adjustments: Performance weighted by opposition quality and game state</li> <li>Predictive Indicators: Metrics tested for year-over-year correlation stability</li> <li>Tactical Profiling: Automated playing style identification</li> </ul>"},{"location":"#unmatched-data-foundation","title":"\ud83c\udfaf Unmatched Data Foundation","text":"<p>Our analysis is built on the most comprehensive NWSL database ever assembled:</p> Metric Coverage Seasons Covered 13 complete seasons (2013-2025) Data Completeness 99.38% across all records Player Records 42,572 individual match performances Matches Analyzed 1,563 with detailed statistics Statistical Dimensions 35+ metrics per player per match"},{"location":"#platform-capabilities","title":"Platform Capabilities","text":""},{"location":"#advanced-team-intelligence","title":"\ud83d\udd0d Advanced Team Intelligence","text":"<ul> <li>NIR-based team performance analysis</li> <li>Tactical style identification and evolution</li> <li>Context-adjusted seasonal comparisons</li> <li>Predictive matchup analysis</li> </ul>"},{"location":"#sophisticated-player-analysis","title":"\u26bd Sophisticated Player Analysis","text":"<ul> <li>Individual NWSL Impact Ratings</li> <li>Role optimization recommendations  </li> <li>Performance trajectory analysis</li> <li>Market value indicators</li> </ul>"},{"location":"#league-wide-insights","title":"\ud83d\udcc8 League-Wide Insights","text":"<ul> <li>Historical trend analysis</li> <li>Performance leader identification (beyond just goal scorers)</li> <li>Tactical evolution tracking</li> <li>Competitive balance assessment</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Explore our documentation to understand:</p> <ol> <li>Our Methodology \u2192 How we apply sabermetrics principles to soccer</li> <li>Data Foundation \u2192 The comprehensive database powering our analysis</li> <li>Analytics Engine \u2192 Why we moved beyond basic statistics</li> <li>Platform Architecture \u2192 How our intelligence system works</li> </ol>"},{"location":"#transparency-trust","title":"Transparency &amp; Trust","text":"<p>We believe in complete transparency about our methods, limitations, and assumptions. Every aspect of our analytical approach is documented, from data collection procedures to statistical methodologies.</p> <p>Open Development</p> <p>Our platform development is open source and our methodologies are fully documented. We believe the NWSL community deserves to understand exactly how our analysis works.</p> <p>Built with advanced statistical methodologies inspired by Jim Albert's sabermetrics research and powered by 13 years of comprehensive NWSL data.</p>"},{"location":"database_changes/","title":"Database Schema Updates - Team Season Name Table","text":""},{"location":"database_changes/#changes-made","title":"Changes Made","text":""},{"location":"database_changes/#1-added-match-scores-to-match-table","title":"1. Added match scores to match table","text":"<ul> <li>Added <code>home_goals</code> and <code>away_goals</code> columns computed from <code>match_team_summary</code></li> <li>Populated historical match scores manually (97 missing records)</li> </ul>"},{"location":"database_changes/#2-created-team_season_name-table","title":"2. Created team_season_name table","text":"<ul> <li>Purpose: Single source of truth for team-season relationships</li> <li>Primary Key: <code>tsn_id</code> (tsn_ + 8-character hex)</li> <li>Schema:   <pre><code>CREATE TABLE team_season_name (\n    tsn_id TEXT PRIMARY KEY,\n    season_id INTEGER NOT NULL,\n    team_id TEXT NOT NULL,\n    team_name_season_1 TEXT,\n    team_name_season_2 TEXT,\n    UNIQUE(season_id, team_id),\n    FOREIGN KEY (season_id) REFERENCES season(season_id),\n    FOREIGN KEY (team_id) REFERENCES team(team_id)\n);\n</code></pre></li> <li>Records: 135 team-season combinations (2013-2025)</li> </ul>"},{"location":"database_changes/#3-updated-team-table-column-names","title":"3. Updated team table column names","text":"<ul> <li>Renamed columns to: <code>team_name_1</code>, <code>team_name_2</code>, <code>team_name_3</code>, <code>team_name_4</code></li> </ul>"},{"location":"database_changes/#next-steps","title":"Next Steps","text":"<ul> <li>Create <code>team_record</code> table for season standings</li> <li>Populate historical team records from match data</li> <li>Update MCP server with new standings functionality</li> </ul> <p>Date: $(date)</p>"},{"location":"mcp_server_usage/","title":"NWSL Data MCP Server Usage Guide","text":""},{"location":"mcp_server_usage/#overview","title":"Overview","text":"<p>This MCP server provides comprehensive access to NWSL (National Women's Soccer League) data from 2013-2025, including player statistics, team performance, match results, and league standings.</p>"},{"location":"mcp_server_usage/#available-tools","title":"Available Tools","text":""},{"location":"mcp_server_usage/#1-search_players","title":"1. <code>search_players</code>","text":"<p>Search for NWSL players by name or partial name.</p> <p>Parameters: - <code>search_term</code> (string): Player name or partial name to search for - <code>limit</code> (int, optional): Number of results to return (default: 10, max: 50)</p> <p>Example: <pre><code>Search for \"Morgan\" \u2192 Returns players like Alex Morgan, etc.\n</code></pre></p>"},{"location":"mcp_server_usage/#2-get_player_stats","title":"2. <code>get_player_stats</code>","text":"<p>Get detailed statistics for a specific NWSL player.</p> <p>Parameters: - <code>player_name</code> (string): Name of the player to query - <code>season</code> (int, optional): Season year (2013-2025). If not provided, returns all seasons</p> <p>Example: <pre><code>get_player_stats(\"Alex Morgan\", 2025) \u2192 Returns Alex Morgan's 2025 stats\n</code></pre></p>"},{"location":"mcp_server_usage/#3-get_team_performance","title":"3. <code>get_team_performance</code>","text":"<p>Get team performance data and expected goals statistics.</p> <p>Parameters: - <code>team_name</code> (string): Name of the team (e.g., \"Courage\", \"Angel City\", \"Thorns\") - <code>season</code> (int, optional): Season year. If not provided, returns last 5 seasons</p> <p>Example: <pre><code>get_team_performance(\"Courage\", 2025) \u2192 Returns North Carolina Courage's 2025 performance\n</code></pre></p>"},{"location":"mcp_server_usage/#4-get_league_standings","title":"4. <code>get_league_standings</code>","text":"<p>Get NWSL league standings ranked by points and goal differential.</p> <p>Parameters: - <code>season</code> (int): Season year (2013-2025)</p> <p>Example: <pre><code>get_league_standings(2025) \u2192 Returns 2025 NWSL standings table\n</code></pre></p>"},{"location":"mcp_server_usage/#5-get_season_overview","title":"5. <code>get_season_overview</code>","text":"<p>Get comprehensive overview statistics for a specific NWSL season.</p> <p>Parameters: - <code>season</code> (int): Season year (2013-2025)</p> <p>Example: <pre><code>get_season_overview(2025) \u2192 Returns games played, teams, avg goals, etc.\n</code></pre></p>"},{"location":"mcp_server_usage/#6-get_match_results","title":"6. <code>get_match_results</code>","text":"<p>Get detailed match information and results.</p> <p>Parameters: - <code>team1</code> (string): First team name (required) - <code>team2</code> (string, optional): Second team name for head-to-head matchups - <code>season</code> (int, optional): Season year filter - <code>limit</code> (int, optional): Number of matches to return (default: 10, max: 50)</p> <p>Examples: <pre><code>get_match_results(\"Courage\") \u2192 Returns recent Courage matches\nget_match_results(\"Courage\", \"Thorns\") \u2192 Returns head-to-head matches\nget_match_results(\"Courage\", season=2025) \u2192 Returns Courage's 2025 matches\n</code></pre></p>"},{"location":"mcp_server_usage/#7-get_top_performers","title":"7. <code>get_top_performers</code>","text":"<p>Get top performing players by various statistical categories.</p> <p>Parameters: - <code>season</code> (int): Season year (2013-2025) - <code>category</code> (string): Statistical category - 'goals', 'assists', 'minutes', or 'games' - <code>limit</code> (int, optional): Number of players to return (default: 10, max: 50)</p> <p>Examples: <pre><code>get_top_performers(2025, \"goals\") \u2192 Returns top goal scorers in 2025\nget_top_performers(2025, \"assists\", 15) \u2192 Returns top 15 assist leaders\n</code></pre></p>"},{"location":"mcp_server_usage/#8-get_team_roster","title":"8. <code>get_team_roster</code>","text":"<p>Get current roster for a specific team and season.</p> <p>Parameters: - <code>team_name</code> (string): Name of the team - <code>season</code> (int): Season year (2013-2025)</p> <p>Example: <pre><code>get_team_roster(\"Courage\", 2025) \u2192 Returns North Carolina Courage's 2025 roster\n</code></pre></p>"},{"location":"mcp_server_usage/#9-list_teams","title":"9. <code>list_teams</code>","text":"<p>List all teams that participated in a given season.</p> <p>Parameters: - <code>season</code> (int, optional): Season year. If not provided, shows current season (2025)</p> <p>Example: <pre><code>list_teams(2025) \u2192 Returns all teams in 2025 season\n</code></pre></p>"},{"location":"mcp_server_usage/#10-analyze_season-new","title":"10. <code>analyze_season</code> \u2b50 NEW","text":"<p>Comprehensive season analysis with narrative insights and context.</p> <p>Parameters: - <code>season</code> (int): Season year (2013-2025)</p> <p>Example: <pre><code>analyze_season(2025) \u2192 Rich analysis: \"\ud83d\udcca 2025 NWSL Season Analysis... Current dominated with 33 points...\"\n</code></pre></p>"},{"location":"mcp_server_usage/#11-compare_seasons-new","title":"11. <code>compare_seasons</code> \u2b50 NEW","text":"<p>Compare two NWSL seasons with key differences highlighted.</p> <p>Parameters: - <code>season1</code> (int): First season year (2013-2025) - <code>season2</code> (int): Second season year (2013-2025)</p> <p>Example: <pre><code>compare_seasons(2024, 2025) \u2192 \"\u2696\ufe0f Season Comparison: League growth, scoring trends, competitiveness...\"\n</code></pre></p>"},{"location":"mcp_server_usage/#12-get_data_status-new","title":"12. <code>get_data_status</code> \u2b50 NEW","text":"<p>Show what NWSL data is available and suggest optimal queries.</p> <p>Returns: Overview of data availability and recommended queries</p> <p>Example: <pre><code>get_data_status() \u2192 \"\ud83d\udccb NWSL Data Availability Status... Current season has 91 games...\"\n</code></pre></p>"},{"location":"mcp_server_usage/#team-name-reference","title":"Team Name Reference","text":"<p>For 2025 season, use these team names: - Angel City - Bay FC - Courage (North Carolina Courage) - Current (Kansas City Current) - Dash (Houston Dash) - Gotham FC (NJ/NY Gotham FC) - Louisville (Racing Louisville FC) - Pride (Orlando Pride) - Reign (Seattle Reign FC) - Royals (Utah Royals) - Spirit (Washington Spirit) - Stars (San Diego Wave FC) - Thorns (Portland Thorns FC) - Wave (San Diego Wave FC)</p>"},{"location":"mcp_server_usage/#usage-with-claude-desktop","title":"Usage with Claude Desktop","text":"<p>Add this server to your <code>claude_desktop_config.json</code>:</p> <pre><code>{\n  \"mcpServers\": {\n    \"nwsl-data\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"src.server\"]\n    }\n  }\n}\n</code></pre>"},{"location":"mcp_server_usage/#usage-with-openai-responses-api","title":"Usage with OpenAI Responses API","text":"<pre><code>from openai import OpenAI\n\nclient = OpenAI()\n\nresp = client.responses.create(\n    model=\"gpt-4.1\",\n    tools=[{\n        \"type\": \"mcp\",\n        \"server_label\": \"nwsl-data\", \n        \"server_url\": \"https://your-server-url.com/mcp\",\n        \"require_approval\": \"never\"\n    }],\n    input=\"What are the 2025 NWSL league standings?\"\n)\n</code></pre>"},{"location":"mcp_server_usage/#performance-features","title":"Performance Features","text":"<ul> <li>Caching: Season overviews and league standings are cached for better performance</li> <li>Input Validation: All inputs are validated with helpful error messages</li> <li>Team Name Matching: Intelligent team name matching supports various formats</li> <li>Error Handling: Comprehensive error handling with meaningful messages</li> <li>Live Calculations: Standings calculated from match data when needed</li> <li>Intelligent Fallbacks: Graceful handling when data is missing</li> </ul>"},{"location":"mcp_server_usage/#data-coverage","title":"Data Coverage","text":"<ul> <li>Seasons: 2013-2025 (complete)</li> <li>Teams: All NWSL teams across all seasons</li> <li>Statistics: Goals, assists, minutes played, expected goals (xG), team performance</li> <li>Match Data: Complete match results, scores, expected goals</li> </ul>"},{"location":"mcp_server_usage/#common-usage-patterns","title":"Common Usage Patterns","text":""},{"location":"mcp_server_usage/#start-with-analytical-tools-recommended","title":"\ud83d\udd25 Start with Analytical Tools (Recommended)","text":"<ol> <li>Season Overview: <code>analyze_season(2025)</code> - Get narrative insights and context</li> <li>Compare Seasons: <code>compare_seasons(2024, 2025)</code> - Understand trends and changes</li> <li>Check Data: <code>get_data_status()</code> - Know what queries work best</li> </ol>"},{"location":"mcp_server_usage/#traditional-deep-dives","title":"\ud83d\udcca Traditional Deep Dives","text":"<ol> <li>Team Analysis: <code>get_team_performance(\"Courage\", 2025)</code> \u2192 <code>get_match_results(\"Courage\", season=2025)</code></li> <li>Player Research: <code>search_players(\"Morgan\")</code> \u2192 <code>get_player_stats(\"Alex Morgan\", 2025)</code></li> <li>League Context: <code>get_league_standings(2025)</code> \u2192 <code>get_top_performers(2025, \"goals\")</code></li> </ol>"},{"location":"mcp_server_usage/#conversational-queries","title":"\ud83d\udcac Conversational Queries","text":"<p>The analytical tools are designed for natural conversation: - \"Tell me about the 2025 season\" \u2192 <code>analyze_season(2025)</code> - \"How does 2025 compare to 2024?\" \u2192 <code>compare_seasons(2024, 2025)</code> - \"What data do you have?\" \u2192 <code>get_data_status()</code></p>"},{"location":"mcp_server_usage/#error-messages-fallbacks","title":"Error Messages &amp; Fallbacks","text":"<p>The server provides helpful error messages for: - Invalid season years (must be 2013-2025) - Unknown team names (suggests alternatives) - Invalid limits (must be 1-50) - Missing data for specific queries</p> <p>Smart Fallbacks: - When pre-calculated standings are missing, calculates from match data - When player data is unavailable, suggests team-level queries - When queries fail, provides alternative suggestions - Always includes follow-up query recommendations</p>"},{"location":"mcp_server_usage/#support","title":"Support","text":"<p>For issues or questions about the NWSL Data MCP Server, check the database integrity and ensure all required data files are present.</p>"},{"location":"schema/","title":"Schema","text":"<pre><code>---\nconfig:\n    layout: elk\n    properties:\n        nodePlacementStrategy: \"LINEAR_SEGMENTS\"\n---\n\nerDiagram\n%% === core dimensions ===\n\n    Season {\n        int     season_id   PK\n        int    season_year\n        text    league_name\n    }\n\n    Player {\n        int    player_id PK\n        string player_name\n        date   dob\n        string nationality\n        string  preferred_foot\n        string  pos\n    }\n\n    Team {\n        int    team_id         PK\n        string team_name\n        string team_name_short\n        string team_name_alias\n        string  city\n    }\n\n    Match {\n        int      match_id      PK\n        int      season_id     FK\n        date     match_date\n        time    match_time\n        int      home_team_id  FK\n        int      away_team_id  FK\n\n        tinyint home_goals\n        tinyint away_goals\n        float   home_xg\n        float   away_xg\n\n        int      attendance\n        string   venue\n        string  referee\n        float   temperature\n    }\n\n%% === match participation ===\n\n    MatchTeam {\n        int match_team_id PK\n        int match_id      FK\n        int team_id       FK\n        string formation\n    }\n\n    Lineup {\n        int lineup_id PK\n        int match_team_id FK \n    }\n\n    LineupPlayer {\n        int lineup_player_id PK\n        int lineup_id  FK\n        int player_id  FK\n        int shirt_no\n        string position\n    }\n\n%% === per-match stats ===\n\n    PlayerMatchStats {\n        int player_match_stats_id PK\n        int match_id   FK\n        int player_id  FK\n        int team_id    FK\n    }\n\n    TeamMatchStats {\n        int team_match_stats_id PK\n        int match_id FK\n        int team_id  FK\n    }\n\n    GoalkeeperStats {\n        int goalkeeper_stats_id PK\n        int match_id FK\n        int player_id FK\n    }\n\n%% === events &amp; shots ===\n\n    Event {\n        int  event_id PK\n        int  match_id FK\n        int  minute\n        string event_type          \n        int  primary_player_id FK\n        int  secondary_player_id FK\n    }\n\n    Shot {\n        int  shot_id PK\n        int  match_id FK\n        int  team_id  FK\n        int  player_id FK\n        tinyint minute\n        float xG\n        string outcome\n    }\n\n%% === bridge table for shot-creating actions ===\n\n    ShotSCA {\n        int  shot_sca_id PK\n        int  shot_id     FK\n        int  contributor_player_id FK\n        string sca_type         \n    }\n\n    %% === relationships ===\n    Season ||--o{ Match : has\n\n    Player ||--o{ LineupPlayer       : appears_in\n    Player ||--o{ PlayerMatchStats   : has_stats\n    Player ||--o{ GoalkeeperStats    : keeper_stats\n    Player ||--o{ Event              : involved_in\n    Player ||--o{ Shot               : takes\n\n    Team   ||--o{ MatchTeam          : participates\n    Team   ||--o{ TeamMatchStats     : team_stats\n    Team   ||--o{ Shot               : attempts\n\n    Match  ||--o{ MatchTeam          : comprises\n    Match  ||--o{ PlayerMatchStats   : collects\n    Match  ||--o{ TeamMatchStats     : collects\n    Match  ||--o{ GoalkeeperStats    : collects\n    Match  ||--o{ Event              : logs\n    Match  ||--o{ Shot               : includes\n\n    MatchTeam ||--|| Lineup          : produces\n    Lineup ||--o{ LineupPlayer       : selects\n    Shot ||--o{ ShotSCA              : \"has SCA\"\n    Player ||--o{ ShotSCA            : \"creates SCA\"\n</code></pre>"},{"location":"scraping/","title":"Scraping Fbref \u2014 Creating a Pipeline","text":"<p>Henrik Schj\u00f8th</p> <p>10 min read \u00b7 Feb 15, 2025</p> <p></p> <p>In this guide, we will scrape match data from Fbref and build a full web scraping pipeline using BeautifulSoup and Selenium. We\u2019ll start by scraping a single table and gradually scale up to scraping all teams and two seasons of match data using a combination of BeautifulSoup and Selenium. Below is the final result.</p> <p></p>"},{"location":"scraping/#why-is-web-scraping-useful","title":"Why is web scraping useful?","text":"<p>When I discovered web scraping, I thought it was only for semi\u2011professional hackers. I was wrong, and now I can do it too! The process of learning has included a lot of trial and error and a lot of time spent on something that I\u2019ve since discovered can be done much easier. Web scraping allow us to quickly retrieve specific data, for any project we have in mind. That data can be anything, from biological science to Oscar\u2011winners. In football analytics we often collect different data to analyze teams, players or matches. We can get market values from Transfermarkt while Sofascore, Fotmob and Fbref have match\u2011data and aggregated event data. Here I\u2019ll share strategies that can automate the process and gets the tables you are searching for. There are certainly more useful tips that can be added, feel free to let me know! Let\u2019s jump into it!</p>"},{"location":"scraping/#get-one-table-using-beautifulsoup","title":"Get one table \u2014Using BeautifulSoup","text":"<p>Lets break it down and start with extracting one table from one team page.</p> <p></p> <p>First, we need to determine which information we want to extract, specifically which table. The aim for this project is retrieving match results, xG, xGA for each gameweek.</p> <p></p> <p>Table with match data</p> <pre><code>import requests\nimport pandas as pd\nimport warnings\n# Hide FutureWarnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# URL for Leeds\nteam_url = \"https://fbref.com/en/squads/5bfb9659/Leeds-United-Stats\"\n\n# Sendeing HTTP GET-request to webpage\ndata = requests.get(team_url)\n\n# check for success response (statuscode 200)\nif response.status_code == 200:\n    print(\"Successfully retrieved page\")\nelse:\n    print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n</code></pre>"},{"location":"scraping/#breakdown-of-code","title":"Breakdown of code","text":"<ul> <li>Importing libraries</li> <li>Team-url (I used Leeds)</li> <li>Sending request to page</li> <li>Check for response from webpage</li> </ul> <pre><code>from bs4 import BeautifulSoup\n# Create a BeautifulSoup-objekt for navigating in HTML\nsoup = BeautifulSoup(data.text, \"html.parser\")\n\n# Print first 500 characters to get an overview\nprint(soup.prettify()[:500])\n</code></pre> <ul> <li>Using BeautifulSoup to create an object, soup</li> <li>Printing first 500 charachters to get an overview</li> </ul> <p>Now we have to find the table int the HTML. We can use inspect to find the table we wuld like to get. In this case it is the scores and figures-table. Right click and inspect allow to see how this look like in HTML.</p> <p></p> <p>We can use the select function in top corner to find the different elements in the HTML-code.</p> <p></p> <p></p> <p>We can hover over to find the table. Notice the class and id. This is the information we need to get the table. Both class and id can be used to find the table. In this case I have used id.</p> <pre><code># finding all tables in page\ntables = soup.find_all(\"table\") # finding table-elements\n# print all ids for each table\nfor table in tables:\n    print(table.get(\"id\")) #print all ids\n\nstats_standard_10\nmatchlogs_for\nstats_keeper_10\nstats_keeper_adv_10\nstats_shooting_10\nstats_passing_10\nstats_passing_types_10\nstats_gca_10\nstats_defense_10\nstats_possession_10\nstats_playing_time_10\nstats_misc_10\nresults2024-2025101_overall\nresults2024-2025101_home_away\n</code></pre>"},{"location":"scraping/#breakdown","title":"Breakdown","text":"<ul> <li>Using soup-method -&gt; find_all-&gt; table elements</li> <li>Get all the table \u201cid\u201d</li> <li>Printing all the ids</li> </ul> <p>The table we want have \u201cmatchlogs_for\u201d as id. We can use a method \u201cselect\u201d from soup, which is called a CSS-selector. It selects elements which match the description, in this case the \u201cmatchlogs_for\u201d as id.</p> <pre><code>table= soup.select(\"#matchlogs_for\") #using '#' to indicate id we want \ntable= table[0] #soup creates lists, so get first element\ndf=pd.read_html(str(table))[0]\ndf.head()\n\n#another strategy using select\n#table_two=soup.select(\"table#matchlogs_for\") #finding tag(table) and id(\"matchlogs_for\")\n#table_two=table_two[0]\n#df=pd.read_html(str(table_two))[0]\n</code></pre>"},{"location":"scraping/#breakdown_1","title":"Breakdown","text":"<ul> <li>We use soup.select to find the table by its ID (matchlogs_for). Select() returns a list</li> <li>We select first element from list</li> <li>Convert element into a Pandas DataFrame using pd.read_html()</li> <li>Printing first five rows</li> </ul> <pre><code>df.columns \nIndex(['Date', 'Time', 'Comp', 'Round', 'Day', 'Venue', 'Result', 'GF', 'GA', 'Opponent', 'xG', 'xGA', 'Poss', 'Attendance', 'Captain', 'Formation', 'Opp Formation', 'Referee', 'Match Report', 'Notes'],\n    dtype='object')\n</code></pre> <p>Here is columns from the dataframe with statistics like results and xG. Perfect for a machine learning prediction project!</p> <p>In this example the table was already present in the HTML source, and BeautifulSoup and requests made it easy to retrieve the table quickly, without launching a browser. This is an effective and fast strategy. BeautifulSoup is great for static HTML, but if the page loads content dynamically, we need Selenium. Unfortunately many modern websites dynamically load content using JavaScript, meaning some elements (like tables, buttons, or text) are not present in the initial HTML source when you request the page using requests or BeautifulSoup. Instead, the page executes JavaScript to fetch and display the content after the page has initially loaded. Luckily Selenium can solve this problem. Here is an example of Selenium getting the same table as above.</p>"},{"location":"scraping/#get-one-table-using-selenium","title":"Get One table \u2014 Using Selenium","text":"<pre><code>from selenium import webdriver\nfrom selenium.webdriver.chrome.service import Service as ChromeService\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nimport pandas as pd\n\n# Setup for Selenium\nchrome_options = Options()\nchrome_options.add_argument(\"--headless\")  \nchrome_options.add_argument(\"--disable-gpu\")\nservice = ChromeService(executable_path=r\"C:\\Users\\henri\\Documents\\Chromedriver\\chromedriver-win64\\chromedriver.exe\") # add path to driver\ndriver = webdriver.Chrome(service=service, options=chrome_options)\n\n\nteam_url = \"https://fbref.com/en/squads/5bfb9659/Leeds-United-Stats\" # Add team url\n\n# load page\ndriver.get(team_url)\n\ntry:\n    # wait for page to load \n    wait = WebDriverWait(driver, 10)  # waiting max 10sec, can be adjusted\n    table = wait.until(EC.presence_of_element_located((By.ID, \"matchlogs_for\")))\n\n    html_table = table.get_attribute('outerHTML')\n\n    # Convert HTML to pandas DataFrame\n    matches = pd.read_html(html_table)[0]\n    print(matches.head())  \nexcept Exception as e:\n    print(f\"Error: {e}\")\nfinally:\n    driver.quit()  \n</code></pre>"},{"location":"scraping/#breakdown_2","title":"Breakdown","text":"<ul> <li>Starts Selenium in headless mode (does not open up a browser window).</li> <li>Loads the webpage for Leeds United\u2019s statistics.</li> <li>Waits for a specific table (\u201cmatchlogs for\u201d) to appear.</li> <li>Extracts the table\u2019s HTML code.</li> <li>Converts it into a Pandas DataFrame.</li> <li>Handles errors and closes the browser.</li> </ul>"},{"location":"scraping/#why-not-only-use-selenium","title":"Why not only use Selenium?","text":"<ul> <li>Selenium is slow \u2014 it loads the whole page, including JavaScript, CSS and images</li> <li>Selenium using a real browser \u2014 could be easier to detect and blocked</li> </ul> <p>We have doubled our repertoire! For doing that we need to be able to navigate from leaguepage to teampage. Now that we\u2019ve successfully extracted data from one team, let\u2019s scale up our pipeline to scrape data from all teams in the league. To do this, we first need to retrieve the links to each team\u2019s page.</p>"},{"location":"scraping/#navigate-from-leaguepage-to-teampage","title":"Navigate From Leaguepage to Teampage","text":"<p>We start from the main leaguepage. We want to access the team links in this table.</p> <pre><code>standings_url = \"https://fbref.com/en/comps/10/Championship-Stats\" #league-url\ndata = requests.get(standings_url) \nsoup = BeautifulSoup(data.text) \nstandings_table = soup.select('table.stats_table')[0]\n</code></pre> <p>The first steps is same as we started with in teampage. Using request soup.select to get the table.</p> <p></p> <p>We want to find the team links. Inspect give us information about the different team links. In HTML, the <code>&lt;a&gt;</code> tag (anchor tag) is used to create a hyperlink. The href attribute (Hypertext Reference) specifies the URL that the link points to.</p> <p></p> <pre><code>links = standings_table.find_all('a') # finding the \"a\"-tags\nlinks # print to check \nlinks = [l.get(\"href\") for l in links] \nlinks = [l for l in links if '/squads/' in l] # filtering for squad links\nlinks #print to check again\nteam_urls = [f\"https://fbref.com{l}\" for l in links] # adding text to complete links \n</code></pre>"},{"location":"scraping/#breakdown_3","title":"Breakdown","text":"<ul> <li>Using soups find_all method -&gt; looking for elements with an \u201ca\u201d tag.</li> <li>Printing links</li> </ul> <ul> <li>Using list-comprehension to get all links, which is the href \u2014 property</li> <li>Filtering all links to get only those containing squads</li> <li>Printing again to check</li> </ul> <ul> <li>Adding the start of the link containing the domain ( Links are missing the beginning)</li> </ul> <p>Now we have the full links to all teams in the league and can access them by using team_urls[\u201clinknumber\u201d]. We can access the table from any team in the league, but doing that manually will take hours.</p> <pre><code>team_name= team_urls[0].split(\"/\")[-1].replace(\"-Stats\", \"\").replace(\"-\", \" \") # finding team name\ndata = requests.get(team_urls[0]) #checking data from first teamurl (which is leeds, same url as code in beginning)\n# Create a BeautifulSoup-objekt for navigating in HTML\nsoup = BeautifulSoup(data.text, \"html.parser\") # now we are back to where we began\ntable= soup.select(\"#matchlogs_for\") #using '#' to indicate id we want \ntable= table[0] #soup creates lists, so get first element\ndf=pd.read_html(str(table))[0]\n</code></pre> <p>Now that we have successfully retrieved team links and extracted match data from one team, let\u2019s scale up and scrape all teams in the league.</p>"},{"location":"scraping/#creating-a-full-webscraper","title":"Creating a full webscraper","text":"<p>To create a full webscraper we need a function that does all the above, looping through all teams and collecting tables. After trying this with BeautifulSoup, i discovered some match-tables were missing. Therefore I combined BeautifulSoup and Selenium to get all matches. This code first search for tables with BeautifulSoup. If the page is loaded dynamically, we use Selenium for that particular table. It loops through all team links and we can put in a list with years which make it possible to get data from multiple seasons for one league.</p> <pre><code># # add the years you want to scrape. \nyears = list(range(2024, 2022, -1)) # Starts with 2024 season, ends with 2023 season\nall_matches = [] #create empty list for dataframes\nstandings_url = \"https://fbref.com/en/comps/9/Premier-League-Stats\" # league url\nheaders = {\n    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n} #headers make the request look like it come from a user-agent. Some sites demand headers\n</code></pre> <pre><code># combination of beautiful soup and selenium\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.service import Service as ChromeService\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport requests\nimport time\n\n# setup for Selenium\nchrome_options = Options()\nchrome_options.add_argument(\"--headless\")  \nchrome_options.add_argument(\"--disable-gpu\")\nservice = ChromeService(executable_path=r\"C:\\Users\\henri\\Documents\\Chromedriver\\chromedriver-win64\\chromedriver.exe\") # add path to driver here\ndriver = webdriver.Chrome(service=service, options=chrome_options)\n\nfor year in years:\n    data = requests.get(standings_url,headers=headers)\n    soup = BeautifulSoup(data.text, 'html.parser')\n    standings_table = soup.select('table.stats_table')[0]\n    links = [l.get(\"href\") for l in standings_table.find_all('a')]\n    links = [l for l in links if '/squads/' in l]\n    team_urls = [f\"https://fbref.com{l}\" for l in links]\n\n    # Finding previous season\n    previous_season = soup.select(\"a.prev\")[0].get(\"href\")\n    standings_url = f\"https://fbref.com{previous_season}\"\n\n    for team_url in team_urls:\n        team_name = team_url.split(\"/\")[-1].replace(\"-Stats\", \"\").replace(\"-\", \" \")\n        print(f'Scraping {team_name} for season {year}')\n\n        # Try first using BeautifulSoup\n        data = requests.get(team_url,headers=headers)\n        soup = BeautifulSoup(data.text, 'html.parser')\n        table = soup.find('table', id='matchlogs_for')\n\n        if table:  # Checking if table exists in HTML\n            matches = pd.read_html(str(table))[0]\n            matches[\"Season\"] = year\n            matches[\"Team\"] = team_name\n            all_matches.append(matches)\n        else:  # If table does not exist in HTML, using Selenium\n            print(f\"Using Selenium for {team_name}\") \n            driver.get(team_url)\n            try:\n                # adding waiting time to load page\n                wait = WebDriverWait(driver, 10)  # Set max waiting time to 10 sec\n                selenium_table = wait.until(EC.presence_of_element_located((By.ID, \"matchlogs_for\")))\n                html_table = selenium_table.get_attribute('outerHTML')\n\n                # Convert table to pandas DataFrame\n                matches = pd.read_html(html_table)[0]\n                matches[\"Season\"] = year\n                matches[\"Team\"] = team_name\n                all_matches.append(matches)\n            except Exception as e:\n                print(f\"Failed to scrape {team_name} with Selenium: {e}\")\n\n        # Wait to avoid blocking\n        time.sleep(6)\n\n# Combine all dataframes \ndriver.quit()  # Close Selenium-driver\nall_seasons = pd.concat(all_matches, ignore_index=True)\nall_seasons.sort_values(by=\"Date\")\n\n# Check result\nprint(all_seasons.head())\n</code></pre> <ol> <li>Sets up Selenium (Headless Chrome)</li> <li>Loops through each season <code>(years)</code></li> <li>Fetches the league table <code>(standings_table)</code> using BeautifulSoup</li> <li>Collects links to all teams <code>(</code>team_urls)`</li> <li>Finds the link to the previous season for the next iteration</li> <li>Loops through each team <code>(team_urls)</code></li> <li>Fetches the team\u2019s statistics page</li> <li>Attempts to find the <code>matchlogs_for</code> table using BeautifulSoup first</li> <li>If BeautifulSoup fails, uses Selenium to retrieve it dynamically</li> <li>Converts the table into a Pandas DataFrame and adds it to <code>all_matches</code></li> <li>Combines all season data into a single DataFrame <code>(all_seasons)</code></li> <li>Closes Selenium <code>(driver.quit())</code> to free up system resources</li> <li>Displays the first few rows of <code>all_seasons</code></li> </ol> <p>\ud83d\ude80 This makes the scraping process both efficient and reliable!</p> <p></p>"},{"location":"scraping/#review-the-dataframe","title":"Review the DataFrame","text":"<p>Sometimes the results is different to what we expected. That makes verifying and cleaning the data important before using it. Filtering for only matches played and check for number of matches.</p> <p></p> <pre><code>len(all_seasons) #checking number of rows\n# removing other competitions, keep only championsshipp games\nall_seasons = all_seasons.loc[all_seasons['Comp'] == 'Championship']\ndf_filtered= all_seasons[all_seasons[\"Date\"]&lt;\"2025-02-14\"] #filtering for today, keeping only matches played\n</code></pre> <pre><code># finding real number of games - checking that we got all games (some teams had played 31 and some 32 matches)\nnumber_matches=31*8 +32*16 \nnumber_matches_df= len(df_filtered) #check number of matches\nprint(f'number of games played: {number_matches}')\nprint(f'number of matches in DataFrame: {number_matches_df_filtered}')\n</code></pre> <p></p>"},{"location":"scraping/#to-summarize","title":"To summarize","text":"<p>We have now built a full web scraping pipeline that extracts match data for multiple seasons using a combination of BeautifulSoup and Selenium. By prioritizing BeautifulSoup when possible, we keep the process efficient, and by using Selenium only when necessary, we ensure we get all data even when JavaScript is involved. The data is now ready to be used, for example in a machine learning project. For this article I have been inspired from various ressources. There are definately more techniques and advanced tips and tricks which can be added, feel free to let me know ! Good luck with scraping!</p>"},{"location":"scraping/#references","title":"References","text":"<p>https://medium.com/@conalhenderson/how-to-build-a-custom-web-scraper-to-extract-premier-league-player-market-data-3b8e5378cca2</p> <p>https://ricardoheredia94.medium.com/scraping-fbref-for-data-driven-scouting-and-enhanced-player-profiling-464acad83270</p> <p>https://github.com/dataquestio/project-walkthroughs/tree/master/football_matches</p>"},{"location":"system_design/","title":"System design","text":"<pre><code>\ngraph TB\n      User[\ud83d\udc64 User] --&gt; NextJS[\ud83c\udf10 NextJS App&lt;br/&gt;platform.nwsldata.com]\n\n      NextJS --&gt; Choice{Where should&lt;br/&gt;orchestration live?}\n\n      Choice --&gt; A[Option A: MCP Orchestration]\n      Choice --&gt; B[Option B: NextJS Orchestration]\n      Choice --&gt; C[Option C: Hybrid Agents]\n\n      A --&gt; MCP_Complex[MCP Server&lt;br/&gt;\ud83d\udcca Tools + \ud83e\udde0 Intelligence]\n      MCP_Complex --&gt; DB[(\ud83d\uddc4\ufe0f NWSL Database)]\n\n      B --&gt; NextJS_Complex[NextJS App&lt;br/&gt;\ud83c\udf10 UI + \ud83e\udde0 Intelligence]\n      NextJS_Complex --&gt; MCP_Simple[MCP Server&lt;br/&gt;\ud83d\udcca Simple Tools Only]\n      MCP_Simple --&gt; DB\n\n      C --&gt; Agent[\ud83e\udd16 OpenAI Agent&lt;br/&gt;\ud83e\udde0 Orchestration Logic]\n      Agent --&gt; MCP_Tools[MCP Server&lt;br/&gt;\ud83d\udcca Focused Tools]\n      MCP_Tools --&gt; DB\n</code></pre>"},{"location":"system_prompt/","title":"NWSL Data Assistant System Prompt","text":"<p>You are an expert NWSL (National Women's Soccer League) data analyst with access to a comprehensive database spanning 2013-2025 seasons. Your role is to provide accurate, insightful analysis of NWSL teams, players, matches, and trends using the available data.</p>"},{"location":"system_prompt/#database-overview","title":"Database Overview","text":"<ul> <li>Coverage: Complete NWSL data from 2013-2025 (13 seasons)</li> <li>Total: 1,563 matches, 17 teams, 946 players</li> <li>Current Season: 2025 (most recent data available)</li> <li>Data Quality: </li> <li>Team-level statistics: Excellent (59% coverage with detailed stats)  </li> <li>Player-level statistics: Limited but growing</li> <li>Most comprehensive data: 2019-2025 seasons</li> </ul>"},{"location":"system_prompt/#available-data-categories","title":"Available Data Categories","text":""},{"location":"system_prompt/#match-level-data","title":"Match-Level Data","text":"<ul> <li>Team Performance: Goals, possession %, passing accuracy, shots on target %, saves %</li> <li>Detailed Team Stats: Fouls, corners, crosses, touches, tackles, interceptions, aerials won, clearances, offsides, goal kicks, throw-ins, long balls</li> <li>Match Context: Dates, venues, attendance, officials, formations</li> </ul>"},{"location":"system_prompt/#team-level-analysis","title":"Team-Level Analysis","text":"<ul> <li>Season Records: Wins/losses/draws, goals for/against, average performance metrics</li> <li>Head-to-Head Records: Historical matchups between any two teams</li> <li>Form Analysis: Recent performance trends, best/worst performances</li> <li>League Standings: Season-by-season team rankings and achievements</li> </ul>"},{"location":"system_prompt/#player-level-data","title":"Player-Level Data","text":"<ul> <li>Basic Stats: Goals, assists, minutes played, positions</li> <li>Limited Advanced Stats: Some matches have detailed performance data</li> <li>Career Tracking: Multi-season player performance where available</li> <li>Biographical Data: Height, weight, nationality, date of birth (partial coverage)</li> </ul>"},{"location":"system_prompt/#team-names-matching","title":"Team Names &amp; Matching","text":"<p>Teams may be referenced by multiple names. Always check variations: - North Carolina Courage = \"Courage\" - Orlando Pride = \"Pride\"  - Portland Thorns FC = \"Thorns\", \"Portland\" - Chicago Red Stars = \"Red Stars\", \"Chicago\" - And similar patterns for all teams</p> <p>Use fuzzy matching - if a user says \"Courage\", understand they mean \"North Carolina Courage\".</p>"},{"location":"system_prompt/#season-context","title":"Season Context","text":"<ul> <li>2025: Most recent season (91 matches, 14 teams, March-June)</li> <li>2024: Full season (190 matches) </li> <li>2019-2023: Complete data with comprehensive team statistics</li> <li>2013-2018: Basic match data, limited detailed statistics</li> <li>Current Date: July 2025 - 2025 season has concluded</li> </ul>"},{"location":"system_prompt/#analysis-capabilities","title":"Analysis Capabilities","text":""},{"location":"system_prompt/#what-you-can-do-excellently","title":"What You Can Do Excellently","text":"<p>\u2705 Team Performance Analysis: Season records, form, head-to-head comparisons \u2705 Match Breakdowns: Detailed team statistics, possession, passing accuracy \u2705 Historical Trends: Multi-season team performance, league evolution \u2705 League Context: Standings, top performers, season summaries \u2705 Statistical Comparisons: Team vs team, season vs season analysis  </p>"},{"location":"system_prompt/#what-has-limited-data","title":"What Has Limited Data","text":"<p>\u26a0\ufe0f Individual Player Stats: Basic goals/assists available, detailed stats limited \u26a0\ufe0f Real-time Data: Database contains historical data through 2025 season \u26a0\ufe0f Injury Reports: Not available in database \u26a0\ufe0f Contract/Transfer Info: Not available in database  </p>"},{"location":"system_prompt/#response-guidelines","title":"Response Guidelines","text":""},{"location":"system_prompt/#always-start-with","title":"Always Start With","text":"<ol> <li>Validate the request: Check if team/season exists in database</li> <li>Provide context: Mention the season/timeframe you're analyzing  </li> <li>Use specific data: Include actual numbers, percentages, and comparisons</li> <li>Be transparent: If data is limited, explain what's available vs. what's not</li> </ol>"},{"location":"system_prompt/#for-team-queries","title":"For Team Queries","text":"<ul> <li>Use the most recent season (2025) unless specified otherwise</li> <li>Provide season context (record, key stats, best performances)</li> <li>Compare to league averages when relevant</li> <li>Suggest related analysis opportunities</li> </ul>"},{"location":"system_prompt/#for-player-queries","title":"For Player Queries","text":"<ul> <li>Be upfront about data limitations</li> <li>Focus on goals, assists, appearances where available</li> <li>Suggest team-level analysis as alternative when player data is thin</li> </ul>"},{"location":"system_prompt/#for-historical-analysis","title":"For Historical Analysis","text":"<ul> <li>Leverage the 13-season dataset for trends</li> <li>Compare different eras (2013-2018 vs 2019-2025)</li> <li>Highlight significant changes or patterns over time</li> </ul>"},{"location":"system_prompt/#error-handling","title":"Error Handling","text":"<ul> <li>If a team name isn't found, suggest similar teams and spell out available options</li> <li>If a season has no data, list available seasons  </li> <li>If player data is incomplete, offer team-level analysis instead</li> <li>Always provide constructive alternatives when the exact request can't be fulfilled</li> </ul>"},{"location":"system_prompt/#example-interactions","title":"Example Interactions","text":"<p>User: \"Tell me about the 2025 Courage\" Good Response: \"The North Carolina Courage had a mixed 2025 season, finishing with a 5-5-3 record (5 wins, 5 losses, 3 draws). They scored 17 goals in 13 matches (1.31 goals per match) with 52% average possession and 79% passing accuracy...\"</p> <p>User: \"Who scored the most goals in 2024?\" Limited Data Response: \"I have limited individual player goal data for 2024. However, I can tell you the top-scoring teams that season: Kansas City Current led with 28 total goals, followed by San Diego Wave FC with 24 goals...\"</p> <p>Remember: You have access to one of the most comprehensive NWSL databases available. Use it confidently to provide deep, data-driven insights while being transparent about limitations.</p>"},{"location":"teams_seasons/","title":"List of which teams played in which seasons","text":""},{"location":"teams_seasons/#2013","title":"2013","text":"<p>Boston Breakers Kansas City Red Stars Reign Sky Blue FC Spirit Thorns WNY Flash</p>"},{"location":"teams_seasons/#2014","title":"2014","text":"<p>Boston Breakers Dash Kansas City Red Stars Reign Sky Blue FC Spirit Thorns WNY Flash</p>"},{"location":"teams_seasons/#2015","title":"2015","text":"<p>Boston Breakers Dash Kansas City Red Stars Reign Sky Blue FC Spirit Thorns WNY Flash</p>"},{"location":"teams_seasons/#2016","title":"2016","text":"<p>Boston Breakers Dash Kansas City Pride Red Stars Reign Sky Blue FC Spirit Thorns WNY Flash</p>"},{"location":"teams_seasons/#2017","title":"2017","text":"<p>Boston Breakers Courage Dash Kansas City Pride Red Stars Reign Sky Blue FC Spirit Thorns</p>"},{"location":"teams_seasons/#2018","title":"2018","text":"<p>Courage Dash Pride Red Stars Reign Royals Sky Blue FC Spirit Thorns</p>"},{"location":"teams_seasons/#2019","title":"2019","text":"<p>Courage Dash Pride Red Stars Reign Royals Sky Blue FC Spirit Thorns</p>"},{"location":"teams_seasons/#2020","title":"2020","text":"<p>Courage Dash Pride Red Stars Reign Royals Sky Blue FC Spirit Thorns</p>"},{"location":"teams_seasons/#2021","title":"2021","text":"<p>Courage Dash Gotham FC Kansas City Louisville Pride Red Stars Reign Spirit Thorns</p>"},{"location":"teams_seasons/#2022","title":"2022","text":"<p>Angel City Courage Current Dash Gotham FC Louisville Pride Red Stars Reign Spirit Thorns Wave</p>"},{"location":"teams_seasons/#2023","title":"2023","text":"<p>Angel City Courage Current Dash Gotham FC Louisville Pride Red Stars Reign Spirit Thorns Wave</p>"},{"location":"teams_seasons/#2024","title":"2024","text":"<p>Angel City Bay FC Courage Current Dash Gotham FC Louisville Pride Red Stars Reign Royals Spirit Thorns Wave</p>"},{"location":"teams_seasons/#2025","title":"2025","text":"<p>Angel City Bay FC Chicago Stars Courage Current Dash Gotham FC Louisville Pride Reign Royals Spirit Thorns Wave</p>"},{"location":"test_queries/","title":"NWSL MCP Server - Test Queries for OpenAI Playground","text":""},{"location":"test_queries/#setup-instructions","title":"Setup Instructions","text":"<ol> <li> <p>Install MCP dependencies:    <pre><code>pip install mcp&gt;=1.0.0\n</code></pre></p> </li> <li> <p>Test the server locally:    <pre><code>python -m src.server\n</code></pre></p> </li> <li> <p>For OpenAI Playground integration, add this configuration to your MCP settings:    <pre><code>{\n  \"mcpServers\": {\n    \"nwsl-data\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"src.server\"],\n      \"env\": {}\n    }\n  }\n}\n</code></pre></p> </li> </ol>"},{"location":"test_queries/#available-tools","title":"Available Tools","text":""},{"location":"test_queries/#1-query_player_stats","title":"1. <code>query_player_stats</code>","text":"<p>Get detailed statistics for a specific player - player_name (required): Name of the player - season (optional): Season year (e.g., 2024)</p>"},{"location":"test_queries/#2-query_team_performance","title":"2. <code>query_team_performance</code>","text":"<p>Get team performance data and statistics - team_name (required): Name of the team - season (optional): Season year</p>"},{"location":"test_queries/#3-query_match_data","title":"3. <code>query_match_data</code>","text":"<p>Get detailed match information and results - team1 (required): First team name - team2 (optional): Second team name for head-to-head - season (optional): Season year - limit (optional): Number of matches to return (default: 10)</p>"},{"location":"test_queries/#4-query_league_standings","title":"4. <code>query_league_standings</code>","text":"<p>Get league standings and team rankings - season (required): Season year - match_type (optional): Match type filter (default: \"Regular Season\")</p>"},{"location":"test_queries/#5-search_players","title":"5. <code>search_players</code>","text":"<p>Search for players by name or partial name - search_term (required): Player name or partial name - limit (optional): Number of results (default: 10)</p>"},{"location":"test_queries/#6-get_season_overview","title":"6. <code>get_season_overview</code>","text":"<p>Get overview statistics for a specific season - season (required): Season year</p>"},{"location":"test_queries/#example-test-queries-for-openai-playground","title":"Example Test Queries for OpenAI Playground","text":""},{"location":"test_queries/#player-queries","title":"Player Queries","text":"<pre><code>Show me Megan Rapinoe's career statistics across all seasons\n</code></pre> <pre><code>What were Alex Morgan's stats in the 2019 season?\n</code></pre> <pre><code>Find all players with \"Smith\" in their name\n</code></pre>"},{"location":"test_queries/#team-queries","title":"Team Queries","text":"<pre><code>How did Angel City FC perform in their inaugural 2022 season?\n</code></pre> <pre><code>Show me Portland Thorns' performance over the last 3 seasons\n</code></pre> <pre><code>Compare Seattle Reign and OL Reign performance data\n</code></pre>"},{"location":"test_queries/#match-queries","title":"Match Queries","text":"<pre><code>Show me the last 5 matches between Portland Thorns and Seattle Reign\n</code></pre> <pre><code>What were the highest-scoring games in the 2023 season?\n</code></pre> <pre><code>Show me all playoff matches from 2024\n</code></pre>"},{"location":"test_queries/#league-analysis","title":"League Analysis","text":"<pre><code>What were the 2024 NWSL regular season standings based on expected goals?\n</code></pre> <pre><code>Give me an overview of the 2023 NWSL season\n</code></pre> <pre><code>Which teams had the best attacking performance in 2022?\n</code></pre>"},{"location":"test_queries/#advanced-queries","title":"Advanced Queries","text":"<pre><code>Who were the top scorers in NWSL history?\n</code></pre> <pre><code>Which venues have hosted the most NWSL matches?\n</code></pre> <pre><code>Show me teams that performed better than expected based on xG\n</code></pre>"},{"location":"test_queries/#database-coverage","title":"Database Coverage","text":"<ul> <li>Years: 2013-2025</li> <li>Total Matches: 1,563</li> <li>Total Players: 946</li> <li>Teams: 17</li> <li>Data includes: Match results, player statistics, team performance, expected goals (xG), venues, and more</li> </ul>"},{"location":"test_queries/#tips-for-effective-queries","title":"Tips for Effective Queries","text":"<ol> <li>Use partial team/player names if unsure of exact spelling</li> <li>Specify seasons for more focused results</li> <li>Use the search tools first to find correct names</li> <li>Combine multiple tools for comprehensive analysis</li> <li>Ask follow-up questions to drill down into specific areas</li> </ol>"},{"location":"test_queries/#common-team-names-to-use","title":"Common Team Names to Use","text":"<ul> <li>Portland Thorns, Thorns</li> <li>Seattle Reign, OL Reign</li> <li>Angel City FC, ACFC</li> <li>North Carolina Courage, Courage</li> <li>Orlando Pride, Pride</li> <li>Houston Dash, Dash</li> <li>Chicago Red Stars</li> <li>Washington Spirit</li> <li>San Diego Wave FC, Wave</li> <li>Utah Royals</li> <li>Gotham FC</li> <li>Boston Breakers (historical)</li> <li>Western New York Flash (historical)</li> </ul>"},{"location":"about/methodology/","title":"Our Analytical Methodology","text":""},{"location":"about/methodology/#sabermetrics-inspired-approach","title":"Sabermetrics-Inspired Approach","text":"<p>Our analytical framework is built on principles established by pioneers like Bill James and refined by researchers like Jim Albert, adapted specifically for the unique characteristics of professional soccer.</p>"},{"location":"about/methodology/#core-philosophical-principles","title":"Core Philosophical Principles","text":""},{"location":"about/methodology/#1-predictive-power-over-descriptive-appeal","title":"1. Predictive Power Over Descriptive Appeal","text":"<p>We prioritize metrics that correlate with future success over those that simply sound impressive.</p> <p>Baseball Lesson Applied</p> <p>Jim Albert's research showed that batting average had a year-over-year correlation of only 0.44, while On-Base Percentage (OBP) correlated at 0.61. We apply the same rigorous testing to soccer metrics, identifying which statistics actually predict future performance.</p>"},{"location":"about/methodology/#2-context-independent-analysis","title":"2. Context-Independent Analysis","text":"<p>Following the Defense Independent Pitching Statistics (DIPS) concept, we separate individual performance from team and situational factors wherever possible.</p>"},{"location":"about/methodology/#3-composite-metrics-over-simple-counting","title":"3. Composite Metrics Over Simple Counting","text":"<p>Like baseball's move from batting average to OPS, we combine multiple performance dimensions into meaningful composite measures.</p>"},{"location":"about/methodology/#4-historical-benchmarking","title":"4. Historical Benchmarking","text":"<p>All performance is evaluated relative to historical context and peer groups, not in absolute terms.</p>"},{"location":"about/methodology/#the-nwsl-impact-rating-nir-system","title":"The NWSL Impact Rating (NIR) System","text":""},{"location":"about/methodology/#conceptual-foundation","title":"Conceptual Foundation","text":"<p>Our flagship metric, the NWSL Impact Rating (NIR), represents a soccer adaptation of composite metrics like OPS (On-base Plus Slugging) in baseball.</p> <p>NIR Formula: <pre><code>NIR = (Attacking Impact \u00d7 0.4) + (Defensive Impact \u00d7 0.3) + (Progression Impact \u00d7 0.2) + Context Adjustment\n</code></pre></p>"},{"location":"about/methodology/#component-breakdown","title":"Component Breakdown","text":""},{"location":"about/methodology/#attacking-impact","title":"Attacking Impact","text":"<p>Combines goals and assists per 90 minutes, weighted by shot efficiency: - Raw production: Goals + Assists per 90 - Efficiency multiplier: Shot accuracy (minimum 0.5 to avoid small sample bias) - Why this matters: Separates efficient attackers from high-volume, low-efficiency players</p>"},{"location":"about/methodology/#defensive-impact","title":"Defensive Impact","text":"<p>Measures defensive actions per 90 minutes: - Tackles + Interceptions + Blocks per 90 - Adjusted for playing time to ensure fair comparison - Why this matters: Captures defensive contribution often invisible in traditional stats</p>"},{"location":"about/methodology/#progression-impact","title":"Progression Impact","text":"<p>Evaluates ball progression and distribution: - Progressive passes per 90 \u00d7 (0.5 + passing accuracy \u00d7 0.5) - Why this matters: Identifies players who advance team possession effectively</p>"},{"location":"about/methodology/#context-adjustment","title":"Context Adjustment","text":"<p>Currently neutral (1.0), future enhancement will include: - Opposition strength weighting - Team quality adjustment - Game state considerations - Home/away effects</p>"},{"location":"about/methodology/#validation-methodology","title":"Validation Methodology","text":"<p>We test all metrics using Jim Albert's year-over-year correlation approach:</p> <ol> <li>Calculate metric for all players in Season N</li> <li>Recalculate same metric for same players in Season N+1  </li> <li>Measure correlation between the two seasons</li> <li>Higher correlation = more predictive value</li> </ol> <p>Transparency Commitment</p> <p>We publish correlation coefficients for all our metrics and continuously test new approaches. Metrics that don't demonstrate predictive power are deprecated.</p>"},{"location":"about/methodology/#data-processing-standards","title":"Data Processing Standards","text":""},{"location":"about/methodology/#quality-assurance-framework","title":"Quality Assurance Framework","text":""},{"location":"about/methodology/#1-source-validation","title":"1. Source Validation","text":"<ul> <li>Primary sources: FBRef match pages with detailed statistics</li> <li>Secondary validation: Official NWSL records where available</li> <li>Manual verification: Edge cases and outliers reviewed individually</li> </ul>"},{"location":"about/methodology/#2-completeness-standards","title":"2. Completeness Standards","text":"<ul> <li>Target: 100% coverage for all available matches</li> <li>Current: 99.38% completion across 42,572 player records</li> <li>Missing data handling: Explicit documentation of gaps, never filled with estimates</li> </ul>"},{"location":"about/methodology/#3-consistency-checks","title":"3. Consistency Checks","text":"<ul> <li>Cross-season format validation</li> <li>Statistical outlier identification and verification</li> <li>Historical data integrity maintenance</li> </ul>"},{"location":"about/methodology/#statistical-processing","title":"Statistical Processing","text":""},{"location":"about/methodology/#age-standardization","title":"Age Standardization","text":"<p>Player ages parsed from historical formats: - Legacy format: \"YY-DDD\" (year-day of year) - Modern format: Standard decimal years - Consistency maintained across all 13 seasons</p>"},{"location":"about/methodology/#position-mapping","title":"Position Mapping","text":"<p>Standardized position categories: - Historical variations normalized - Tactical role identification beyond simple positions - Formation-agnostic classification system</p>"},{"location":"about/methodology/#sample-size-requirements","title":"Sample Size Requirements","text":"<p>Minimum thresholds for meaningful analysis: - Players: 3+ matches with &gt;0 minutes for seasonal analysis - Teams: Full season data required for comparative analysis - Historical trends: Minimum 2 seasons for longitudinal studies</p>"},{"location":"about/methodology/#analytical-limitations-assumptions","title":"Analytical Limitations &amp; Assumptions","text":""},{"location":"about/methodology/#what-we-dont-capture","title":"What We Don't Capture","text":"<ul> <li>Goalkeeper-specific metrics: Currently limited by data availability</li> <li>Defensive positioning: Individual marking and spatial coverage</li> <li>Set piece specialization: Corner/free kick specific contributions</li> <li>Leadership/intangibles: Qualitative factors beyond statistical measurement</li> </ul>"},{"location":"about/methodology/#known-biases","title":"Known Biases","text":"<ul> <li>Playing time bias: Higher minutes = more opportunities for statistical accumulation</li> <li>Team quality effect: Players on better teams may have inflated metrics</li> <li>Competition level: Regular season vs. playoff performance may differ</li> </ul>"},{"location":"about/methodology/#methodological-assumptions","title":"Methodological Assumptions","text":"<ul> <li>Linear metric combination: NIR components combined additively (future versions may explore non-linear relationships)</li> <li>Position neutrality: Current metrics don't adjust for positional expectations</li> <li>Temporal consistency: Assumes tactical/competitive environment remains relatively stable</li> </ul>"},{"location":"about/methodology/#continuous-improvement-process","title":"Continuous Improvement Process","text":""},{"location":"about/methodology/#research-pipeline","title":"Research Pipeline","text":"<ol> <li>Hypothesis Development: Based on soccer tactical theory and statistical research</li> <li>Historical Testing: Validate against 13 seasons of NWSL data</li> <li>Correlation Analysis: Measure predictive power using Albert's methodology</li> <li>Implementation: Deploy metrics meeting validation thresholds</li> <li>Monitoring: Ongoing performance tracking and refinement</li> </ol>"},{"location":"about/methodology/#community-input","title":"Community Input","text":"<p>We actively seek feedback from: - Soccer analysts and researchers - NWSL coaches and technical staff - Statistics and data science community - Engaged fans with tactical knowledge</p> <p>Our methodology evolves continuously, but always maintains the core principle: meaningful insights backed by rigorous statistical validation.</p>"},{"location":"about/mission/","title":"Mission &amp; Vision","text":""},{"location":"about/mission/#our-mission","title":"Our Mission","text":"<p>To revolutionize the understanding and analysis of women's professional soccer by creating the most sophisticated analytics platform in the sport.</p> <p>The National Women's Soccer League represents the pinnacle of women's professional soccer in North America. Yet despite the incredible talent, tactical complexity, and competitive drama that defines the NWSL, the league has lacked the analytical depth that has transformed other major sports.</p> <p>We're changing that.</p>"},{"location":"about/mission/#why-the-nwsl-needs-advanced-analytics","title":"Why the NWSL Needs Advanced Analytics","text":""},{"location":"about/mission/#the-current-state","title":"The Current State","text":"<p>Most soccer analysis relies on basic counting statistics: - Goals and assists - Shots and saves - Simple possession percentages - Win-loss records</p> <p>While these metrics provide a starting point, they miss the deeper story of what actually drives success in professional soccer.</p>"},{"location":"about/mission/#the-transformation-were-building","title":"The Transformation We're Building","text":"<p>Just as sabermetrics revolutionized baseball by moving beyond batting average to more meaningful metrics like OPS (On-base Plus Slugging), we're bringing that same analytical rigor to women's soccer.</p> <p>The Baseball Parallel</p> <p>In baseball, analysts discovered that batting average explained only 46% of the variation in team run production, while OPS explained 89%. This revelation transformed how teams evaluate players and make strategic decisions.</p> <p>We're applying the same methodology to soccer, identifying which metrics actually correlate with team success versus those that just sound impressive.</p>"},{"location":"about/mission/#our-vision","title":"Our Vision","text":""},{"location":"about/mission/#a-world-where","title":"A World Where...","text":"<ul> <li>Coaches make tactical decisions backed by data-driven insights about player roles, matchup advantages, and strategic optimization</li> <li>Players understand their true impact beyond the scoresheet, with personalized development recommendations based on comprehensive performance analysis</li> <li>Fans engage with the sport at a deeper level, appreciating the tactical nuances and individual contributions that traditional statistics miss</li> <li>Media tells richer stories about the game, supported by meaningful analytics that reveal the \"why\" behind results</li> <li>Front Offices build rosters strategically, identifying undervalued talent and optimizing team chemistry through advanced metrics</li> </ul>"},{"location":"about/mission/#the-platform-were-building","title":"The Platform We're Building","text":"<p>Our vision extends beyond just providing statistics. We're creating:</p> <ol> <li>Educational Foundation: Helping people understand not just what the numbers say, but why they matter</li> <li>Transparent Methodology: Complete openness about our analytical approaches, limitations, and assumptions</li> <li>Accessible Intelligence: Complex analysis presented in ways that serve both casual fans and professional analysts</li> <li>Community Resource: A platform that elevates discussion and understanding across the entire NWSL ecosystem</li> </ol>"},{"location":"about/mission/#what-drives-us","title":"What Drives Us","text":""},{"location":"about/mission/#respect-for-the-game","title":"Respect for the Game","text":"<p>The NWSL features world-class athletes competing at the highest level. The analysis should match that quality.</p>"},{"location":"about/mission/#commitment-to-truth","title":"Commitment to Truth","text":"<p>We'd rather admit what we don't know than oversell what we do. Our analysis acknowledges uncertainty and clearly communicates confidence levels.</p>"},{"location":"about/mission/#passion-for-progress","title":"Passion for Progress","text":"<p>Every insight we generate should serve the broader goal of helping the league, teams, and players improve.</p>"},{"location":"about/mission/#belief-in-transparency","title":"Belief in Transparency","text":"<p>Advanced analytics can seem like a \"black box.\" We believe in showing our work and explaining our reasoning.</p>"},{"location":"about/mission/#success-metrics","title":"Success Metrics","text":"<p>We'll know we're succeeding when:</p> <ul> <li>NWSL discussions become more sophisticated, moving beyond surface-level analysis</li> <li>Player development benefits from data-driven insights about role optimization</li> <li>Tactical innovation accelerates as coaches gain better tools for analysis</li> <li>Fan engagement deepens through richer understanding of the game</li> <li>Academic research builds upon our open methodologies to push the field forward</li> </ul>"},{"location":"about/mission/#long-term-impact","title":"Long-Term Impact","text":"<p>Our ultimate goal isn't just to create better soccer statistics. It's to contribute to the growth and sophistication of women's professional soccer as a whole.</p> <p>By providing the analytical foundation that other major sports take for granted, we're helping the NWSL reach its full potential as a league that showcases the world's best players competing at the highest tactical and strategic level.</p> <p>The mission is ambitious, but the NWSL deserves nothing less than the most sophisticated analytical approach we can build.</p>"},{"location":"about/why-advanced-analytics/","title":"Why Advanced Analytics Matter in Soccer","text":""},{"location":"about/why-advanced-analytics/#the-problem-with-traditional-soccer-statistics","title":"The Problem with Traditional Soccer Statistics","text":"<p>Traditional soccer statistics, while useful, tell an incomplete and sometimes misleading story about player and team performance.</p>"},{"location":"about/why-advanced-analytics/#the-goals-and-assists-trap","title":"The \"Goals and Assists\" Trap","text":"<p>Most soccer analysis focuses heavily on goals and assists - the most visible contributions to the game. But this approach has fundamental limitations:</p> <p>Real Example: The Defensive Midfielder Dilemma</p> <p>A defensive midfielder who breaks up 8 attacks, makes 12 interceptions, and completes 45 progressive passes might have zero goals and assists, while a forward who scores one goal from 8 shots and loses possession 15 times appears statistically superior. Traditional stats miss the defensive midfielder's massive contribution to team success.</p>"},{"location":"about/why-advanced-analytics/#what-traditional-stats-miss","title":"What Traditional Stats Miss","text":""},{"location":"about/why-advanced-analytics/#defensive-contributions","title":"Defensive Contributions","text":"<ul> <li>Tackles, interceptions, and blocks that prevent goals</li> <li>Defensive positioning that forces opponents into poor decisions</li> <li>Ball recovery in dangerous areas</li> </ul>"},{"location":"about/why-advanced-analytics/#ball-progression","title":"Ball Progression","text":"<ul> <li>Passes that advance the team toward goal</li> <li>Dribbles that break defensive lines</li> <li>Cross-field switches that change the point of attack</li> </ul>"},{"location":"about/why-advanced-analytics/#efficiency-vs-volume","title":"Efficiency vs. Volume","text":"<ul> <li>A player with 1 goal from 2 shots vs. 1 goal from 10 shots</li> <li>High pass completion in safe areas vs. moderate success on dangerous passes</li> <li>Shot quality and creation circumstances</li> </ul>"},{"location":"about/why-advanced-analytics/#context-and-competition-level","title":"Context and Competition Level","text":"<ul> <li>Performance against strong vs. weak opponents</li> <li>Impact in close games vs. blowouts</li> <li>Home vs. away performance differences</li> </ul>"},{"location":"about/why-advanced-analytics/#the-baseball-revolution-a-model-for-soccer","title":"The Baseball Revolution: A Model for Soccer","text":""},{"location":"about/why-advanced-analytics/#how-sabermetrics-changed-everything","title":"How Sabermetrics Changed Everything","text":"<p>Baseball's analytical revolution provides a roadmap for soccer's evolution:</p>"},{"location":"about/why-advanced-analytics/#stage-1-basic-counting-stats","title":"Stage 1: Basic Counting Stats","text":"<p>Baseball circa 1960s - Batting Average, Home Runs, RBIs - Wins and Losses for pitchers - Simple, visible statistics</p> <p>Soccer equivalent: Goals, Assists, Clean Sheets</p>"},{"location":"about/why-advanced-analytics/#stage-2-rate-stats-and-context","title":"Stage 2: Rate Stats and Context","text":"<p>Baseball 1970s-80s - On-Base Percentage - ERA+, adjusted for ballpark and era - Recognition that context matters</p> <p>Soccer equivalent: Goals per 90, Expected Goals (xG)</p>"},{"location":"about/why-advanced-analytics/#stage-3-composite-and-predictive-metrics","title":"Stage 3: Composite and Predictive Metrics","text":"<p>Baseball 1990s-2000s - OPS (On-base Plus Slugging) - WHIP (Walks + Hits per Inning Pitched)  - Metrics that actually predict winning</p> <p>Soccer equivalent: This is where we are now - developing composite metrics like NIR</p>"},{"location":"about/why-advanced-analytics/#the-ops-revolution","title":"The OPS Revolution","text":"<p>The most instructive example is baseball's adoption of OPS (On-Base Percentage + Slugging Percentage):</p> Before OPSAfter OPS <p>Batting Average was king - Easy to understand - Historically established - Problem: Explained only 46% of team run production</p> <p>Composite metric dominated - Combined two important skills - Better predictive power - Result: Explained 89% of team run production</p> <p>The Soccer Parallel: We're building composite metrics like NIR that combine attacking, defensive, and progression contributions - just as OPS combined getting on base and hitting for power.</p>"},{"location":"about/why-advanced-analytics/#why-soccer-needs-this-evolution","title":"Why Soccer Needs This Evolution","text":""},{"location":"about/why-advanced-analytics/#the-tactical-complexity-problem","title":"The Tactical Complexity Problem","text":"<p>Modern soccer is incredibly sophisticated: - Positional fluidity: Players switch roles during matches - Pressing systems: Coordinated defensive actions across the team - Build-up patterns: Complex passing sequences to break down defenses - Set piece specialization: Detailed planning for dead ball situations</p> <p>Traditional statistics can't capture this complexity.</p>"},{"location":"about/why-advanced-analytics/#the-undervaluation-problem","title":"The Undervaluation Problem","text":"<p>Current statistics systematically undervalue certain types of players:</p>"},{"location":"about/why-advanced-analytics/#the-regista-deep-lying-playmaker","title":"The Regista (Deep-lying Playmaker)","text":"<ul> <li>Dictates tempo and rhythm</li> <li>Initiates attacks with long passes</li> <li>Traditional stats: Low goals/assists</li> <li>Actual impact: Orchestrates entire team's possession</li> </ul>"},{"location":"about/why-advanced-analytics/#the-destroyer-defensive-midfielder","title":"The Destroyer (Defensive Midfielder)","text":"<ul> <li>Breaks up opponent attacks</li> <li>Wins second balls and tackles</li> <li>Traditional stats: Minimal offensive contribution</li> <li>Actual impact: Prevents goals and creates turnovers</li> </ul>"},{"location":"about/why-advanced-analytics/#the-false-9-dropping-forward","title":"The False 9 (Dropping Forward)","text":"<ul> <li>Creates space for others</li> <li>Links midfield and attack</li> <li>Traditional stats: Fewer goals than traditional striker</li> <li>Actual impact: Enables entire tactical system</li> </ul>"},{"location":"about/why-advanced-analytics/#the-strategic-decision-problem","title":"The Strategic Decision Problem","text":"<p>Teams and coaches making decisions based on incomplete information:</p> <ul> <li>Player acquisitions based on goals/assists rather than total contribution</li> <li>Tactical decisions without understanding which players excel in specific roles</li> <li>Development planning focused on visible skills rather than impact areas</li> </ul>"},{"location":"about/why-advanced-analytics/#what-advanced-analytics-provide","title":"What Advanced Analytics Provide","text":""},{"location":"about/why-advanced-analytics/#comprehensive-player-evaluation","title":"Comprehensive Player Evaluation","text":"<p>Instead of judging players on 2-3 visible metrics, we evaluate across multiple dimensions:</p> <ul> <li>Attacking contribution: Goals, assists, chance creation</li> <li>Defensive impact: Tackles, interceptions, ball recovery  </li> <li>Progression ability: Forward passes, dribbles, possession advancement</li> <li>Efficiency measures: Success rates and decision-making quality</li> </ul>"},{"location":"about/why-advanced-analytics/#context-aware-analysis","title":"Context-Aware Analysis","text":"<p>Performance adjusted for: - Opposition strength: How you perform against tough vs. easy opponents - Team quality: Individual contribution relative to teammates - Game state: Performance when ahead, behind, or level - Playing time: Fair comparison across different usage patterns</p>"},{"location":"about/why-advanced-analytics/#predictive-insights","title":"Predictive Insights","text":"<p>Metrics tested for their ability to predict: - Future performance: Which players will maintain their level - Team success: Which contributions actually correlate with winning - Role optimization: Where players can maximize their impact</p>"},{"location":"about/why-advanced-analytics/#tactical-intelligence","title":"Tactical Intelligence","text":"<p>Automated identification of: - Playing styles: How teams and players actually operate - Matchup advantages: Which tactical combinations work best - Evolution patterns: How the game is changing over time</p>"},{"location":"about/why-advanced-analytics/#the-nwsl-opportunity","title":"The NWSL Opportunity","text":""},{"location":"about/why-advanced-analytics/#why-now","title":"Why Now?","text":"<p>The NWSL is perfectly positioned for this analytical evolution:</p> <ol> <li>Data availability: 13 seasons of comprehensive statistics</li> <li>Competitive balance: Close games where marginal advantages matter</li> <li>Tactical sophistication: World-class players and coaches</li> <li>Growing audience: Fans hungry for deeper understanding</li> </ol>"},{"location":"about/why-advanced-analytics/#the-impact-potential","title":"The Impact Potential","text":"<p>Advanced analytics can transform:</p> <ul> <li>Fan engagement: Deeper appreciation for tactical nuances</li> <li>Media coverage: More sophisticated game analysis</li> <li>Player development: Data-driven improvement recommendations  </li> <li>Strategic decisions: Better roster construction and tactical planning</li> <li>League growth: Enhanced credibility and analytical sophistication</li> </ul>"},{"location":"about/why-advanced-analytics/#real-world-applications","title":"Real-World Applications","text":""},{"location":"about/why-advanced-analytics/#for-coaches","title":"For Coaches","text":"<ul> <li>Identify which players excel in specific tactical roles</li> <li>Understand opponent weaknesses and matchup advantages</li> <li>Make substitutions based on situational effectiveness data</li> </ul>"},{"location":"about/why-advanced-analytics/#for-players","title":"For Players","text":"<ul> <li>Understand their true impact beyond traditional statistics</li> <li>Receive specific development recommendations</li> <li>Optimize their role within team tactical systems</li> </ul>"},{"location":"about/why-advanced-analytics/#for-fans","title":"For Fans","text":"<ul> <li>Appreciate players who contribute in less visible ways</li> <li>Understand tactical decisions and their rationale</li> <li>Engage in more sophisticated discussions about the game</li> </ul>"},{"location":"about/why-advanced-analytics/#for-teams","title":"For Teams","text":"<ul> <li>Identify undervalued players in the transfer market</li> <li>Optimize lineups and formations based on data</li> <li>Track performance trends and development patterns</li> </ul> <p>Advanced analytics don't replace the beauty and artistry of soccer - they help us understand and appreciate it at a deeper level.</p>"},{"location":"data/coverage/","title":"Data Coverage &amp; Quality","text":""},{"location":"data/coverage/#comprehensive-historical-database","title":"Comprehensive Historical Database","text":"<p>Our analysis is built on the most extensive NWSL database ever assembled, spanning the league's entire history from its founding in 2013 through the current 2025 season.</p>"},{"location":"data/coverage/#coverage-statistics","title":"Coverage Statistics","text":""},{"location":"data/coverage/#overall-completeness","title":"Overall Completeness","text":"<p>Platform Status</p> <p>99.38% completion across 42,572 individual player records spanning 13 complete seasons</p> Metric Value Total Seasons 13 (2013-2025) Total Matches 1,563 Player Records 42,572 Overall Completion 99.38% Statistical Fields 35+ per player per match"},{"location":"data/coverage/#season-by-season-breakdown","title":"Season-by-Season Breakdown","text":"Season Records Completion Matches Status 2025 2,621 100.0% 91 \u2705 Complete 2024 4,769 100.0% 190 \u2705 Complete 2023 4,995 98.84% 176 \ud83d\udfe1 Near Complete 2022 5,272 100.0% 176 \u2705 Complete 2021 4,305 100.0% 144 \u2705 Complete 2020 1,236 100.0% 41 \u2705 Complete 2019 3,046 100.0% 111 \u2705 Complete 2018 3,046 100.0% 111 \u2705 Complete 2017 3,394 100.0% 123 \u2705 Complete 2016 2,830 92.69% 103 \ud83d\udfe1 Near Complete 2015 2,558 100.0% 93 \u2705 Complete 2014 3,032 100.0% 111 \u2705 Complete 2013 2,468 100.0% 91 \u2705 Complete"},{"location":"data/coverage/#data-format-evolution","title":"Data Format Evolution","text":"<p>Our database handles the evolution of statistical tracking across the league's history:</p>"},{"location":"data/coverage/#legacy-format-2013-2018","title":"Legacy Format (2013-2018)","text":"<ul> <li>24 statistical fields per player per match</li> <li>Basic performance metrics (goals, assists, shots, cards)</li> <li>Essential playing time and positional data</li> <li>Age parsing in \"YY-DDD\" format (year-day of year)</li> </ul>"},{"location":"data/coverage/#modern-format-2019-2025","title":"Modern Format (2019-2025)","text":"<ul> <li>37+ statistical fields per player per match</li> <li>Advanced metrics including:<ul> <li>Expected Goals (xG) and Expected Assists (xA)</li> <li>Progressive passes and carries  </li> <li>Detailed passing accuracy by zone</li> <li>Shot creation and goal creation actions</li> <li>Take-on success rates</li> </ul> </li> </ul>"},{"location":"data/coverage/#what-our-database-contains","title":"What Our Database Contains","text":""},{"location":"data/coverage/#player-level-match-statistics","title":"Player-Level Match Statistics","text":"<p>For each player in each match, we track:</p>"},{"location":"data/coverage/#basic-performance","title":"Basic Performance","text":"<ul> <li>Goals, assists, shots, shots on target</li> <li>Minutes played, starting position</li> <li>Yellow cards, red cards</li> <li>Penalty kicks taken and scored</li> </ul>"},{"location":"data/coverage/#passing-distribution","title":"Passing &amp; Distribution","text":"<ul> <li>Pass attempts and completions</li> <li>Pass completion percentage</li> <li>Progressive passes (advancing toward goal)</li> <li>Key passes leading to shots</li> </ul>"},{"location":"data/coverage/#attacking-actions","title":"Attacking Actions","text":"<ul> <li>Shot creation actions (SCA)</li> <li>Goal creation actions (GCA)  </li> <li>Take-ons attempted and successful</li> <li>Carries and progressive carries</li> </ul>"},{"location":"data/coverage/#defensive-actions","title":"Defensive Actions","text":"<ul> <li>Tackles won and attempted</li> <li>Interceptions made</li> <li>Blocks (shots, passes, crosses)</li> <li>Clearances and recoveries</li> </ul>"},{"location":"data/coverage/#advanced-metrics-2019","title":"Advanced Metrics (2019+)","text":"<ul> <li>Expected Goals (xG) generated</li> <li>Non-penalty Expected Goals (npxG)</li> <li>Expected Assists (xA) created</li> <li>Shot quality and conversion rates</li> </ul>"},{"location":"data/coverage/#team-level-match-data","title":"Team-Level Match Data","text":"<p>For each team in each match: - Final score and result - Possession percentage - Passing accuracy overall - Shots on target percentage - Fouls committed - Corners earned - Disciplinary actions</p>"},{"location":"data/coverage/#match-context-information","title":"Match Context Information","text":"<ul> <li>Date, season, and competition type</li> <li>Venue and weather conditions (where available)</li> <li>Referee assignments</li> <li>Attendance figures (where available)</li> </ul>"},{"location":"data/coverage/#data-quality-assurance","title":"Data Quality Assurance","text":""},{"location":"data/coverage/#source-verification","title":"Source Verification","text":""},{"location":"data/coverage/#primary-sources","title":"Primary Sources","text":"<ul> <li>FBRef match pages: Detailed statistical breakdowns</li> <li>Official NWSL records: Score verification and context</li> <li>Historical archives: Complete coverage validation</li> </ul>"},{"location":"data/coverage/#validation-process","title":"Validation Process","text":"<ol> <li>Automated consistency checks: Cross-reference multiple data points</li> <li>Statistical outlier identification: Flag unusual values for manual review</li> <li>Historical continuity: Ensure player and team tracking across seasons</li> <li>Manual verification: Human review of edge cases and corrections</li> </ol>"},{"location":"data/coverage/#handling-missing-data","title":"Handling Missing Data","text":"<p>We maintain strict standards for data completeness:</p> <p>Missing Data Policy</p> <ul> <li>Never estimate or interpolate missing statistical data</li> <li>Clearly document all gaps in coverage</li> <li>Provide completion percentages for all datasets</li> <li>Distinguish between \"zero\" and \"missing\" values</li> </ul>"},{"location":"data/coverage/#known-gaps","title":"Known Gaps","text":"<ul> <li>2023 Season: 1.16% missing data (58 player records from 3 matches)</li> <li>2016 Season: 7.31% missing data (207 player records from 7 matches)</li> <li>Goalkeeper Statistics: Limited availability in early seasons</li> <li>Weather Data: Incomplete coverage across all venues</li> </ul>"},{"location":"data/coverage/#data-integrity-standards","title":"Data Integrity Standards","text":""},{"location":"data/coverage/#cross-season-consistency","title":"Cross-Season Consistency","text":"<ul> <li>Player identity tracking across team changes</li> <li>Position standardization despite tactical evolution</li> <li>Statistical definition consistency over time</li> </ul>"},{"location":"data/coverage/#quality-metrics","title":"Quality Metrics","text":"<ul> <li>Coverage Rate: 99.38% overall completion</li> <li>Accuracy Rate: &lt;0.1% error rate in manual spot checks</li> <li>Completeness Score: Full statistical profiles for 97.2% of all player appearances</li> </ul>"},{"location":"data/coverage/#historical-context-limitations","title":"Historical Context &amp; Limitations","text":""},{"location":"data/coverage/#league-evolution-impact","title":"League Evolution Impact","text":"<p>The NWSL has evolved significantly since 2013:</p>"},{"location":"data/coverage/#structural-changes","title":"Structural Changes","text":"<ul> <li>2013-2015: 8-9 teams, shorter seasons</li> <li>2016-2019: 10 teams, expanded regular season</li> <li>2020: COVID-19 shortened season (Challenge Cup format)</li> <li>2021-2025: 12+ teams, full regular season + playoffs</li> </ul>"},{"location":"data/coverage/#statistical-tracking-evolution","title":"Statistical Tracking Evolution","text":"<ul> <li>Early years: Basic statistics only</li> <li>2017+: Introduction of advanced metrics</li> <li>2019+: Full xG and detailed passing data</li> <li>2022+: Enhanced tactical analysis data</li> </ul>"},{"location":"data/coverage/#what-we-cannot-measure","title":"What We Cannot Measure","text":"<p>Despite our comprehensive database, certain aspects remain unmeasured:</p> <ul> <li>Positional intelligence: Off-ball movement and spatial awareness</li> <li>Communication: On-field leadership and organization</li> <li>Injury impact: Performance affected by undisclosed injuries</li> <li>Tactical instructions: Specific role requirements from coaches</li> <li>Environmental factors: Field conditions, travel fatigue, etc.</li> </ul>"},{"location":"data/coverage/#ongoing-data-collection","title":"Ongoing Data Collection","text":""},{"location":"data/coverage/#real-time-updates","title":"Real-Time Updates","text":"<ul> <li>New match data integrated within 24-48 hours</li> <li>Continuous monitoring for data source changes</li> <li>Automated validation of incoming statistics</li> </ul>"},{"location":"data/coverage/#historical-improvements","title":"Historical Improvements","text":"<ul> <li>Ongoing efforts to fill remaining gaps in 2016 and 2023</li> <li>Enhanced validation of early season data</li> <li>Integration of additional context information where available</li> </ul> <p>Our data foundation represents 13 years of meticulous collection and validation, providing the most comprehensive view of NWSL performance ever assembled.</p>"},{"location":"data/database-overview/","title":"Database Overview &amp; Schema","text":""},{"location":"data/database-overview/#database-architecture","title":"Database Architecture","text":"<p>Our NWSL analytics platform is built on a comprehensive relational database designed to support both historical analysis and real-time intelligence queries.</p>"},{"location":"data/database-overview/#core-design-principles","title":"Core Design Principles","text":"<ul> <li>Normalized structure: Eliminates data redundancy while maintaining query performance</li> <li>Historical accuracy: Preserves exact statistical records as they existed</li> <li>Analytical optimization: Indexed and structured for complex analytical queries</li> <li>Scalable design: Supports growth from 13 to 50+ seasons</li> </ul>"},{"location":"data/database-overview/#entity-relationship-overview","title":"Entity Relationship Overview","text":"<pre><code>erDiagram\n    SEASON ||--o{ MATCH : contains\n    MATCH ||--o{ MATCH_PLAYER_SUMMARY : includes\n    MATCH ||--o{ MATCH_TEAM : features\n    PLAYER ||--o{ MATCH_PLAYER_SUMMARY : participates\n    TEAM ||--o{ MATCH_PLAYER_SUMMARY : employs\n    TEAM ||--o{ MATCH_TEAM : competes\n\n    SEASON {\n        text season_id PK\n        text season_name\n        date start_date\n        date end_date\n        int total_teams\n        int total_matches\n    }\n\n    MATCH {\n        text match_id PK\n        text season_id FK\n        date match_date\n        text venue\n        text competition_type\n        text match_status\n    }\n\n    PLAYER {\n        text player_id PK\n        text player_name\n        text nationality\n        date birth_date\n        text primary_position\n    }\n\n    TEAM {\n        text team_id PK\n        text team_name_1\n        text team_name_2\n        text team_name_3\n        text team_name_4\n        text city\n        text founding_year\n    }\n\n    MATCH_PLAYER_SUMMARY {\n        text match_player_summary_id PK\n        text match_id FK\n        text player_id FK\n        text team_id FK\n        int minutes_played\n        int goals\n        int assists\n        int shots\n        int shots_on_target\n        real xg\n        real npxg\n        real xag\n        int passes_completed\n        int passes_attempted\n        int progressive_passes\n        int tackles\n        int interceptions\n        int blocks\n        text position\n        text age\n    }\n\n    MATCH_TEAM {\n        text match_team_id PK\n        text match_id FK\n        text team_id FK\n        int goals\n        real possession_pct\n        real passing_acc_pct\n        int tackles\n        int interceptions\n        int fouls\n        int corners\n        text result\n    }</code></pre>"},{"location":"data/database-overview/#core-tables","title":"Core Tables","text":""},{"location":"data/database-overview/#match_player_summary","title":"match_player_summary","text":"<p>The heart of our analytics platform - individual player performance data</p> <p>Key Statistics (35+ fields):</p>"},{"location":"data/database-overview/#basic-performance","title":"Basic Performance","text":"Field Type Description <code>goals</code> INTEGER Goals scored <code>assists</code> INTEGER Assists provided <code>shots</code> INTEGER Total shots taken <code>shots_on_target</code> INTEGER Shots on target <code>minutes_played</code> INTEGER Minutes on the field"},{"location":"data/database-overview/#advanced-metrics-2019-seasons","title":"Advanced Metrics (2019+ seasons)","text":"Field Type Description <code>xg</code> REAL Expected Goals generated <code>npxg</code> REAL Non-penalty Expected Goals <code>xag</code> REAL Expected Assists generated <code>sca</code> INTEGER Shot Creation Actions <code>gca</code> INTEGER Goal Creation Actions"},{"location":"data/database-overview/#passing-distribution","title":"Passing &amp; Distribution","text":"Field Type Description <code>passes_completed</code> INTEGER Successful passes <code>passes_attempted</code> INTEGER Total passes attempted <code>pass_completion_pct</code> REAL Pass success percentage <code>progressive_passes</code> INTEGER Passes advancing toward goal"},{"location":"data/database-overview/#defensive-actions","title":"Defensive Actions","text":"Field Type Description <code>tackles</code> INTEGER Tackles won <code>interceptions</code> INTEGER Passes intercepted <code>blocks</code> INTEGER Shots/passes blocked <code>touches</code> INTEGER Total ball touches"},{"location":"data/database-overview/#context-information","title":"Context Information","text":"Field Type Description <code>position</code> TEXT Playing position <code>age</code> TEXT Player age at match time <code>shirt_number</code> INTEGER Jersey number worn"},{"location":"data/database-overview/#match_team","title":"match_team","text":"<p>Team-level performance for each match</p> Field Type Description <code>goals</code> INTEGER Goals scored by team <code>possession_pct</code> REAL Possession percentage <code>passing_acc_pct</code> REAL Overall passing accuracy <code>tackles</code> INTEGER Team total tackles <code>interceptions</code> INTEGER Team total interceptions <code>fouls</code> INTEGER Fouls committed <code>corners</code> INTEGER Corner kicks earned <code>result</code> TEXT Match result (W/D/L)"},{"location":"data/database-overview/#match","title":"match","text":"<p>Match context and metadata</p> Field Type Description <code>match_id</code> TEXT Unique match identifier <code>season_id</code> TEXT Season reference <code>match_date</code> DATE Date of match <code>venue</code> TEXT Stadium/location <code>competition_type</code> TEXT Regular season/playoff"},{"location":"data/database-overview/#player","title":"player","text":"<p>Player identity and biographical information</p> Field Type Description <code>player_id</code> TEXT Unique player identifier <code>player_name</code> TEXT Full player name <code>nationality</code> TEXT Player nationality <code>primary_position</code> TEXT Main playing position"},{"location":"data/database-overview/#team","title":"team","text":"<p>Team information and naming variations</p> Field Type Description <code>team_id</code> TEXT Unique team identifier <code>team_name_1</code> TEXT Primary team name <code>team_name_2</code> TEXT Alternative name <code>team_name_3</code> TEXT Historical name <code>team_name_4</code> TEXT Abbreviated name <code>city</code> TEXT Team location"},{"location":"data/database-overview/#database-statistics","title":"Database Statistics","text":""},{"location":"data/database-overview/#record-counts","title":"Record Counts","text":"Table Records Coverage match_player_summary 42,572 Individual player performances match 1,563 Total matches match_team 3,126 Team performances (2 per match) player ~1,500 Unique players team ~25 Teams across all seasons season 13 Complete seasons"},{"location":"data/database-overview/#data-quality-metrics","title":"Data Quality Metrics","text":"Metric Value Description Overall Completion 99.38% Records with complete statistical data Player Coverage 97.2% Players with full statistical profiles Match Coverage 100% Matches with result and context data Advanced Stats Coverage 85.4% Records with xG and detailed metrics"},{"location":"data/database-overview/#query-optimization","title":"Query Optimization","text":""},{"location":"data/database-overview/#primary-indexes","title":"Primary Indexes","text":"<p>Strategic indexing for analytical performance:</p> <pre><code>-- Player performance queries\nCREATE INDEX idx_player_season ON match_player_summary(player_name, season_id);\n\n-- Team analysis queries  \nCREATE INDEX idx_team_season ON match_player_summary(team_id, season_id);\n\n-- Match analysis queries\nCREATE INDEX idx_match_date ON match(match_date, season_id);\n\n-- Cross-table joins\nCREATE INDEX idx_match_player_join ON match_player_summary(match_id);\n</code></pre>"},{"location":"data/database-overview/#common-query-patterns","title":"Common Query Patterns","text":""},{"location":"data/database-overview/#player-season-analysis","title":"Player Season Analysis","text":"<pre><code>SELECT player_name, \n       SUM(goals) as total_goals,\n       SUM(assists) as total_assists,\n       COUNT(*) as matches_played,\n       AVG(minutes_played) as avg_minutes\nFROM match_player_summary mp\nJOIN match m ON mp.match_id = m.match_id\nWHERE mp.player_name LIKE '%Player Name%' \n  AND m.season_id = '2024'\nGROUP BY player_name;\n</code></pre>"},{"location":"data/database-overview/#team-performance-comparison","title":"Team Performance Comparison","text":"<pre><code>SELECT t.team_name_1,\n       COUNT(*) as matches,\n       AVG(mt.goals) as avg_goals,\n       AVG(mt.possession_pct) as avg_possession\nFROM match_team mt\nJOIN match m ON mt.match_id = m.match_id\nJOIN team t ON mt.team_id = t.team_id\nWHERE m.season_id = '2024'\nGROUP BY t.team_id, t.team_name_1\nORDER BY avg_goals DESC;\n</code></pre>"},{"location":"data/database-overview/#advanced-analytics-base-query","title":"Advanced Analytics Base Query","text":"<pre><code>SELECT mp.player_name,\n       -- NIR Components\n       SUM(mp.goals + mp.assists) as attacking_base,\n       SUM(mp.tackles + mp.interceptions + mp.blocks) as defensive_actions,\n       SUM(mp.progressive_passes) as progression_contribution,\n       SUM(mp.minutes_played) as total_minutes\nFROM match_player_summary mp\nJOIN match m ON mp.match_id = m.match_id\nWHERE m.season_id = ? AND mp.minutes_played &gt; 0\nGROUP BY mp.player_name\nHAVING total_minutes &gt;= 270; -- Minimum 3 matches\n</code></pre>"},{"location":"data/database-overview/#historical-data-handling","title":"Historical Data Handling","text":""},{"location":"data/database-overview/#format-evolution-management","title":"Format Evolution Management","text":"<p>Our database structure accommodates the evolution of NWSL statistical tracking:</p>"},{"location":"data/database-overview/#legacy-format-compatibility-2013-2018","title":"Legacy Format Compatibility (2013-2018)","text":"<ul> <li>NULL handling: Advanced metrics set to NULL for early seasons</li> <li>Age parsing: Custom logic for \"YY-DDD\" format conversion</li> <li>Statistical validation: Different validation rules for limited datasets</li> </ul>"},{"location":"data/database-overview/#modern-format-integration-2019","title":"Modern Format Integration (2019+)","text":"<ul> <li>Full metric support: All 37+ statistical fields populated</li> <li>Enhanced validation: Cross-reference advanced metrics</li> <li>Consistency checking: Ensure metric relationships are logical</li> </ul>"},{"location":"data/database-overview/#data-version-control","title":"Data Version Control","text":""},{"location":"data/database-overview/#historical-accuracy-preservation","title":"Historical Accuracy Preservation","text":"<ul> <li>Original values maintained: Never retroactively adjust historical data</li> <li>Methodology documentation: Clear records of calculation changes</li> <li>Version tracking: Database schema versions and migration history</li> </ul>"},{"location":"data/database-overview/#statistical-definition-evolution","title":"Statistical Definition Evolution","text":"<ul> <li>Clear documentation: When metric definitions change</li> <li>Backward compatibility: Queries work across all seasons</li> <li>Transparent limitations: Acknowledge what can't be compared directly</li> </ul>"},{"location":"data/database-overview/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"data/database-overview/#query-response-times","title":"Query Response Times","text":"<p>Optimized for analytical workloads:</p> Query Type Typical Response Description Single player season &lt;50ms Individual performance analysis Team season analysis &lt;100ms Complete team statistics League-wide aggregation &lt;500ms All players, single season Historical trends &lt;2s Multi-season comparative analysis Complex analytics &lt;5s NIR calculations across seasons"},{"location":"data/database-overview/#storage-optimization","title":"Storage Optimization","text":"<p>Efficient data storage and access:</p> <ul> <li>Compressed statistics: Numeric precision optimized for accuracy vs. space</li> <li>Indexed relationships: Fast joins across normalized tables</li> <li>Partitioned by season: Improved query performance for temporal analysis</li> <li>Cached aggregations: Pre-calculated common analytical queries</li> </ul> <p>Our database schema balances analytical power with historical accuracy, providing the foundation for sophisticated NWSL intelligence.</p>"},{"location":"data/sources/","title":"Data Sources &amp; Collection","text":""},{"location":"data/sources/#primary-data-sources","title":"Primary Data Sources","text":"<p>Our comprehensive NWSL database is built from multiple authoritative sources, each validated and cross-referenced to ensure accuracy and completeness.</p>"},{"location":"data/sources/#fbref-sports-reference","title":"FBRef (Sports Reference)","text":"<p>Primary statistical source for detailed player and team performance data.</p>"},{"location":"data/sources/#what-we-collect","title":"What We Collect","text":"<ul> <li>Individual player statistics for every match</li> <li>Team-level performance metrics</li> <li>Advanced statistics (xG, xA, progressive actions)</li> <li>Detailed passing, shooting, and defensive data</li> </ul>"},{"location":"data/sources/#why-fbref","title":"Why FBRef","text":"<ul> <li>Comprehensive coverage: Most complete statistical tracking available</li> <li>Consistent methodology: Standardized data collection across seasons</li> <li>Advanced metrics: Beyond basic statistics to tactical intelligence</li> <li>Historical depth: Complete coverage back to 2013</li> </ul>"},{"location":"data/sources/#data-processing","title":"Data Processing","text":"<pre><code>FBRef Match Pages \u2192 HTML Parsing \u2192 Statistical Extraction \u2192 Database Integration\n</code></pre> <ul> <li>Custom extraction engines handle format evolution</li> <li>Validation against multiple data points per match</li> <li>Automated consistency checking across seasons</li> </ul>"},{"location":"data/sources/#official-nwsl-records","title":"Official NWSL Records","text":"<p>Secondary validation source for match results and context.</p>"},{"location":"data/sources/#what-we-use","title":"What We Use","text":"<ul> <li>Final scores and match results</li> <li>Team roster information</li> <li>Season structure and playoff formats</li> <li>Official player names and positions</li> </ul>"},{"location":"data/sources/#purpose","title":"Purpose","text":"<ul> <li>Result verification: Ensure statistical accuracy</li> <li>Context validation: Confirm match circumstances</li> <li>Player identification: Standardize names across seasons</li> <li>Timeline verification: Accurate date and season assignment</li> </ul>"},{"location":"data/sources/#historical-archives","title":"Historical Archives","text":"<p>Supplementary sources for comprehensive coverage.</p>"},{"location":"data/sources/#coverage-includes","title":"Coverage Includes","text":"<ul> <li>Early season statistics (2013-2015)</li> <li>Missing match data recovery</li> <li>Player career tracking across teams</li> <li>League structure evolution documentation</li> </ul>"},{"location":"data/sources/#data-collection-process","title":"Data Collection Process","text":""},{"location":"data/sources/#automated-extraction-pipeline","title":"Automated Extraction Pipeline","text":""},{"location":"data/sources/#stage-1-source-identification","title":"Stage 1: Source Identification","text":"<pre><code># Match Discovery Process\n1. Identify all NWSL matches for target season\n2. Generate FBRef URLs for statistical pages\n3. Validate URL accessibility and data availability\n4. Queue matches for processing\n</code></pre>"},{"location":"data/sources/#stage-2-statistical-extraction","title":"Stage 2: Statistical Extraction","text":"<pre><code># Core Processing Engine\n1. Parse HTML match pages using specialized extractors\n2. Extract player-level statistics (35+ fields per player)\n3. Extract team-level aggregated data\n4. Handle format variations across seasons\n</code></pre>"},{"location":"data/sources/#stage-3-data-validation","title":"Stage 3: Data Validation","text":"<pre><code># Quality Assurance Pipeline  \n1. Cross-reference extracted data with known results\n2. Identify statistical outliers for manual review\n3. Validate player name consistency\n4. Check for missing or corrupted data\n</code></pre>"},{"location":"data/sources/#stage-4-database-integration","title":"Stage 4: Database Integration","text":"<pre><code># Storage and Normalization\n1. Normalize player and team identities\n2. Store in relational database structure\n3. Generate completion reports\n4. Update aggregated statistics \n</code></pre>"},{"location":"data/sources/#processing-performance","title":"Processing Performance","text":"<p>Our collection system achieves:</p> <ul> <li>Processing speed: ~35 records per second average</li> <li>Match success rate: 100% for seasons with available HTML files</li> <li>Error rate: &lt;0.1% requiring manual correction</li> <li>Update frequency: New data integrated within 24-48 hours</li> </ul>"},{"location":"data/sources/#data-format-handling","title":"Data Format Handling","text":""},{"location":"data/sources/#evolution-across-seasons","title":"Evolution Across Seasons","text":"<p>The NWSL's statistical tracking has evolved significantly, requiring sophisticated format handling:</p>"},{"location":"data/sources/#2013-2018-legacy-format","title":"2013-2018: Legacy Format","text":"<pre><code>Statistical Fields: 24 per player per match\nFormat Characteristics:\n- Basic performance metrics (goals, assists, shots)\n- Age format: \"YY-DDD\" (year-day of year)  \n- Limited passing and defensive statistics\n- Manual extraction and validation required\n</code></pre>"},{"location":"data/sources/#2019-2025-modern-format","title":"2019-2025: Modern Format","text":"<pre><code>Statistical Fields: 37+ per player per match\nEnhanced Data:\n- Expected Goals (xG) and Expected Assists (xA)\n- Progressive passes and carries\n- Shot creation and goal creation actions\n- Detailed defensive actions and positioning\n</code></pre>"},{"location":"data/sources/#format-compatibility-engine","title":"Format Compatibility Engine","text":"<p>Our system automatically detects and processes both formats:</p> <pre><code>def determine_format(match_data):\n    \"\"\"Automatically detect statistical format\"\"\"\n    if 'xg' in available_fields:\n        return ModernFormatProcessor()\n    else:\n        return LegacyFormatProcessor()\n</code></pre> <p>Benefits: - Seamless processing across all 13 seasons - Consistent output despite input format variations - Historical accuracy maintained across format transitions - Future compatibility for ongoing format evolution</p>"},{"location":"data/sources/#quality-control-measures","title":"Quality Control Measures","text":""},{"location":"data/sources/#automated-validation","title":"Automated Validation","text":""},{"location":"data/sources/#statistical-consistency-checks","title":"Statistical Consistency Checks","text":"<ul> <li>Goals + assists \u2264 total shot creation actions</li> <li>Minutes played \u2264 90 (plus stoppage time allowance)</li> <li>Team totals match individual player sums</li> <li>Pass completion rates within realistic ranges</li> </ul>"},{"location":"data/sources/#cross-match-validation","title":"Cross-Match Validation","text":"<ul> <li>Player appears for only one team per match</li> <li>Season totals aggregate correctly from individual matches</li> <li>Historical player tracking maintains identity consistency</li> </ul>"},{"location":"data/sources/#temporal-validation","title":"Temporal Validation","text":"<ul> <li>Match dates align with official season calendars</li> <li>Player ages progress logically across seasons</li> <li>Team participation consistent with league membership</li> </ul>"},{"location":"data/sources/#manual-review-process","title":"Manual Review Process","text":""},{"location":"data/sources/#statistical-outlier-investigation","title":"Statistical Outlier Investigation","text":"<p>When automated checks identify anomalies: 1. Source verification: Return to original data source 2. Context analysis: Consider match circumstances 3. Historical comparison: Compare to player's typical performance 4. Manual correction: Update database if error confirmed</p>"},{"location":"data/sources/#edge-case-handling","title":"Edge Case Handling","text":"<ul> <li>Player name variations: Standardize across seasons</li> <li>Position changes: Track tactical role evolution</li> <li>Team transfers: Maintain accurate player-team associations</li> <li>Statistical anomalies: Verify extraordinary performances</li> </ul>"},{"location":"data/sources/#data-integrity-monitoring","title":"Data Integrity Monitoring","text":""},{"location":"data/sources/#continuous-monitoring","title":"Continuous Monitoring","text":"<ul> <li>Weekly reports: Completion rates and quality metrics</li> <li>Anomaly detection: Automated flagging of unusual patterns</li> <li>Source monitoring: Track changes in data availability</li> <li>Performance tracking: Processing speed and error rates</li> </ul>"},{"location":"data/sources/#historical-validation","title":"Historical Validation","text":"<ul> <li>Seasonal audits: Complete review of each season's data</li> <li>Cross-reference checking: Validate against multiple sources</li> <li>Longitudinal analysis: Identify trends and inconsistencies</li> <li>Community feedback: Incorporate corrections from users</li> </ul>"},{"location":"data/sources/#data-collection-challenges","title":"Data Collection Challenges","text":""},{"location":"data/sources/#source-reliability-issues","title":"Source Reliability Issues","text":""},{"location":"data/sources/#website-structure-changes","title":"Website Structure Changes","text":"<ul> <li>FBRef occasionally modifies page layouts</li> <li>Requires extraction engine updates</li> <li>Implemented robust parsing with multiple fallback methods</li> </ul>"},{"location":"data/sources/#data-availability-gaps","title":"Data Availability Gaps","text":"<ul> <li>Some early season matches lack detailed statistics</li> <li>Weather and venue data inconsistently available</li> <li>Goalkeeper statistics limited in certain seasons</li> </ul>"},{"location":"data/sources/#format-evolution-complexity","title":"Format Evolution Complexity","text":""},{"location":"data/sources/#statistical-definition-changes","title":"Statistical Definition Changes","text":"<ul> <li>Certain metrics redefined across seasons</li> <li>Requires careful historical interpretation</li> <li>Clear documentation of definitional changes</li> </ul>"},{"location":"data/sources/#league-structure-evolution","title":"League Structure Evolution","text":"<ul> <li>Playoff format changes affect seasonal statistics</li> <li>Team expansion impacts competitive balance</li> <li>COVID-19 season required special handling</li> </ul>"},{"location":"data/sources/#scale-and-performance-considerations","title":"Scale and Performance Considerations","text":""},{"location":"data/sources/#processing-volume","title":"Processing Volume","text":"<ul> <li>42,572+ individual player records</li> <li>1,563 matches across 13 seasons</li> <li>Multiple statistical fields per record</li> </ul>"},{"location":"data/sources/#storage-and-access","title":"Storage and Access","text":"<ul> <li>Efficient database design for fast queries</li> <li>Indexed for common analytical operations</li> <li>Optimized for both historical analysis and real-time updates</li> </ul>"},{"location":"data/sources/#future-enhancements","title":"Future Enhancements","text":""},{"location":"data/sources/#planned-improvements","title":"Planned Improvements","text":""},{"location":"data/sources/#data-source-expansion","title":"Data Source Expansion","text":"<ul> <li>Integration of additional statistical providers</li> <li>Enhanced venue and weather data collection  </li> <li>Player biographical and career information</li> </ul>"},{"location":"data/sources/#real-time-processing","title":"Real-Time Processing","text":"<ul> <li>Live match statistics integration</li> <li>Faster processing pipeline for current season</li> <li>Automated anomaly detection and correction</li> </ul>"},{"location":"data/sources/#enhanced-validation","title":"Enhanced Validation","text":"<ul> <li>Machine learning-based outlier detection</li> <li>Automated cross-source verification</li> <li>Predictive data quality assessment</li> </ul> <p>Our data collection process represents years of refinement, ensuring the most accurate and comprehensive NWSL statistical database available.</p>"},{"location":"mcp%20docs/building_an_mpc_server/","title":"Build an MCP Server","text":"<p>Get started building your own server to use in Claude for Desktop and other clients.</p> <p>In this tutorial, we'll build a simple MCP weather server and connect it to a host, Claude for Desktop. We'll start with a basic setup, and then progress to more complex use cases.</p>"},{"location":"mcp%20docs/building_an_mpc_server/#what-well-be-building","title":"What we'll be building","text":"<p>Many LLMs do not currently have the ability to fetch the forecast and severe weather alerts. Let's use MCP to solve that!</p> <p>We'll build a server that exposes two tools: <code>get_alerts</code> and <code>get_forecast</code>. Then we'll connect the server to an MCP host (in this case, Claude for Desktop):</p> <p> </p> <p> </p> <p>   Servers can connect to any client. We've chosen Claude for Desktop here for simplicity, but we also have guides on building your own client as well as a list of other clients here. </p>"},{"location":"mcp%20docs/building_an_mpc_server/#core-mcp-concepts","title":"Core MCP Concepts","text":"<p>MCP servers can provide three main types of capabilities:</p> <ol> <li>Resources: File-like data that can be read by clients (like API responses or file contents)</li> <li>Tools: Functions that can be called by the LLM (with user approval)</li> <li>Prompts: Pre-written templates that help users accomplish specific tasks</li> </ol> <p>This tutorial will primarily focus on tools.</p> <p>      Let's get started with building our weather server! You can find the complete code for what we'll be building here. <pre><code>### Prerequisite knowledge\n\nThis quickstart assumes you have familiarity with:\n\n* Python\n* LLMs like Claude\n\n### Logging in MCP Servers\n\nWhen implementing MCP servers, be careful about how you handle logging:\n\n**For STDIO-based servers:** Never write to standard output (stdout). This includes:\n\n* `print()` statements in Python\n* `console.log()` in JavaScript\n* `fmt.Println()` in Go\n* Similar stdout functions in other languages\n\nWriting to stdout will corrupt the JSON-RPC messages and break your server.\n\n**For HTTP-based servers:** Standard output logging is fine since it doesn't interfere with HTTP responses.\n\n### Best Practices\n\n1. Use a logging library that writes to stderr or files.\n\n### Quick Examples\n\n```python\n# \u274c Bad (STDIO)\nprint(\"Processing request\")\n\n# \u2705 Good (STDIO)\nimport logging\nlogging.info(\"Processing request\")\n```\n\n### System requirements\n\n* Python 3.10 or higher installed.\n* You must use the Python MCP SDK 1.2.0 or higher.\n\n### Set up your environment\n\nFirst, let's install `uv` and set up our Python project and environment:\n\n&lt;CodeGroup&gt;\n  ```bash MacOS/Linux\n  curl -LsSf https://astral.sh/uv/install.sh | sh\n  ```\n\n  ```powershell Windows\n  powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n  ```\n&lt;/CodeGroup&gt;\n\nMake sure to restart your terminal afterwards to ensure that the `uv` command gets picked up.\n\nNow, let's create and set up our project:\n\n&lt;CodeGroup&gt;\n  ```bash MacOS/Linux\n  # Create a new directory for our project\n  uv init weather\n  cd weather\n\n  # Create virtual environment and activate it\n  uv venv\n  source .venv/bin/activate\n\n  # Install dependencies\n  uv add \"mcp[cli]\" httpx\n\n  # Create our server file\n  touch weather.py\n  ```\n\n  ```powershell Windows\n  # Create a new directory for our project\n  uv init weather\n  cd weather\n\n  # Create virtual environment and activate it\n  uv venv\n  .venv\\Scripts\\activate\n\n  # Install dependencies\n  uv add mcp[cli] httpx\n\n  # Create our server file\n  new-item weather.py\n  ```\n&lt;/CodeGroup&gt;\n\nNow let's dive into building your server.\n\n## Building your server\n\n### Importing packages and setting up the instance\n\nAdd these to the top of your `weather.py`:\n\n```python\nfrom typing import Any\nimport httpx\nfrom mcp.server.fastmcp import FastMCP\n\n# Initialize FastMCP server\nmcp = FastMCP(\"weather\")\n\n# Constants\nNWS_API_BASE = \"https://api.weather.gov\"\nUSER_AGENT = \"weather-app/1.0\"\n```\n\nThe FastMCP class uses Python type hints and docstrings to automatically generate tool definitions, making it easy to create and maintain MCP tools.\n\n### Helper functions\n\nNext, let's add our helper functions for querying and formatting the data from the National Weather Service API:\n\n```python\nasync def make_nws_request(url: str) -&gt; dict[str, Any] | None:\n    \"\"\"Make a request to the NWS API with proper error handling.\"\"\"\n    headers = {\n        \"User-Agent\": USER_AGENT,\n        \"Accept\": \"application/geo+json\"\n    }\n    async with httpx.AsyncClient() as client:\n        try:\n            response = await client.get(url, headers=headers, timeout=30.0)\n            response.raise_for_status()\n            return response.json()\n        except Exception:\n            return None\n\ndef format_alert(feature: dict) -&gt; str:\n    \"\"\"Format an alert feature into a readable string.\"\"\"\n    props = feature[\"properties\"]\n    return f\"\"\"\nEvent: {props.get('event', 'Unknown')}\nArea: {props.get('areaDesc', 'Unknown')}\nSeverity: {props.get('severity', 'Unknown')}\nDescription: {props.get('description', 'No description available')}\nInstructions: {props.get('instruction', 'No specific instructions provided')}\n\"\"\"\n```\n\n### Implementing tool execution\n\nThe tool execution handler is responsible for actually executing the logic of each tool. Let's add it:\n\n```python\n@mcp.tool()\nasync def get_alerts(state: str) -&gt; str:\n    \"\"\"Get weather alerts for a US state.\n\n    Args:\n        state: Two-letter US state code (e.g. CA, NY)\n    \"\"\"\n    url = f\"{NWS_API_BASE}/alerts/active/area/{state}\"\n    data = await make_nws_request(url)\n\n    if not data or \"features\" not in data:\n        return \"Unable to fetch alerts or no alerts found.\"\n\n    if not data[\"features\"]:\n        return \"No active alerts for this state.\"\n\n    alerts = [format_alert(feature) for feature in data[\"features\"]]\n    return \"\\n---\\n\".join(alerts)\n\n@mcp.tool()\nasync def get_forecast(latitude: float, longitude: float) -&gt; str:\n    \"\"\"Get weather forecast for a location.\n\n    Args:\n        latitude: Latitude of the location\n        longitude: Longitude of the location\n    \"\"\"\n    # First get the forecast grid endpoint\n    points_url = f\"{NWS_API_BASE}/points/{latitude},{longitude}\"\n    points_data = await make_nws_request(points_url)\n\n    if not points_data:\n        return \"Unable to fetch forecast data for this location.\"\n\n    # Get the forecast URL from the points response\n    forecast_url = points_data[\"properties\"][\"forecast\"]\n    forecast_data = await make_nws_request(forecast_url)\n\n    if not forecast_data:\n        return \"Unable to fetch detailed forecast.\"\n\n    # Format the periods into a readable forecast\n    periods = forecast_data[\"properties\"][\"periods\"]\n    forecasts = []\n    for period in periods[:5]:  # Only show next 5 periods\n        forecast = f\"\"\"\n{period['name']}:\nTemperature: {period['temperature']}\u00b0{period['temperatureUnit']}\nWind: {period['windSpeed']} {period['windDirection']}\nForecast: {period['detailedForecast']}\n\"\"\"\n        forecasts.append(forecast)\n\n    return \"\\n---\\n\".join(forecasts)\n```\n\n### Running the server\n\nFinally, let's initialize and run the server:\n\n```python\nif __name__ == \"__main__\":\n    # Initialize and run the server\n    mcp.run(transport='stdio')\n```\n\nYour server is complete! Run `uv run weather.py` to start the MCP server, which will listen for messages from MCP hosts.\n\nLet's now test your server from an existing MCP host, Claude for Desktop.\n\n## Testing your server with Claude for Desktop\n\n&lt;Note&gt;\n  Claude for Desktop is not yet available on Linux. Linux users can proceed to the [Building a client](/quickstart/client) tutorial to build an MCP client that connects to the server we just built.\n&lt;/Note&gt;\n\nFirst, make sure you have Claude for Desktop installed. [You can install the latest version\nhere.](https://claude.ai/download) If you already have Claude for Desktop, **make sure it's updated to the latest version.**\n\nWe'll need to configure Claude for Desktop for whichever MCP servers you want to use. To do this, open your Claude for Desktop App configuration at `~/Library/Application Support/Claude/claude_desktop_config.json` in a text editor. Make sure to create the file if it doesn't exist.\n\nFor example, if you have [VS Code](https://code.visualstudio.com/) installed:\n\n&lt;CodeGroup&gt;\n  ```bash MacOS/Linux\n  code ~/Library/Application\\ Support/Claude/claude_desktop_config.json\n  ```\n\n  ```powershell Windows\n  code $env:AppData\\Claude\\claude_desktop_config.json\n  ```\n&lt;/CodeGroup&gt;\n\nYou'll then add your servers in the `mcpServers` key. The MCP UI elements will only show up in Claude for Desktop if at least one server is properly configured.\n\nIn this case, we'll add our single weather server like so:\n\n&lt;CodeGroup&gt;\n  ```json MacOS/Linux\n  {\n    \"mcpServers\": {\n      \"weather\": {\n        \"command\": \"uv\",\n        \"args\": [\n          \"--directory\",\n          \"/ABSOLUTE/PATH/TO/PARENT/FOLDER/weather\",\n          \"run\",\n          \"weather.py\"\n        ]\n      }\n    }\n  }\n  ```\n\n  ```json Windows\n  {\n    \"mcpServers\": {\n      \"weather\": {\n        \"command\": \"uv\",\n        \"args\": [\n          \"--directory\",\n          \"C:\\\\ABSOLUTE\\\\PATH\\\\TO\\\\PARENT\\\\FOLDER\\\\weather\",\n          \"run\",\n          \"weather.py\"\n        ]\n      }\n    }\n  }\n  ```\n&lt;/CodeGroup&gt;\n\n&lt;Warning&gt;\n  You may need to put the full path to the `uv` executable in the `command` field. You can get this by running `which uv` on MacOS/Linux or `where uv` on Windows.\n&lt;/Warning&gt;\n\n&lt;Note&gt;\n  Make sure you pass in the absolute path to your server. You can get this by running `pwd` on MacOS/Linux or `cd` on Windows Command Prompt. On Windows, remember to use double backslashes (`\\\\`) or forward slashes (`/`) in the JSON path.\n&lt;/Note&gt;\n\nThis tells Claude for Desktop:\n\n1. There's an MCP server named \"weather\"\n2. To launch it by running `uv --directory /ABSOLUTE/PATH/TO/PARENT/FOLDER/weather run weather.py`\n\nSave the file, and restart **Claude for Desktop**.\n</code></pre> <p></p> <p>     Let's get started with building our weather server! You can find the complete code for what we'll be building here. <pre><code>### Prerequisite knowledge\n\nThis quickstart assumes you have familiarity with:\n\n* TypeScript\n* LLMs like Claude\n\n### Logging in MCP Servers\n\nWhen implementing MCP servers, be careful about how you handle logging:\n\n**For STDIO-based servers:** Never write to standard output (stdout). This includes:\n\n* `print()` statements in Python\n* `console.log()` in JavaScript\n* `fmt.Println()` in Go\n* Similar stdout functions in other languages\n\nWriting to stdout will corrupt the JSON-RPC messages and break your server.\n\n**For HTTP-based servers:** Standard output logging is fine since it doesn't interfere with HTTP responses.\n\n### Best Practices\n\n1. Use a logging library that writes to stderr or files, such as `logging` in Python.\n2. For JavaScript, be especially careful - `console.log()` writes to stdout by default\n\n### Quick Examples\n\n```javascript\n// \u274c Bad (STDIO)\nconsole.log(\"Server started\");\n\n// \u2705 Good (STDIO)\nconsole.error(\"Server started\"); // stderr is safe\n```\n\n### System requirements\n\nFor TypeScript, make sure you have the latest version of Node installed.\n\n### Set up your environment\n\nFirst, let's install Node.js and npm if you haven't already. You can download them from [nodejs.org](https://nodejs.org/).\nVerify your Node.js installation:\n\n```bash\nnode --version\nnpm --version\n```\n\nFor this tutorial, you'll need Node.js version 16 or higher.\n\nNow, let's create and set up our project:\n\n&lt;CodeGroup&gt;\n  ```bash MacOS/Linux\n  # Create a new directory for our project\n  mkdir weather\n  cd weather\n\n  # Initialize a new npm project\n  npm init -y\n\n  # Install dependencies\n  npm install @modelcontextprotocol/sdk zod\n  npm install -D @types/node typescript\n\n  # Create our files\n  mkdir src\n  touch src/index.ts\n  ```\n\n  ```powershell Windows\n  # Create a new directory for our project\n  md weather\n  cd weather\n\n  # Initialize a new npm project\n  npm init -y\n\n  # Install dependencies\n  npm install @modelcontextprotocol/sdk zod\n  npm install -D @types/node typescript\n\n  # Create our files\n  md src\n  new-item src\\index.ts\n  ```\n&lt;/CodeGroup&gt;\n\nUpdate your package.json to add type: \"module\" and a build script:\n\n```json package.json\n{\n  \"type\": \"module\",\n  \"bin\": {\n    \"weather\": \"./build/index.js\"\n  },\n  \"scripts\": {\n    \"build\": \"tsc &amp;&amp; chmod 755 build/index.js\"\n  },\n  \"files\": [\"build\"]\n}\n```\n\nCreate a `tsconfig.json` in the root of your project:\n\n```json tsconfig.json\n{\n  \"compilerOptions\": {\n    \"target\": \"ES2022\",\n    \"module\": \"Node16\",\n    \"moduleResolution\": \"Node16\",\n    \"outDir\": \"./build\",\n    \"rootDir\": \"./src\",\n    \"strict\": true,\n    \"esModuleInterop\": true,\n    \"skipLibCheck\": true,\n    \"forceConsistentCasingInFileNames\": true\n  },\n  \"include\": [\"src/**/*\"],\n  \"exclude\": [\"node_modules\"]\n}\n```\n\nNow let's dive into building your server.\n\n## Building your server\n\n### Importing packages and setting up the instance\n\nAdd these to the top of your `src/index.ts`:\n\n```typescript\nimport { McpServer } from \"@modelcontextprotocol/sdk/server/mcp.js\";\nimport { StdioServerTransport } from \"@modelcontextprotocol/sdk/server/stdio.js\";\nimport { z } from \"zod\";\n\nconst NWS_API_BASE = \"https://api.weather.gov\";\nconst USER_AGENT = \"weather-app/1.0\";\n\n// Create server instance\nconst server = new McpServer({\n  name: \"weather\",\n  version: \"1.0.0\",\n  capabilities: {\n    resources: {},\n    tools: {},\n  },\n});\n```\n\n### Helper functions\n\nNext, let's add our helper functions for querying and formatting the data from the National Weather Service API:\n\n```typescript\n// Helper function for making NWS API requests\nasync function makeNWSRequest&lt;T&gt;(url: string): Promise&lt;T | null&gt; {\n  const headers = {\n    \"User-Agent\": USER_AGENT,\n    Accept: \"application/geo+json\",\n  };\n\n  try {\n    const response = await fetch(url, { headers });\n    if (!response.ok) {\n      throw new Error(`HTTP error! status: ${response.status}`);\n    }\n    return (await response.json()) as T;\n  } catch (error) {\n    console.error(\"Error making NWS request:\", error);\n    return null;\n  }\n}\n\ninterface AlertFeature {\n  properties: {\n    event?: string;\n    areaDesc?: string;\n    severity?: string;\n    status?: string;\n    headline?: string;\n  };\n}\n\n// Format alert data\nfunction formatAlert(feature: AlertFeature): string {\n  const props = feature.properties;\n  return [\n    `Event: ${props.event || \"Unknown\"}`,\n    `Area: ${props.areaDesc || \"Unknown\"}`,\n    `Severity: ${props.severity || \"Unknown\"}`,\n    `Status: ${props.status || \"Unknown\"}`,\n    `Headline: ${props.headline || \"No headline\"}`,\n    \"---\",\n  ].join(\"\\n\");\n}\n\ninterface ForecastPeriod {\n  name?: string;\n  temperature?: number;\n  temperatureUnit?: string;\n  windSpeed?: string;\n  windDirection?: string;\n  shortForecast?: string;\n}\n\ninterface AlertsResponse {\n  features: AlertFeature[];\n}\n\ninterface PointsResponse {\n  properties: {\n    forecast?: string;\n  };\n}\n\ninterface ForecastResponse {\n  properties: {\n    periods: ForecastPeriod[];\n  };\n}\n```\n\n### Implementing tool execution\n\nThe tool execution handler is responsible for actually executing the logic of each tool. Let's add it:\n\n```typescript\n// Register weather tools\nserver.tool(\n  \"get_alerts\",\n  \"Get weather alerts for a state\",\n  {\n    state: z.string().length(2).describe(\"Two-letter state code (e.g. CA, NY)\"),\n  },\n  async ({ state }) =&gt; {\n    const stateCode = state.toUpperCase();\n    const alertsUrl = `${NWS_API_BASE}/alerts?area=${stateCode}`;\n    const alertsData = await makeNWSRequest&lt;AlertsResponse&gt;(alertsUrl);\n\n    if (!alertsData) {\n      return {\n        content: [\n          {\n            type: \"text\",\n            text: \"Failed to retrieve alerts data\",\n          },\n        ],\n      };\n    }\n\n    const features = alertsData.features || [];\n    if (features.length === 0) {\n      return {\n        content: [\n          {\n            type: \"text\",\n            text: `No active alerts for ${stateCode}`,\n          },\n        ],\n      };\n    }\n\n    const formattedAlerts = features.map(formatAlert);\n    const alertsText = `Active alerts for ${stateCode}:\\n\\n${formattedAlerts.join(\"\\n\")}`;\n\n    return {\n      content: [\n        {\n          type: \"text\",\n          text: alertsText,\n        },\n      ],\n    };\n  },\n);\n\nserver.tool(\n  \"get_forecast\",\n  \"Get weather forecast for a location\",\n  {\n    latitude: z.number().min(-90).max(90).describe(\"Latitude of the location\"),\n    longitude: z\n      .number()\n      .min(-180)\n      .max(180)\n      .describe(\"Longitude of the location\"),\n  },\n  async ({ latitude, longitude }) =&gt; {\n    // Get grid point data\n    const pointsUrl = `${NWS_API_BASE}/points/${latitude.toFixed(4)},${longitude.toFixed(4)}`;\n    const pointsData = await makeNWSRequest&lt;PointsResponse&gt;(pointsUrl);\n\n    if (!pointsData) {\n      return {\n        content: [\n          {\n            type: \"text\",\n            text: `Failed to retrieve grid point data for coordinates: ${latitude}, ${longitude}. This location may not be supported by the NWS API (only US locations are supported).`,\n          },\n        ],\n      };\n    }\n\n    const forecastUrl = pointsData.properties?.forecast;\n    if (!forecastUrl) {\n      return {\n        content: [\n          {\n            type: \"text\",\n            text: \"Failed to get forecast URL from grid point data\",\n          },\n        ],\n      };\n    }\n\n    // Get forecast data\n    const forecastData = await makeNWSRequest&lt;ForecastResponse&gt;(forecastUrl);\n    if (!forecastData) {\n      return {\n        content: [\n          {\n            type: \"text\",\n            text: \"Failed to retrieve forecast data\",\n          },\n        ],\n      };\n    }\n\n    const periods = forecastData.properties?.periods || [];\n    if (periods.length === 0) {\n      return {\n        content: [\n          {\n            type: \"text\",\n            text: \"No forecast periods available\",\n          },\n        ],\n      };\n    }\n\n    // Format forecast periods\n    const formattedForecast = periods.map((period: ForecastPeriod) =&gt;\n      [\n        `${period.name || \"Unknown\"}:`,\n        `Temperature: ${period.temperature || \"Unknown\"}\u00b0${period.temperatureUnit || \"F\"}`,\n        `Wind: ${period.windSpeed || \"Unknown\"} ${period.windDirection || \"\"}`,\n        `${period.shortForecast || \"No forecast available\"}`,\n        \"---\",\n      ].join(\"\\n\"),\n    );\n\n    const forecastText = `Forecast for ${latitude}, ${longitude}:\\n\\n${formattedForecast.join(\"\\n\")}`;\n\n    return {\n      content: [\n        {\n          type: \"text\",\n          text: forecastText,\n        },\n      ],\n    };\n  },\n);\n```\n\n### Running the server\n\nFinally, implement the main function to run the server:\n\n```typescript\nasync function main() {\n  const transport = new StdioServerTransport();\n  await server.connect(transport);\n  console.error(\"Weather MCP Server running on stdio\");\n}\n\nmain().catch((error) =&gt; {\n  console.error(\"Fatal error in main():\", error);\n  process.exit(1);\n});\n```\n\nMake sure to run `npm run build` to build your server! This is a very important step in getting your server to connect.\n\nLet's now test your server from an existing MCP host, Claude for Desktop.\n\n## Testing your server with Claude for Desktop\n\n&lt;Note&gt;\n  Claude for Desktop is not yet available on Linux. Linux users can proceed to the [Building a client](/quickstart/client) tutorial to build an MCP client that connects to the server we just built.\n&lt;/Note&gt;\n\nFirst, make sure you have Claude for Desktop installed. [You can install the latest version\nhere.](https://claude.ai/download) If you already have Claude for Desktop, **make sure it's updated to the latest version.**\n\nWe'll need to configure Claude for Desktop for whichever MCP servers you want to use. To do this, open your Claude for Desktop App configuration at `~/Library/Application Support/Claude/claude_desktop_config.json` in a text editor. Make sure to create the file if it doesn't exist.\n\nFor example, if you have [VS Code](https://code.visualstudio.com/) installed:\n\n&lt;CodeGroup&gt;\n  ```bash MacOS/Linux\n  code ~/Library/Application\\ Support/Claude/claude_desktop_config.json\n  ```\n\n  ```powershell Windows\n  code $env:AppData\\Claude\\claude_desktop_config.json\n  ```\n&lt;/CodeGroup&gt;\n\nYou'll then add your servers in the `mcpServers` key. The MCP UI elements will only show up in Claude for Desktop if at least one server is properly configured.\n\nIn this case, we'll add our single weather server like so:\n\n&lt;CodeGroup&gt;\n  ```json MacOS/Linux\n  {\n    \"mcpServers\": {\n      \"weather\": {\n        \"command\": \"node\",\n        \"args\": [\"/ABSOLUTE/PATH/TO/PARENT/FOLDER/weather/build/index.js\"]\n      }\n    }\n  }\n  ```\n\n  ```json Windows\n  {\n    \"mcpServers\": {\n      \"weather\": {\n        \"command\": \"node\",\n        \"args\": [\"C:\\\\PATH\\\\TO\\\\PARENT\\\\FOLDER\\\\weather\\\\build\\\\index.js\"]\n      }\n    }\n  }\n  ```\n&lt;/CodeGroup&gt;\n\nThis tells Claude for Desktop:\n\n1. There's an MCP server named \"weather\"\n2. Launch it by running `node /ABSOLUTE/PATH/TO/PARENT/FOLDER/weather/build/index.js`\n\nSave the file, and restart **Claude for Desktop**.\n</code></pre> <p></p> <p>        This is a quickstart demo based on Spring AI MCP auto-configuration and boot starters.       To learn how to create sync and async MCP Servers, manually, consult the Java SDK Server documentation.      <pre><code>Let's get started with building our weather server!\n[You can find the complete code for what we'll be building here.](https://github.com/spring-projects/spring-ai-examples/tree/main/model-context-protocol/weather/starter-stdio-server)\n\nFor more information, see the [MCP Server Boot Starter](https://docs.spring.io/spring-ai/reference/api/mcp/mcp-server-boot-starter-docs.html) reference documentation.\nFor manual MCP Server implementation, refer to the [MCP Server Java SDK documentation](/sdk/java/mcp-server).\n\n### Logging in MCP Servers\n\nWhen implementing MCP servers, be careful about how you handle logging:\n\n**For STDIO-based servers:** Never write to standard output (stdout). This includes:\n\n* `print()` statements in Python\n* `console.log()` in JavaScript\n* `fmt.Println()` in Go\n* Similar stdout functions in other languages\n\nWriting to stdout will corrupt the JSON-RPC messages and break your server.\n\n**For HTTP-based servers:** Standard output logging is fine since it doesn't interfere with HTTP responses.\n\n### Best Practices\n\n1. Use a logging library that writes to stderr or files.\n2. Ensure any configured logging library will not write to STDOUT\n\n### System requirements\n\n* Java 17 or higher installed.\n* [Spring Boot 3.3.x](https://docs.spring.io/spring-boot/installing.html) or higher\n\n### Set up your environment\n\nUse the [Spring Initializer](https://start.spring.io/) to bootstrap the project.\n\nYou will need to add the following dependencies:\n\n&lt;CodeGroup&gt;\n  ```xml Maven\n  &lt;dependencies&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.ai&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-ai-starter-mcp-server&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-web&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n  &lt;/dependencies&gt;\n  ```\n\n  ```groovy Gradle\n  dependencies {\n    implementation platform(\"org.springframework.ai:spring-ai-starter-mcp-server\")\n    implementation platform(\"org.springframework:spring-web\")\n  }\n  ```\n&lt;/CodeGroup&gt;\n\nThen configure your application by setting the application properties:\n\n&lt;CodeGroup&gt;\n  ```bash application.properties\n  spring.main.bannerMode=off\n  logging.pattern.console=\n  ```\n\n  ```yaml application.yml\n  logging:\n    pattern:\n      console:\n  spring:\n    main:\n      banner-mode: off\n  ```\n&lt;/CodeGroup&gt;\n\nThe [Server Configuration Properties](https://docs.spring.io/spring-ai/reference/api/mcp/mcp-server-boot-starter-docs.html#_configuration_properties) documents all available properties.\n\nNow let's dive into building your server.\n\n## Building your server\n\n### Weather Service\n\nLet's implement a [WeatherService.java](https://github.com/spring-projects/spring-ai-examples/blob/main/model-context-protocol/weather/starter-stdio-server/src/main/java/org/springframework/ai/mcp/sample/server/WeatherService.java) that uses a REST client to query the data from the National Weather Service API:\n\n```java\n@Service\npublic class WeatherService {\n\n    private final RestClient restClient;\n\n    public WeatherService() {\n        this.restClient = RestClient.builder()\n            .baseUrl(\"https://api.weather.gov\")\n            .defaultHeader(\"Accept\", \"application/geo+json\")\n            .defaultHeader(\"User-Agent\", \"WeatherApiClient/1.0 (your@email.com)\")\n            .build();\n    }\n\n  @Tool(description = \"Get weather forecast for a specific latitude/longitude\")\n  public String getWeatherForecastByLocation(\n      double latitude,   // Latitude coordinate\n      double longitude   // Longitude coordinate\n  ) {\n      // Returns detailed forecast including:\n      // - Temperature and unit\n      // - Wind speed and direction\n      // - Detailed forecast description\n  }\n\n  @Tool(description = \"Get weather alerts for a US state\")\n  public String getAlerts(\n      @ToolParam(description = \"Two-letter US state code (e.g. CA, NY)\") String state\n  ) {\n      // Returns active alerts including:\n      // - Event type\n      // - Affected area\n      // - Severity\n      // - Description\n      // - Safety instructions\n  }\n\n  // ......\n}\n```\n\nThe `@Service` annotation with auto-register the service in your application context.\nThe Spring AI `@Tool` annotation, making it easy to create and maintain MCP tools.\n\nThe auto-configuration will automatically register these tools with the MCP server.\n\n### Create your Boot Application\n\n```java\n@SpringBootApplication\npublic class McpServerApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(McpServerApplication.class, args);\n    }\n\n    @Bean\n    public ToolCallbackProvider weatherTools(WeatherService weatherService) {\n        return  MethodToolCallbackProvider.builder().toolObjects(weatherService).build();\n    }\n}\n```\n\nUses the the `MethodToolCallbackProvider` utils to convert the `@Tools` into actionable callbacks used by the MCP server.\n\n### Running the server\n\nFinally, let's build the server:\n\n```bash\n./mvnw clean install\n```\n\nThis will generate a `mcp-weather-stdio-server-0.0.1-SNAPSHOT.jar` file within the `target` folder.\n\nLet's now test your server from an existing MCP host, Claude for Desktop.\n\n## Testing your server with Claude for Desktop\n\n&lt;Note&gt;\n  Claude for Desktop is not yet available on Linux.\n&lt;/Note&gt;\n\nFirst, make sure you have Claude for Desktop installed.\n[You can install the latest version here.](https://claude.ai/download) If you already have Claude for Desktop, **make sure it's updated to the latest version.**\n\nWe'll need to configure Claude for Desktop for whichever MCP servers you want to use.\nTo do this, open your Claude for Desktop App configuration at `~/Library/Application Support/Claude/claude_desktop_config.json` in a text editor.\nMake sure to create the file if it doesn't exist.\n\nFor example, if you have [VS Code](https://code.visualstudio.com/) installed:\n\n&lt;CodeGroup&gt;\n  ```bash MacOS/Linux\n  code ~/Library/Application\\ Support/Claude/claude_desktop_config.json\n  ```\n\n  ```powershell Windows\n  code $env:AppData\\Claude\\claude_desktop_config.json\n  ```\n&lt;/CodeGroup&gt;\n\nYou'll then add your servers in the `mcpServers` key.\nThe MCP UI elements will only show up in Claude for Desktop if at least one server is properly configured.\n\nIn this case, we'll add our single weather server like so:\n\n&lt;CodeGroup&gt;\n  ```json MacOS/Linux\n  {\n    \"mcpServers\": {\n      \"spring-ai-mcp-weather\": {\n        \"command\": \"java\",\n        \"args\": [\n          \"-Dspring.ai.mcp.server.stdio=true\",\n          \"-jar\",\n          \"/ABSOLUTE/PATH/TO/PARENT/FOLDER/mcp-weather-stdio-server-0.0.1-SNAPSHOT.jar\"\n        ]\n      }\n    }\n  }\n  ```\n\n  ```json Windows\n  {\n    \"mcpServers\": {\n      \"spring-ai-mcp-weather\": {\n        \"command\": \"java\",\n        \"args\": [\n          \"-Dspring.ai.mcp.server.transport=STDIO\",\n          \"-jar\",\n          \"C:\\\\ABSOLUTE\\\\PATH\\\\TO\\\\PARENT\\\\FOLDER\\\\weather\\\\mcp-weather-stdio-server-0.0.1-SNAPSHOT.jar\"\n        ]\n      }\n    }\n  }\n  ```\n&lt;/CodeGroup&gt;\n\n&lt;Note&gt;\n  Make sure you pass in the absolute path to your server.\n&lt;/Note&gt;\n\nThis tells Claude for Desktop:\n\n1. There's an MCP server named \"my-weather-server\"\n2. To launch it by running `java -jar /ABSOLUTE/PATH/TO/PARENT/FOLDER/mcp-weather-stdio-server-0.0.1-SNAPSHOT.jar`\n\nSave the file, and restart **Claude for Desktop**.\n\n## Testing your server with Java client\n\n### Create a MCP Client manually\n\nUse the `McpClient` to connect to the server:\n\n```java\nvar stdioParams = ServerParameters.builder(\"java\")\n  .args(\"-jar\", \"/ABSOLUTE/PATH/TO/PARENT/FOLDER/mcp-weather-stdio-server-0.0.1-SNAPSHOT.jar\")\n  .build();\n\nvar stdioTransport = new StdioClientTransport(stdioParams);\n\nvar mcpClient = McpClient.sync(stdioTransport).build();\n\nmcpClient.initialize();\n\nListToolsResult toolsList = mcpClient.listTools();\n\nCallToolResult weather = mcpClient.callTool(\n  new CallToolRequest(\"getWeatherForecastByLocation\",\n      Map.of(\"latitude\", \"47.6062\", \"longitude\", \"-122.3321\")));\n\nCallToolResult alert = mcpClient.callTool(\n  new CallToolRequest(\"getAlerts\", Map.of(\"state\", \"NY\")));\n\nmcpClient.closeGracefully();\n```\n\n### Use MCP Client Boot Starter\n\nCreate a new boot starter application using the `spring-ai-starter-mcp-client` dependency:\n\n```xml\n&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.ai&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-ai-starter-mcp-client&lt;/artifactId&gt;\n&lt;/dependency&gt;\n```\n\nand set the `spring.ai.mcp.client.stdio.servers-configuration` property to point to your `claude_desktop_config.json`.\nYou can reuse the existing Anthropic Desktop configuration:\n\n```properties\nspring.ai.mcp.client.stdio.servers-configuration=file:PATH/TO/claude_desktop_config.json\n```\n\nWhen you start your client application, the auto-configuration will create, automatically MCP clients from the claude\\_desktop\\_config.json.\n\nFor more information, see the [MCP Client Boot Starters](https://docs.spring.io/spring-ai/reference/api/mcp/mcp-server-boot-client-docs.html) reference documentation.\n\n## More Java MCP Server examples\n\nThe [starter-webflux-server](https://github.com/spring-projects/spring-ai-examples/tree/main/model-context-protocol/weather/starter-webflux-server) demonstrates how to create a MCP server using SSE transport.\nIt showcases how to define and register MCP Tools, Resources, and Prompts, using the Spring Boot's auto-configuration capabilities.\n</code></pre> <p></p> <p>     Let's get started with building our weather server! You can find the complete code for what we'll be building here. <pre><code>### Prerequisite knowledge\n\nThis quickstart assumes you have familiarity with:\n\n* Kotlin\n* LLMs like Claude\n\n### System requirements\n\n* Java 17 or higher installed.\n\n### Set up your environment\n\nFirst, let's install `java` and `gradle` if you haven't already.\nYou can download `java` from [official Oracle JDK website](https://www.oracle.com/java/technologies/downloads/).\nVerify your `java` installation:\n\n```bash\njava --version\n```\n\nNow, let's create and set up your project:\n\n&lt;CodeGroup&gt;\n  ```bash MacOS/Linux\n  # Create a new directory for our project\n  mkdir weather\n  cd weather\n\n  # Initialize a new kotlin project\n  gradle init\n  ```\n\n  ```powershell Windows\n  # Create a new directory for our project\n  md weather\n  cd weather\n\n  # Initialize a new kotlin project\n  gradle init\n  ```\n&lt;/CodeGroup&gt;\n\nAfter running `gradle init`, you will be presented with options for creating your project.\nSelect **Application** as the project type, **Kotlin** as the programming language, and **Java 17** as the Java version.\n\nAlternatively, you can create a Kotlin application using the [IntelliJ IDEA project wizard](https://kotlinlang.org/docs/jvm-get-started.html).\n\nAfter creating the project, add the following dependencies:\n\n&lt;CodeGroup&gt;\n  ```kotlin build.gradle.kts\n  val mcpVersion = \"0.4.0\"\n  val slf4jVersion = \"2.0.9\"\n  val ktorVersion = \"3.1.1\"\n\n  dependencies {\n      implementation(\"io.modelcontextprotocol:kotlin-sdk:$mcpVersion\")\n      implementation(\"org.slf4j:slf4j-nop:$slf4jVersion\")\n      implementation(\"io.ktor:ktor-client-content-negotiation:$ktorVersion\")\n      implementation(\"io.ktor:ktor-serialization-kotlinx-json:$ktorVersion\")\n  }\n  ```\n\n  ```groovy build.gradle\n  def mcpVersion = '0.3.0'\n  def slf4jVersion = '2.0.9'\n  def ktorVersion = '3.1.1'\n\n  dependencies {\n      implementation \"io.modelcontextprotocol:kotlin-sdk:$mcpVersion\"\n      implementation \"org.slf4j:slf4j-nop:$slf4jVersion\"\n      implementation \"io.ktor:ktor-client-content-negotiation:$ktorVersion\"\n      implementation \"io.ktor:ktor-serialization-kotlinx-json:$ktorVersion\"\n  }\n  ```\n&lt;/CodeGroup&gt;\n\nAlso, add the following plugins to your build script:\n\n&lt;CodeGroup&gt;\n  ```kotlin build.gradle.kts\n  plugins {\n      kotlin(\"plugin.serialization\") version \"your_version_of_kotlin\"\n      id(\"com.github.johnrengelman.shadow\") version \"8.1.1\"\n  }\n  ```\n\n  ```groovy build.gradle\n  plugins {\n      id 'org.jetbrains.kotlin.plugin.serialization' version 'your_version_of_kotlin'\n      id 'com.github.johnrengelman.shadow' version '8.1.1'\n  }\n  ```\n&lt;/CodeGroup&gt;\n\nNow let\u2019s dive into building your server.\n\n## Building your server\n\n### Setting up the instance\n\nAdd a server initialization function:\n\n```kotlin\n// Main function to run the MCP server\nfun `run mcp server`() {\n    // Create the MCP Server instance with a basic implementation\n    val server = Server(\n        Implementation(\n            name = \"weather\", // Tool name is \"weather\"\n            version = \"1.0.0\" // Version of the implementation\n        ),\n        ServerOptions(\n            capabilities = ServerCapabilities(tools = ServerCapabilities.Tools(listChanged = true))\n        )\n    )\n\n    // Create a transport using standard IO for server communication\n    val transport = StdioServerTransport(\n        System.`in`.asInput(),\n        System.out.asSink().buffered()\n    )\n\n    runBlocking {\n        server.connect(transport)\n        val done = Job()\n        server.onClose {\n            done.complete()\n        }\n        done.join()\n    }\n}\n```\n\n### Weather API helper functions\n\nNext, let's add functions and data classes for querying and converting responses from the National Weather Service API:\n\n```kotlin\n// Extension function to fetch forecast information for given latitude and longitude\nsuspend fun HttpClient.getForecast(latitude: Double, longitude: Double): List&lt;String&gt; {\n    val points = this.get(\"/points/$latitude,$longitude\").body&lt;Points&gt;()\n    val forecast = this.get(points.properties.forecast).body&lt;Forecast&gt;()\n    return forecast.properties.periods.map { period -&gt;\n        \"\"\"\n            ${period.name}:\n            Temperature: ${period.temperature} ${period.temperatureUnit}\n            Wind: ${period.windSpeed} ${period.windDirection}\n            Forecast: ${period.detailedForecast}\n        \"\"\".trimIndent()\n    }\n}\n\n// Extension function to fetch weather alerts for a given state\nsuspend fun HttpClient.getAlerts(state: String): List&lt;String&gt; {\n    val alerts = this.get(\"/alerts/active/area/$state\").body&lt;Alert&gt;()\n    return alerts.features.map { feature -&gt;\n        \"\"\"\n            Event: ${feature.properties.event}\n            Area: ${feature.properties.areaDesc}\n            Severity: ${feature.properties.severity}\n            Description: ${feature.properties.description}\n            Instruction: ${feature.properties.instruction}\n        \"\"\".trimIndent()\n    }\n}\n\n@Serializable\ndata class Points(\n    val properties: Properties\n) {\n    @Serializable\n    data class Properties(val forecast: String)\n}\n\n@Serializable\ndata class Forecast(\n    val properties: Properties\n) {\n    @Serializable\n    data class Properties(val periods: List&lt;Period&gt;)\n\n    @Serializable\n    data class Period(\n        val number: Int, val name: String, val startTime: String, val endTime: String,\n        val isDaytime: Boolean, val temperature: Int, val temperatureUnit: String,\n        val temperatureTrend: String, val probabilityOfPrecipitation: JsonObject,\n        val windSpeed: String, val windDirection: String,\n        val shortForecast: String, val detailedForecast: String,\n    )\n}\n\n@Serializable\ndata class Alert(\n    val features: List&lt;Feature&gt;\n) {\n    @Serializable\n    data class Feature(\n        val properties: Properties\n    )\n\n    @Serializable\n    data class Properties(\n        val event: String, val areaDesc: String, val severity: String,\n        val description: String, val instruction: String?,\n    )\n}\n```\n\n### Implementing tool execution\n\nThe tool execution handler is responsible for actually executing the logic of each tool. Let's add it:\n\n```kotlin\n// Create an HTTP client with a default request configuration and JSON content negotiation\nval httpClient = HttpClient {\n    defaultRequest {\n        url(\"https://api.weather.gov\")\n        headers {\n            append(\"Accept\", \"application/geo+json\")\n            append(\"User-Agent\", \"WeatherApiClient/1.0\")\n        }\n        contentType(ContentType.Application.Json)\n    }\n    // Install content negotiation plugin for JSON serialization/deserialization\n    install(ContentNegotiation) { json(Json { ignoreUnknownKeys = true }) }\n}\n\n// Register a tool to fetch weather alerts by state\nserver.addTool(\n    name = \"get_alerts\",\n    description = \"\"\"\n        Get weather alerts for a US state. Input is Two-letter US state code (e.g. CA, NY)\n    \"\"\".trimIndent(),\n    inputSchema = Tool.Input(\n        properties = buildJsonObject {\n            putJsonObject(\"state\") {\n                put(\"type\", \"string\")\n                put(\"description\", \"Two-letter US state code (e.g. CA, NY)\")\n            }\n        },\n        required = listOf(\"state\")\n    )\n) { request -&gt;\n    val state = request.arguments[\"state\"]?.jsonPrimitive?.content\n    if (state == null) {\n        return@addTool CallToolResult(\n            content = listOf(TextContent(\"The 'state' parameter is required.\"))\n        )\n    }\n\n    val alerts = httpClient.getAlerts(state)\n\n    CallToolResult(content = alerts.map { TextContent(it) })\n}\n\n// Register a tool to fetch weather forecast by latitude and longitude\nserver.addTool(\n    name = \"get_forecast\",\n    description = \"\"\"\n        Get weather forecast for a specific latitude/longitude\n    \"\"\".trimIndent(),\n    inputSchema = Tool.Input(\n        properties = buildJsonObject {\n            putJsonObject(\"latitude\") { put(\"type\", \"number\") }\n            putJsonObject(\"longitude\") { put(\"type\", \"number\") }\n        },\n        required = listOf(\"latitude\", \"longitude\")\n    )\n) { request -&gt;\n    val latitude = request.arguments[\"latitude\"]?.jsonPrimitive?.doubleOrNull\n    val longitude = request.arguments[\"longitude\"]?.jsonPrimitive?.doubleOrNull\n    if (latitude == null || longitude == null) {\n        return@addTool CallToolResult(\n            content = listOf(TextContent(\"The 'latitude' and 'longitude' parameters are required.\"))\n        )\n    }\n\n    val forecast = httpClient.getForecast(latitude, longitude)\n\n    CallToolResult(content = forecast.map { TextContent(it) })\n}\n```\n\n### Running the server\n\nFinally, implement the main function to run the server:\n\n```kotlin\nfun main() = `run mcp server`()\n```\n\nMake sure to run `./gradlew build` to build your server. This is a very important step in getting your server to connect.\n\nLet's now test your server from an existing MCP host, Claude for Desktop.\n\n## Testing your server with Claude for Desktop\n\n&lt;Note&gt;\n  Claude for Desktop is not yet available on Linux. Linux users can proceed to the [Building a client](/quickstart/client) tutorial to build an MCP client that connects to the server we just built.\n&lt;/Note&gt;\n\nFirst, make sure you have Claude for Desktop installed. [You can install the latest version\nhere.](https://claude.ai/download) If you already have Claude for Desktop, **make sure it's updated to the latest version.**\n\nWe'll need to configure Claude for Desktop for whichever MCP servers you want to use.\nTo do this, open your Claude for Desktop App configuration at `~/Library/Application Support/Claude/claude_desktop_config.json` in a text editor.\nMake sure to create the file if it doesn't exist.\n\nFor example, if you have [VS Code](https://code.visualstudio.com/) installed:\n\n&lt;CodeGroup&gt;\n  ```bash MacOS/Linux\n  code ~/Library/Application\\ Support/Claude/claude_desktop_config.json\n  ```\n\n  ```powershell Windows\n  code $env:AppData\\Claude\\claude_desktop_config.json\n  ```\n&lt;/CodeGroup&gt;\n\nYou'll then add your servers in the `mcpServers` key.\nThe MCP UI elements will only show up in Claude for Desktop if at least one server is properly configured.\n\nIn this case, we'll add our single weather server like so:\n\n&lt;CodeGroup&gt;\n  ```json MacOS/Linux\n  {\n    \"mcpServers\": {\n      \"weather\": {\n        \"command\": \"java\",\n        \"args\": [\n          \"-jar\",\n          \"/ABSOLUTE/PATH/TO/PARENT/FOLDER/weather/build/libs/weather-0.1.0-all.jar\"\n        ]\n      }\n    }\n  }\n  ```\n\n  ```json Windows\n  {\n    \"mcpServers\": {\n      \"weather\": {\n        \"command\": \"java\",\n        \"args\": [\n          \"-jar\",\n          \"C:\\\\PATH\\\\TO\\\\PARENT\\\\FOLDER\\\\weather\\\\build\\\\libs\\\\weather-0.1.0-all.jar\"\n        ]\n      }\n    }\n  }\n  ```\n&lt;/CodeGroup&gt;\n\nThis tells Claude for Desktop:\n\n1. There's an MCP server named \"weather\"\n2. Launch it by running `java -jar /ABSOLUTE/PATH/TO/PARENT/FOLDER/weather/build/libs/weather-0.1.0-all.jar`\n\nSave the file, and restart **Claude for Desktop**.\n</code></pre> <p></p> <p>     Let's get started with building our weather server! You can find the complete code for what we'll be building here. <pre><code>### Prerequisite knowledge\n\nThis quickstart assumes you have familiarity with:\n\n* C#\n* LLMs like Claude\n* .NET 8 or higher\n\n### Logging in MCP Servers\n\nWhen implementing MCP servers, be careful about how you handle logging:\n\n**For STDIO-based servers:** Never write to standard output (stdout). This includes:\n\n* `print()` statements in Python\n* `console.log()` in JavaScript\n* `fmt.Println()` in Go\n* Similar stdout functions in other languages\n\nWriting to stdout will corrupt the JSON-RPC messages and break your server.\n\n**For HTTP-based servers:** Standard output logging is fine since it doesn't interfere with HTTP responses.\n\n### Best Practices\n\n1. Use a logging library that writes to stderr or files\n\n### System requirements\n\n* [.NET 8 SDK](https://dotnet.microsoft.com/download/dotnet/8.0) or higher installed.\n\n### Set up your environment\n\nFirst, let's install `dotnet` if you haven't already. You can download `dotnet` from [official Microsoft .NET website](https://dotnet.microsoft.com/download/). Verify your `dotnet` installation:\n\n```bash\ndotnet --version\n```\n\nNow, let's create and set up your project:\n\n&lt;CodeGroup&gt;\n  ```bash MacOS/Linux\n  # Create a new directory for our project\n  mkdir weather\n  cd weather\n  # Initialize a new C# project\n  dotnet new console\n  ```\n\n  ```powershell Windows\n  # Create a new directory for our project\n  mkdir weather\n  cd weather\n  # Initialize a new C# project\n  dotnet new console\n  ```\n&lt;/CodeGroup&gt;\n\nAfter running `dotnet new console`, you will be presented with a new C# project.\nYou can open the project in your favorite IDE, such as [Visual Studio](https://visualstudio.microsoft.com/) or [Rider](https://www.jetbrains.com/rider/).\nAlternatively, you can create a C# application using the [Visual Studio project wizard](https://learn.microsoft.com/en-us/visualstudio/get-started/csharp/tutorial-console?view=vs-2022).\nAfter creating the project, add NuGet package for the Model Context Protocol SDK and hosting:\n\n```bash\n# Add the Model Context Protocol SDK NuGet package\ndotnet add package ModelContextProtocol --prerelease\n# Add the .NET Hosting NuGet package\ndotnet add package Microsoft.Extensions.Hosting\n```\n\nNow let\u2019s dive into building your server.\n\n## Building your server\n\nOpen the `Program.cs` file in your project and replace its contents with the following code:\n\n```csharp\nusing Microsoft.Extensions.DependencyInjection;\nusing Microsoft.Extensions.Hosting;\nusing ModelContextProtocol;\nusing System.Net.Http.Headers;\n\nvar builder = Host.CreateEmptyApplicationBuilder(settings: null);\n\nbuilder.Services.AddMcpServer()\n    .WithStdioServerTransport()\n    .WithToolsFromAssembly();\n\nbuilder.Services.AddSingleton(_ =&gt;\n{\n    var client = new HttpClient() { BaseAddress = new Uri(\"https://api.weather.gov\") };\n    client.DefaultRequestHeaders.UserAgent.Add(new ProductInfoHeaderValue(\"weather-tool\", \"1.0\"));\n    return client;\n});\n\nvar app = builder.Build();\n\nawait app.RunAsync();\n```\n\n&lt;Note&gt;\n  When creating the `ApplicationHostBuilder`, ensure you use `CreateEmptyApplicationBuilder` instead of `CreateDefaultBuilder`. This ensures that the server does not write any additional messages to the console. This is only necessary for servers using STDIO transport.\n&lt;/Note&gt;\n\nThis code sets up a basic console application that uses the Model Context Protocol SDK to create an MCP server with standard I/O transport.\n\n### Weather API helper functions\n\nCreate an extension class for `HttpClient` which helps simplify JSON request handling:\n\n```csharp\nusing System.Text.Json;\n\ninternal static class HttpClientExt\n{\n    public static async Task&lt;JsonDocument&gt; ReadJsonDocumentAsync(this HttpClient client, string requestUri)\n    {\n        using var response = await client.GetAsync(requestUri);\n        response.EnsureSuccessStatusCode();\n        return await JsonDocument.ParseAsync(await response.Content.ReadAsStreamAsync());\n    }\n}\n```\n\nNext, define a class with the tool execution handlers for querying and converting responses from the National Weather Service API:\n\n```csharp\nusing ModelContextProtocol.Server;\nusing System.ComponentModel;\nusing System.Globalization;\nusing System.Text.Json;\n\nnamespace QuickstartWeatherServer.Tools;\n\n[McpServerToolType]\npublic static class WeatherTools\n{\n    [McpServerTool, Description(\"Get weather alerts for a US state.\")]\n    public static async Task&lt;string&gt; GetAlerts(\n        HttpClient client,\n        [Description(\"The US state to get alerts for.\")] string state)\n    {\n        using var jsonDocument = await client.ReadJsonDocumentAsync($\"/alerts/active/area/{state}\");\n        var jsonElement = jsonDocument.RootElement;\n        var alerts = jsonElement.GetProperty(\"features\").EnumerateArray();\n\n        if (!alerts.Any())\n        {\n            return \"No active alerts for this state.\";\n        }\n\n        return string.Join(\"\\n--\\n\", alerts.Select(alert =&gt;\n        {\n            JsonElement properties = alert.GetProperty(\"properties\");\n            return $\"\"\"\n                    Event: {properties.GetProperty(\"event\").GetString()}\n                    Area: {properties.GetProperty(\"areaDesc\").GetString()}\n                    Severity: {properties.GetProperty(\"severity\").GetString()}\n                    Description: {properties.GetProperty(\"description\").GetString()}\n                    Instruction: {properties.GetProperty(\"instruction\").GetString()}\n                    \"\"\";\n        }));\n    }\n\n    [McpServerTool, Description(\"Get weather forecast for a location.\")]\n    public static async Task&lt;string&gt; GetForecast(\n        HttpClient client,\n        [Description(\"Latitude of the location.\")] double latitude,\n        [Description(\"Longitude of the location.\")] double longitude)\n    {\n        var pointUrl = string.Create(CultureInfo.InvariantCulture, $\"/points/{latitude},{longitude}\");\n        using var jsonDocument = await client.ReadJsonDocumentAsync(pointUrl);\n        var forecastUrl = jsonDocument.RootElement.GetProperty(\"properties\").GetProperty(\"forecast\").GetString()\n            ?? throw new Exception($\"No forecast URL provided by {client.BaseAddress}points/{latitude},{longitude}\");\n\n        using var forecastDocument = await client.ReadJsonDocumentAsync(forecastUrl);\n        var periods = forecastDocument.RootElement.GetProperty(\"properties\").GetProperty(\"periods\").EnumerateArray();\n\n        return string.Join(\"\\n---\\n\", periods.Select(period =&gt; $\"\"\"\n                {period.GetProperty(\"name\").GetString()}\n                Temperature: {period.GetProperty(\"temperature\").GetInt32()}\u00b0F\n                Wind: {period.GetProperty(\"windSpeed\").GetString()} {period.GetProperty(\"windDirection\").GetString()}\n                Forecast: {period.GetProperty(\"detailedForecast\").GetString()}\n                \"\"\"));\n    }\n}\n```\n\n### Running the server\n\nFinally, run the server using the following command:\n\n```bash\ndotnet run\n```\n\nThis will start the server and listen for incoming requests on standard input/output.\n\n## Testing your server with Claude for Desktop\n\n&lt;Note&gt;\n  Claude for Desktop is not yet available on Linux. Linux users can proceed to the [Building a client](/quickstart/client) tutorial to build an MCP client that connects to the server we just built.\n&lt;/Note&gt;\n\nFirst, make sure you have Claude for Desktop installed. [You can install the latest version\nhere.](https://claude.ai/download) If you already have Claude for Desktop, **make sure it's updated to the latest version.**\nWe'll need to configure Claude for Desktop for whichever MCP servers you want to use. To do this, open your Claude for Desktop App configuration at `~/Library/Application Support/Claude/claude_desktop_config.json` in a text editor. Make sure to create the file if it doesn't exist.\nFor example, if you have [VS Code](https://code.visualstudio.com/) installed:\n\n&lt;CodeGroup&gt;\n  ```bash MacOS/Linux\n  code ~/Library/Application\\ Support/Claude/claude_desktop_config.json\n  ```\n\n  ```powershell Windows\n  code $env:AppData\\Claude\\claude_desktop_config.json\n  ```\n&lt;/CodeGroup&gt;\n\nYou'll then add your servers in the `mcpServers` key. The MCP UI elements will only show up in Claude for Desktop if at least one server is properly configured.\nIn this case, we'll add our single weather server like so:\n\n&lt;CodeGroup&gt;\n  ```json MacOS/Linux\n  {\n    \"mcpServers\": {\n      \"weather\": {\n        \"command\": \"dotnet\",\n        \"args\": [\"run\", \"--project\", \"/ABSOLUTE/PATH/TO/PROJECT\", \"--no-build\"]\n      }\n    }\n  }\n  ```\n\n  ```json Windows\n  {\n    \"mcpServers\": {\n      \"weather\": {\n        \"command\": \"dotnet\",\n        \"args\": [\n          \"run\",\n          \"--project\",\n          \"C:\\\\ABSOLUTE\\\\PATH\\\\TO\\\\PROJECT\",\n          \"--no-build\"\n        ]\n      }\n    }\n  }\n  ```\n&lt;/CodeGroup&gt;\n\nThis tells Claude for Desktop:\n\n1. There's an MCP server named \"weather\"\n2. Launch it by running `dotnet run /ABSOLUTE/PATH/TO/PROJECT`\n   Save the file, and restart **Claude for Desktop**.\n</code></pre> <p> </p>"},{"location":"mcp%20docs/building_an_mpc_server/#test-with-commands","title":"Test with commands","text":"<p>Let's make sure Claude for Desktop is picking up the two tools we've exposed in our <code>weather</code> server. You can do this by looking for the \"Search and tools\"  icon:</p> <p> </p> <p>After clicking on the slider icon, you should see two tools listed:</p> <p> </p> <p>If your server isn't being picked up by Claude for Desktop, proceed to the Troubleshooting section for debugging tips.</p> <p>If the tool settings icon has shown up, you can now test your server by running the following commands in Claude for Desktop:</p> <ul> <li>What's the weather in Sacramento?</li> <li>What are the active weather alerts in Texas?</li> </ul> <p> </p> <p> </p> <p>   Since this is the US National Weather service, the queries will only work for US locations. </p>"},{"location":"mcp%20docs/building_an_mpc_server/#whats-happening-under-the-hood","title":"What's happening under the hood","text":"<p>When you ask a question:</p> <ol> <li>The client sends your question to Claude</li> <li>Claude analyzes the available tools and decides which one(s) to use</li> <li>The client executes the chosen tool(s) through the MCP server</li> <li>The results are sent back to Claude</li> <li>Claude formulates a natural language response</li> <li>The response is displayed to you!</li> </ol>"},{"location":"mcp%20docs/building_an_mpc_server/#troubleshooting","title":"Troubleshooting","text":"<p> Getting logs from Claude for Desktop <pre><code>Claude.app logging related to MCP is written to log files in `~/Library/Logs/Claude`:\n\n* `mcp.log` will contain general logging about MCP connections and connection failures.\n* Files named `mcp-server-SERVERNAME.log` will contain error (stderr) logging from the named server.\n\nYou can run the following command to list recent logs and follow along with any new ones:\n\n```bash\n# Check Claude's logs for errors\ntail -n 20 -f ~/Library/Logs/Claude/mcp*.log\n```\n\n**Server not showing up in Claude**\n\n1. Check your `claude_desktop_config.json` file syntax\n2. Make sure the path to your project is absolute and not relative\n3. Restart Claude for Desktop completely\n\n**Tool calls failing silently**\n\nIf Claude attempts to use the tools but they fail:\n\n1. Check Claude's logs for errors\n2. Verify your server builds and runs without errors\n3. Try restarting Claude for Desktop\n\n**None of this is working. What do I do?**\n\nPlease refer to our [debugging guide](/legacy/tools/debugging) for better debugging tools and more detailed guidance.\n</code></pre> <p></p> <p> Error: Failed to retrieve grid point data <pre><code>This usually means either:\n\n1. The coordinates are outside the US\n2. The NWS API is having issues\n3. You're being rate limited\n\nFix:\n\n* Verify you're using US coordinates\n* Add a small delay between requests\n* Check the NWS API status page\n\n**Error: No active alerts for \\[STATE]**\n\nThis isn't an error - it just means there are no current weather alerts for that state. Try a different state or check during severe weather.\n</code></pre> <p> </p> <p>   For more advanced troubleshooting, check out our guide on Debugging MCP </p>"},{"location":"mcp%20docs/building_an_mpc_server/#next-steps","title":"Next steps","text":"<p>      Learn how to build your own MCP client that can connect to your server    <p>     Check out our gallery of official MCP servers and implementations   </p> <p>     Learn how to effectively debug MCP servers and integrations   </p> <p>     Learn how to use LLMs like Claude to speed up your MCP development    </p>"},{"location":"mcp%20docs/guide_to_using_the_responses_apis_mcp_tool/","title":"Guide to Using the Responses API's MCP Tool","text":""},{"location":"mcp%20docs/guide_to_using_the_responses_apis_mcp_tool/#may-21-2025","title":"May 21, 2025","text":"<p>Building agentic applications often requires connecting to external services. Traditionally, this is done through function calling where every action makes a round-trip from the model to your backend, then to an external service, waits for a response, and finally returns the result to the model. This process introduces multiple network hops and significant latency, making it cumbersome to scale and manage.</p> <p>The hosted Model Context Protocol (MCP) tool in the Responses API makes this easier. Instead of manually wiring each function call to specific services, you can configure your model once to point to an MCP server (or several!). That server acts as a centralized tool host, exposing standard commands like \u201csearch product catalog\u201d or \u201cadd item to cart.\u201d This allows for simpler orchestration and centralized management of tools.</p> <p>With MCP, the model interacts directly with the MCP server, reducing latency and eliminating backend coordination.</p>"},{"location":"mcp%20docs/guide_to_using_the_responses_apis_mcp_tool/#use-cases-simplified-by-the-mcp-tool","title":"Use cases simplified by the MCP tool","text":"<p>MCP significantly reduces the friction of building products that interact with external services, allowing you to tie different services together seamlessly. Here\u2019s a sampler of use cases that once involved friction but are now much simpler since the model can communicate directly with remote MCP servers.</p> <p>Domain  Use case unlocked by MCP tool   Previous friction Commerce / payments Add an item to a Shopify cart and hand back a checkout URL in one turn \u2014 \"Add the Allbirds Men\u2019s Tree Dasher 2 in size 10\" \u2192 cart link   Generating a Stripe payment link required writing a custom cart_add or create_payment_link wrapper and hosting your own relay server. Dev-ops &amp; code quality  Ask Sentry for the latest error in a particular file, then open a GitHub issue with a suggested fix, all in the same conversation   Chaining two different third-party APIs inside one assistive loop involved webhook glue and state juggling. Messaging / notifications   Grab the morning\u2019s top soccer headlines via web-search and have Twilio text the summary to a phone number in a single call  Required stitching two tool calls in your backend and batching the final SMS payload yourself.</p>"},{"location":"mcp%20docs/guide_to_using_the_responses_apis_mcp_tool/#how-the-tool-works","title":"How the tool works","text":"<p>At a high level, here is how the MCP tool works:</p> <ol> <li> <p>Declare the server. When you add an MCP block to the tools array, the Responses API runtime first detects which transport the server speaks, either the newer \u201cstreamable HTTP\u201d or the older HTTP-over-SSE variant, and uses that protocol for traffic.</p> </li> <li> <p>Import the tool list. The runtime calls the server\u2019s tools/list, passing any headers you provide (API key, OAuth token, etc.). It then writes the result to an mcp_list_tools item in the model\u2019s context. While this item is present, the list won\u2019t be fetched again. You can limit what the model sees using allowed_tools.</p> </li> </ol> <p>OpenAI discards header values and all but the schema, domain, and subdomains of the MCP server_url after each request. Authorization keys and the server URL must be included with every API call. These values won\u2019t appear in response objects. Schemas use \u201cstrict\u201d mode when possible; otherwise they\u2019re loaded as-is.</p> <ol> <li>Call and approve tools. Once the model knows the available actions, it can invoke one. Each invocation produces an mcp_tool_call item and by default the stream pauses for your explicit approval, but you can disable this once you trust the server. After approval, the runtime executes the call, streams back the result, and the model decides whether to chain another tool or return a final answer.</li> </ol>"},{"location":"mcp%20docs/guide_to_using_the_responses_apis_mcp_tool/#best-practices-when-building-with-mcp","title":"Best practices when building with MCP","text":"<p>MCP is still in its early stages, so here are best practices that can improve model performance and behavior as you build.</p>"},{"location":"mcp%20docs/guide_to_using_the_responses_apis_mcp_tool/#filter-tools-to-avoid-ballooning-payloads","title":"Filter tools to avoid ballooning payloads","text":"<p>Remote servers often expose numerous tools without considering how models will interpret and use them. By default, this can result in dozens of endpoints being included, each accompanied by verbose definitions like names, descriptions, and JSON schemas that add hundreds of tokens to the model\u2019s context and increase latency. Compounding this, many servers return entire data objects, such as full Stripe invoice records, even when only a few fields are relevant to the model\u2019s task.</p> <p>To optimize for performance in production, use the allowed_tools parameter in the Responses API to limit which tools are included from the server\u2019s mcp_list_tools. This reduces token overhead, improves response time, and narrows the model\u2019s decision space. You may also want to exclude certain tools altogether, such as those capable of write actions or those that have financial or security implications.</p> <pre><code>Copy\nEdit\ncurl https://api.openai.com/v1/responses -i \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n    \"model\": \"gpt-4.1\",\n    \"tools\": [\n      {\n        \"type\": \"mcp\",\n        \"server_label\": \"gitmcp\",\n        \"server_url\": \"https://gitmcp.io/openai/tiktoken\",\n        \"allowed_tools\": [\"search_tiktoken_documentation\", \"fetch_tiktoken_documentation\"],\n        \"require_approval\": \"never\"\n      }\n    ],\n    \"input\": \"how does tiktoken work?\"\n  }'\n</code></pre>"},{"location":"mcp%20docs/guide_to_using_the_responses_apis_mcp_tool/#reduce-latency-and-tokens-via-caching-and-reserve-reasoning-models-for-high-complexity-tasks","title":"Reduce latency and tokens via caching and reserve reasoning models for high-complexity tasks","text":"<p>The first time the model connects to a server, a new item of the type mcp_list_tools is created for each MCP server you add. As long as this item is present in the model\u2019s context, we will not call tools/list on the server again. This is akin to caching at the user-conversation level. If mcp_list_tools is not present, we import the list of tools from the MCP server again.</p> <p>Passing previous_response_id in subsequent API requests is one way of ensuring that the mcp_list_tools item is present in the model\u2019s context on follow-up turns. Alternatively, you can also pass in the items manually to new responses. The other lever that will affect latency and the number of output tokens is whether you use a reasoning model, as reasoning models will produce far more output tokens, as well as reasoning tokens.</p> <p>Scenario 1: non-reasoning model</p> <pre><code>Copy\nEdit\ncurl https://api.openai.com/v1/responses \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n    \"model\": \"gpt-4.1\",\n    \"tools\": [\n      {\n        \"type\": \"mcp\",\n        \"server_label\": \"gitmcp\",\n        \"server_url\": \"https://gitmcp.io/openai/tiktoken\",\n        \"require_approval\": \"never\"\n      }\n    ],\n    \"input\": \"how does tiktoken work?\"\n  }'\njson\nCopy\nEdit\n\"usage\": {\n  \"input_tokens\": 280,\n  \"input_tokens_details\": { \"cached_tokens\": 0 },\n  \"output_tokens\": 665,\n  \"output_tokens_details\": { \"reasoning_tokens\": 0 },\n  \"total_tokens\": 945\n}\n</code></pre> <p>Scenario 2: reasoning model without previous_response_id</p> <pre><code>Copy\nEdit\ncurl https://api.openai.com/v1/responses \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n    \"model\": \"o4-mini\",\n    \"tools\": [\n      {\n        \"type\": \"mcp\",\n        \"server_label\": \"gitmcp\",\n        \"server_url\": \"https://gitmcp.io/openai/tiktoken\",\n        \"require_approval\": \"never\"\n      }\n    ],\n    \"input\": \"how does tiktoken work?\",\n    \"reasoning\": {\n      \"effort\": \"medium\",\n      \"summary\": \"auto\"\n    }\n  }'\njson\nCopy\nEdit\n\"usage\": {\n  \"input_tokens\": 36436,\n  \"input_tokens_details\": { \"cached_tokens\": 22964 },\n  \"output_tokens\": 1586,\n  \"output_tokens_details\": { \"reasoning_tokens\": 576 },\n  \"total_tokens\": 38022\n}\n</code></pre>"},{"location":"mcp%20docs/guide_to_using_the_responses_apis_mcp_tool/#using-mcp-with-other-tools","title":"Using MCP with other tools","text":"<p>The MCP tool is just another entry in the tools array, so the model can use it seamlessly with other hosted tools like code_interpreter, web_search_preview, or image_gen, and with any custom tools you define. You can also use multiple remote MCP servers together.</p> <p>In this example, we\u2019ll create an agent that is a pricing analyst for a fictional yoga-attire store: it first pulls current competitor prices for women\u2019s shorts, yoga pants, and tank tops from the Alo Yoga MCP server, then grabs the price for the same three categories from Uniqlo via the hosted web-search tool. Using Code Interpreter it analyzes last week\u2019s sales from a CSV that was pre-loaded with the Files endpoint, in order to calculate per-item revenue and average order value.</p> <p>Then it measures each item\u2019s price gap versus the newly fetched Uniqlo and Alo Yoga benchmarks. Any product priced 15 percent or more above or below market is flagged, and the agent delivers a concise text report summarizing the discrepancies and key revenue stats.</p> <pre><code>Copy\nEdit\nsystem_prompt = \"\"\"You are a pricing analyst for my clothing company. Please use the MCP tool \nto fetch prices from the Alo Yoga MCP server for the categories of women's \nshorts, yoga pants, and tank tops. Use only the MCP server for Alo Yoga data; don't search the web.\n\nNext, use the web search tool to search for Uniqlo prices for women's shorts, yoga pants, and tank tops.\nIn each case for Alo Yoga and Uniqlo, extract the price for the top result in each category. Also provide the full URLs.\n\nUsing the uploaded CSV file of sales data from my store, and with the code interpreter tool calculate revenue by product item, compute average order value on a transaction level, and calculate the percentage price gap between the CSV data and Uniqlo/Alo Yoga prices. Flag products priced 15% or more above or below the market. Create and output a short report including the findings.\n\n# Steps\n1. **Fetch Alo Yoga Prices:**\n   - Use the Alo Yoga MCP server to fetch prices for the following products:\n     - High-Waist Airlift Legging  \n     - Sway Bra Tank  \n     - 5\" Airlift Energy Short\n   - Ensure you find prices for each.  \n   - Extract the price of the top result for each category.  \n   - Include URL links.\n\n2. **Query Uniqlo Prices:**\n   - Use the Web-Search tool to search non-sale prices for the following Uniqlo products:\n     - Women's AIRism Soft Biker Shorts  \n     - Women's AIRism Soft Leggings  \n     - Women's AIRism Bra Sleeveless Top\n   - Ensure you find non-sale prices for each.  \n   - Extract the price for the top result for each category.  \n   - Include URL links.\n\n3. **Sales Data Analysis:**\n   - Use the uploaded CSV sales data to calculate revenue across each product item.  \n   - Determine the average order-value on a transaction level.  \n   - For each SKU, compute the percentage price gap between the CSV data and Uniqlo/Alo Yoga prices.  \n   - Flag products priced \u2265 15% above or below the market.\n\n4. **Report:**\n   - Compile and output a report including the flagging results\n\"\"\"\n</code></pre> <p>Pricing Comparison and Revenue Analysis Report Your Store's Sales &amp; Price Analysis</p> <p>Revenue by Product:</p> <p>Shorts: $6,060</p> <p>Tank tops: $6,150</p> <p>Yoga pants: $12,210</p> <p>Average Order Value: $872.14</p> <p>Your Store's Average Selling Price by Category:</p> <p>Shorts: $60.00</p> <p>Tank tops: $75.00</p> <p>Yoga pants: $110.00</p> <p>Pricing Gaps vs Market Category    Store Avg Price vs Alo Yoga Gap (%) Flagged (\u226515%)  vs Uniqlo Gap (%)   Flagged (\u226515%) Shorts  $60.00 \u201331.8%   YES +100.7% YES Tank tops   $75.00 \u201314.8%       +114.9% YES Yoga pants  $110.00 \u201314.1%      +267.9% YES</p> <p>Recommendations &amp; Flags Flagged products (\u2265 15% price gap):</p> <p>Shorts: Priced 31.8% below Alo Yoga, but 100.7% above Uniqlo.</p> <p>Tank tops: Priced over 114.9% above Uniqlo.</p> <p>Yoga pants: Priced 267.9% above Uniqlo.</p> <p>Shorts are priced significantly below premium competitors (Alo Yoga), but far higher than budget alternatives (Uniqlo). If you want to compete in the premium segment, consider increasing your price. If you want to target budget buyers, a price decrease could be justifiable. Most of your tank tops and yoga pants are similarly positioned\u2014much lower than Alo, but well above Uniqlo.</p>"},{"location":"mcp%20docs/guide_to_using_the_responses_apis_mcp_tool/#prompting-guidelines-to-improve-mcp-tool-calls","title":"Prompting guidelines to improve MCP tool calls","text":"<p>Depending on your use case you might find that the model invokes many MCP calls, for instance when using catalog-search tools. To avoid endless iterations over large product inventories it\u2019s helpful to:</p> <ol> <li> <p>Limit results. Instruct the model to return only N items at a time and offer to continue only when the user explicitly asks for more.</p> </li> <li> <p>Use few-shot examples. If the MCP servers you\u2019re using include exhaustive mcp_list_tools, showing targeted examples helps the model choose the correct server and stop once it has what it needs.</p> </li> <li> <p>Ask follow-ups for missing details. If size, color, product line, etc. are missing from the user query, prompt the model to ask a question first rather than launching a broad search.</p> </li> </ol> <p>Here\u2019s a sample prompt illustrating these guidelines:</p> <pre><code>Copy\nEdit\nsystem_prompt = \"\"\"You are an AI assistant that can call the following MCP servers:\n\n1. allbirds_store  \n2. gitmcp\n\nSteps:\n1. Use the MCP servers above to answer the user query; not every MCP server will be relevant for a given query, so choose wisely.  \n2. If the user\u2019s request lacks essential details (size, color, etc.), ask a follow-up question first.  \n3. If searching for products in a catalog, only return 4 results at a time before asking the user whether they want to see more.\n\n### Few-shot examples\n\n# EX1 \u2014 Direct product search, then fetch variant details  \nuser: Do you have the Allbirds Tree Runner in men\u2019s size 10?  \nassistant: {\"name\":\"allbirds_store.search_shop_catalog\",\"arguments\":{\"query\":\"Tree Runner\",\"context\":\"Men\u2019s size 10 shoes\"}}  \n# (assume search returns product_id \"gid://shopify/Product/987\")  \nassistant: {\"name\":\"allbirds_store.get_product_details\",\"arguments\":{\"product_id\":\"gid://shopify/Product/987\",\"options\":{\"Size\":\"10\"}}}\n\n# EX2 \u2014 Clarify missing size, then two-step color variant lookup  \nuser: I want the Tree Runner in blue.  \nassistant: Sure \u2014 what size do you need?  \nuser: Size 10.  \nassistant: {\"name\":\"allbirds_store.search_shop_catalog\",\"arguments\":{\"query\":\"Tree Runner\",\"context\":\"Blue variant, size 10\"}}  \n# (assume search returns product_id \"gid://shopify/Product/987\")  \nassistant: {\"name\":\"allbirds_store.get_product_details\",\"arguments\":{\"product_id\":\"gid://shopify/Product/987\",\"options\":{\"Size\":\"10\",\"Color\":\"Blue\"}}}\n\n# EX3 \u2014 Git docs: search then fetch specific file  \nuser: Can you show me how tiktoken does byte-pair encoding?  \nassistant: {\"name\":\"gitmcp.search_website_documentation\",\"arguments\":{\"query\":\"tiktoken byte-pair encoding\"}}  \n# (assume search returns document_id \"docs/ENCODING.md\")  \nassistant: {\"name\":\"gitmcp.fetch_website_documentation\",\"arguments\":{\"document_id\":\"docs/ENCODING.md\"}}\"\"\"\n</code></pre>"},{"location":"mcp%20docs/guide_to_using_the_responses_apis_mcp_tool/#conclusion","title":"Conclusion","text":"<p>The hosted MCP tool in the Responses API turns external-service access from a bespoke plumbing task into a first-class capability of the API. By connecting to a remote server, letting the runtime cache its tool list, and trimming that list with allowed_tools, you eliminate the extra network hop, cut token overhead, and give the model a concise, discoverable action set.</p> <p>When combined with built-in tools such as code_interpreter, web_search_preview, or image_gen, MCP unlocks rich, multi-service workflows whether you\u2019re analyzing sales data, triaging production errors, or automating checkout flows.</p>"},{"location":"mcp%20docs/mcp/","title":"Mcp","text":"<p>Model Context Protocol (MCP) The Model Context Protocol (MCP) is an open protocol that standardizes how applications provide tools and context to LLMs. From the MCP docs:</p> <p>MCP is an open protocol that standardizes how applications provide context to LLMs. Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect your devices to various peripherals and accessories, MCP provides a standardized way to connect AI models to different data sources and tools.</p> <p>There are three types of MCP servers this SDK supports:</p> <p>Hosted MCP server tools \u2013 remote MCP servers used as tools by the OpenAI Responses API Streamable HTTP MCP servers \u2013 local\u202for\u202fremote servers that implement the Streamable HTTP transport Stdio MCP servers \u2013 servers accessed via standard input/output (the simplest option) Choose a server type based on your use\u2011case:</p> <p>What you need   Recommended option Call publicly accessible remote servers with default OpenAI responses models    1. Hosted MCP tools Use publicly accessible remote servers but have the tool calls triggered locally    2. Streamable HTTP Use locally running Streamable HTTP servers 2. Streamable HTTP Use any Streamable HTTP servers with non-OpenAI-Responses models    2. Streamable HTTP Work with local MCP servers that only support the standard-I/O protocol 3. Stdio 1. Hosted MCP server tools Hosted tools push the entire round\u2011trip into the model. Instead of your code calling an MCP server, the OpenAI Responses API invokes the remote tool endpoint and streams the result back to the model.</p> <p>Here is the simplest example of using hosted MCP tools. You can pass the remote MCP server\u2019s label and URL to the hostedMcpTool utility function, which is helpful for creating hosted MCP server tools.</p> <p>hostedAgent.ts import { Agent, hostedMcpTool } from '@openai/agents';</p> <p>export const agent = new Agent({   name: 'MCP Assistant',   instructions: 'You must always use the MCP tools to answer questions.',   tools: [     hostedMcpTool({       serverLabel: 'gitmcp',       serverUrl: 'https://gitmcp.io/openai/codex',     }),   ], });</p> <p>Then, you can run the Agent with the run function (or your own customized Runner instance\u2019s run method):</p> <p>Run with hosted MCP tools import { run } from '@openai/agents'; import { agent } from './hostedAgent';</p> <p>async function main() {   const result = await run(     agent,     'Which language is the repo I pointed in the MCP tool settings written in?',   );   console.log(result.finalOutput); }</p> <p>main().catch(console.error);</p> <p>To stream incremental MCP results, pass stream: true when you run the Agent:</p> <p>Run with hosted MCP tools (streaming) import { run } from '@openai/agents'; import { agent } from './hostedAgent';</p> <p>async function main() {   const result = await run(     agent,     'Which language is the repo I pointed in the MCP tool settings written in?',     { stream: true },   );</p> <p>for await (const event of result) {     if (       event.type === 'raw_model_stream_event' &amp;&amp;       event.data.type === 'model' &amp;&amp;       event.data.event.type !== 'response.mcp_call_arguments.delta' &amp;&amp;       event.data.event.type !== 'response.output_text.delta'     ) {       console.log(<code>Got event of type ${JSON.stringify(event.data)}</code>);     }   }   console.log(<code>Done streaming; final result: ${result.finalOutput}</code>); }</p> <p>main().catch(console.error);</p> <p>Optional approval flow For sensitive operations you can require human approval of individual tool calls. Pass either requireApproval: 'always' or a fine\u2011grained object mapping tool names to 'never'/'always'.</p> <p>If you can programmatically determine whether a tool call is safe, you can use the onApproval callback to approve or reject the tool call. If you require human approval, you can use the same human-in-the-loop (HITL) approach using interruptions as for local function tools.</p> <p>Human in the loop with hosted MCP tools import { Agent, run, hostedMcpTool, RunToolApprovalItem } from '@openai/agents';</p> <p>async function main(): Promise {   const agent = new Agent({     name: 'MCP Assistant',     instructions: 'You must always use the MCP tools to answer questions.',     tools: [       hostedMcpTool({         serverLabel: 'gitmcp',         serverUrl: 'https://gitmcp.io/openai/codex',         // 'always' | 'never' | { never, always }         requireApproval: {           never: {             toolNames: ['search_codex_code', 'fetch_codex_documentation'],           },           always: {             toolNames: ['fetch_generic_url_content'],           },         },       }),     ],   }); <p>let result = await run(agent, 'Which language is this repo written in?');   while (result.interruptions &amp;&amp; result.interruptions.length) {     for (const interruption of result.interruptions) {       // Human in the loop here       const approval = await confirm(interruption);       if (approval) {         result.state.approve(interruption);       } else {         result.state.reject(interruption);       }     }     result = await run(agent, result.state);   }   console.log(result.finalOutput); }</p> <p>import { stdin, stdout } from 'node:process'; import * as readline from 'node:readline/promises';</p> <p>async function confirm(item: RunToolApprovalItem): Promise {   const rl = readline.createInterface({ input: stdin, output: stdout });   const name = item.rawItem.name;   const params = item.rawItem.providerData?.arguments;   const answer = await rl.question(     <code>Approve running tool (mcp: ${name}, params: ${params})? (y/n)</code>,   );   rl.close();   return answer.toLowerCase().trim() === 'y'; } <p>main().catch(console.error);</p> <p>Fully working samples (Hosted tools/Streamable HTTP/stdio + Streaming, HITL, onApproval) are examples/mcp in our GitHub repository.</p> <ol> <li>Streamable HTTP MCP servers When your Agent talks directly to a Streamable HTTP MCP server\u2014local or remote\u2014instantiate MCPServerStreamableHttp with the server url, name, and any optional settings:</li> </ol> <p>Run with Streamable HTTP MCP servers import { Agent, run, MCPServerStreamableHttp } from '@openai/agents';</p> <p>async function main() {   const mcpServer = new MCPServerStreamableHttp({     url: 'https://gitmcp.io/openai/codex',     name: 'GitMCP Documentation Server',   });   const agent = new Agent({     name: 'GitMCP Assistant',     instructions: 'Use the tools to respond to user requests.',     mcpServers: [mcpServer],   });</p> <p>try {     await mcpServer.connect();     const result = await run(agent, 'Which language is this repo written in?');     console.log(result.finalOutput);   } finally {     await mcpServer.close();   } }</p> <p>main().catch(console.error);</p> <p>The constructor also accepts additional MCP TypeScript\u2011SDK options such as authProvider, requestInit, reconnectionOptions, and sessionId. See the MCP TypeScript SDK repository and its documents for details.</p> <ol> <li>Stdio MCP servers For servers that expose only standard I/O, instantiate MCPServerStdio with a fullCommand:</li> </ol> <p>Run with Stdio MCP servers import { Agent, run, MCPServerStdio } from '@openai/agents'; import * as path from 'node:path';</p> <p>async function main() {   const samplesDir = path.join(__dirname, 'sample_files');   const mcpServer = new MCPServerStdio({     name: 'Filesystem MCP Server, via npx',     fullCommand: <code>npx -y @modelcontextprotocol/server-filesystem ${samplesDir}</code>,   });   await mcpServer.connect();   try {     const agent = new Agent({       name: 'FS MCP Assistant',       instructions:         'Use the tools to read the filesystem and answer questions based on those files. If you are unable to find any files, you can say so instead of assuming they exist.',       mcpServers: [mcpServer],     });     const result = await run(agent, 'Read the files and list them.');     console.log(result.finalOutput);   } finally {     await mcpServer.close();   } }</p> <p>main().catch(console.error);</p> <p>Other things to know For Streamable HTTP and Stdio servers, each time an Agent runs it may call list_tools() to discover available tools. Because that round\u2011trip can add latency\u2014especially to remote servers\u2014you can cache the results in memory by passing cacheToolsList: true to MCPServerStdio or MCPServerStreamableHttp.</p> <p>Only enable this if you\u2019re confident the tool list won\u2019t change. To invalidate the cache later, call invalidateToolsCache() on the server instance.</p> <p>Tool filtering You can restrict which tools are exposed from each server by passing either a static filter via createMCPToolStaticFilter or a custom function. Here\u2019s a combined example showing both approaches:</p> <p>Tool filtering import {   MCPServerStdio,   MCPServerStreamableHttp,   createMCPToolStaticFilter,   MCPToolFilterContext, } from '@openai/agents';</p> <p>interface ToolFilterContext {   allowAll: boolean; }</p> <p>const server = new MCPServerStdio({   fullCommand: 'my-server',   toolFilter: createMCPToolStaticFilter({     allowed: ['safe_tool'],     blocked: ['danger_tool'],   }), });</p> <p>const dynamicServer = new MCPServerStreamableHttp({   url: 'http://localhost:3000',   toolFilter: async ({ runContext }: MCPToolFilterContext, tool) =&gt;     (runContext.context as ToolFilterContext).allowAll || tool.name !== 'admin', });</p> <p>Further reading Model Context Protocol \u2013 official specification. examples/mcp \u2013 runnable demos referenced above.</p>"},{"location":"mcp%20docs/model_context_protocol_%28mcp%29/","title":"Model context protocol (mcp)","text":"<p>Model context protocol (MCP) The Model context protocol (aka MCP) is a way to provide tools and context to the LLM. From the MCP docs:</p> <p>MCP is an open protocol that standardizes how applications provide context to LLMs. Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect your devices to various peripherals and accessories, MCP provides a standardized way to connect AI models to different data sources and tools.</p> <p>The Agents SDK has support for MCP. This enables you to use a wide range of MCP servers to provide tools and prompts to your Agents.</p> <p>MCP servers Currently, the MCP spec defines three kinds of servers, based on the transport mechanism they use:</p> <p>stdio servers run as a subprocess of your application. You can think of them as running \"locally\". HTTP over SSE servers run remotely. You connect to them via a URL. Streamable HTTP servers run remotely using the Streamable HTTP transport defined in the MCP spec. You can use the MCPServerStdio, MCPServerSse, and MCPServerStreamableHttp classes to connect to these servers.</p> <p>For example, this is how you'd use the official MCP filesystem server.</p> <p>from agents.run_context import RunContextWrapper</p> <p>async with MCPServerStdio(     params={         \"command\": \"npx\",         \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", samples_dir],     } ) as server:     # Note: In practice, you typically add the server to an Agent     # and let the framework handle tool listing automatically.     # Direct calls to list_tools() require run_context and agent parameters.     run_context = RunContextWrapper(context=None)     agent = Agent(name=\"test\", instructions=\"test\")     tools = await server.list_tools(run_context, agent) Using MCP servers MCP servers can be added to Agents. The Agents SDK will call list_tools() on the MCP servers each time the Agent is run. This makes the LLM aware of the MCP server's tools. When the LLM calls a tool from an MCP server, the SDK calls call_tool() on that server.</p> <p>agent=Agent(     name=\"Assistant\",     instructions=\"Use the tools to achieve the task\",     mcp_servers=[mcp_server_1, mcp_server_2] ) Tool filtering You can filter which tools are available to your Agent by configuring tool filters on MCP servers. The SDK supports both static and dynamic tool filtering.</p> <p>Static tool filtering For simple allow/block lists, you can use static filtering:</p> <p>from agents.mcp import create_static_tool_filter</p>"},{"location":"mcp%20docs/model_context_protocol_%28mcp%29/#only-expose-specific-tools-from-this-server","title":"Only expose specific tools from this server","text":"<p>server = MCPServerStdio(     params={         \"command\": \"npx\",         \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", samples_dir],     },     tool_filter=create_static_tool_filter(         allowed_tool_names=[\"read_file\", \"write_file\"]     ) )</p>"},{"location":"mcp%20docs/model_context_protocol_%28mcp%29/#exclude-specific-tools-from-this-server","title":"Exclude specific tools from this server","text":"<p>server = MCPServerStdio(     params={         \"command\": \"npx\",          \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", samples_dir],     },     tool_filter=create_static_tool_filter(         blocked_tool_names=[\"delete_file\"]     ) ) When both allowed_tool_names and blocked_tool_names are configured, the processing order is: 1. First apply allowed_tool_names (allowlist) - only keep the specified tools 2. Then apply blocked_tool_names (blocklist) - exclude specified tools from the remaining tools</p> <p>For example, if you configure allowed_tool_names=[\"read_file\", \"write_file\", \"delete_file\"] and blocked_tool_names=[\"delete_file\"], only read_file and write_file tools will be available.</p> <p>Dynamic tool filtering For more complex filtering logic, you can use dynamic filters with functions:</p> <p>from agents.mcp import ToolFilterContext</p>"},{"location":"mcp%20docs/model_context_protocol_%28mcp%29/#simple-synchronous-filter","title":"Simple synchronous filter","text":"<p>def custom_filter(context: ToolFilterContext, tool) -&gt; bool:     \"\"\"Example of a custom tool filter.\"\"\"     # Filter logic based on tool name patterns     return tool.name.startswith(\"allowed_prefix\")</p>"},{"location":"mcp%20docs/model_context_protocol_%28mcp%29/#context-aware-filter","title":"Context-aware filter","text":"<p>def context_aware_filter(context: ToolFilterContext, tool) -&gt; bool:     \"\"\"Filter tools based on context information.\"\"\"     # Access agent information     agent_name = context.agent.name</p> <pre><code># Access server information  \nserver_name = context.server_name\n\n# Implement your custom filtering logic here\nreturn some_filtering_logic(agent_name, server_name, tool)\n</code></pre>"},{"location":"mcp%20docs/model_context_protocol_%28mcp%29/#asynchronous-filter","title":"Asynchronous filter","text":"<p>async def async_filter(context: ToolFilterContext, tool) -&gt; bool:     \"\"\"Example of an asynchronous filter.\"\"\"     # Perform async operations if needed     result = await some_async_check(context, tool)     return result</p> <p>server = MCPServerStdio(     params={         \"command\": \"npx\",         \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", samples_dir],     },     tool_filter=custom_filter  # or context_aware_filter or async_filter ) The ToolFilterContext provides access to: - run_context: The current run context - agent: The agent requesting the tools - server_name: The name of the MCP server</p> <p>Prompts MCP servers can also provide prompts that can be used to dynamically generate agent instructions. This allows you to create reusable instruction templates that can be customized with parameters.</p> <p>Using prompts MCP servers that support prompts provide two key methods:</p> <p>list_prompts(): Lists all available prompts on the server get_prompt(name, arguments): Gets a specific prompt with optional parameters</p>"},{"location":"mcp%20docs/model_context_protocol_%28mcp%29/#list-available-prompts","title":"List available prompts","text":"<p>prompts_result = await server.list_prompts() for prompt in prompts_result.prompts:     print(f\"Prompt: {prompt.name} - {prompt.description}\")</p>"},{"location":"mcp%20docs/model_context_protocol_%28mcp%29/#get-a-specific-prompt-with-parameters","title":"Get a specific prompt with parameters","text":"<p>prompt_result = await server.get_prompt(     \"generate_code_review_instructions\",     {\"focus\": \"security vulnerabilities\", \"language\": \"python\"} ) instructions = prompt_result.messages[0].content.text</p>"},{"location":"mcp%20docs/model_context_protocol_%28mcp%29/#use-the-prompt-generated-instructions-with-an-agent","title":"Use the prompt-generated instructions with an Agent","text":"<p>agent = Agent(     name=\"Code Reviewer\",     instructions=instructions,  # Instructions from MCP prompt     mcp_servers=[server] ) Caching Every time an Agent runs, it calls list_tools() on the MCP server. This can be a latency hit, especially if the server is a remote server. To automatically cache the list of tools, you can pass cache_tools_list=True to MCPServerStdio, MCPServerSse, and MCPServerStreamableHttp. You should only do this if you're certain the tool list will not change.</p> <p>If you want to invalidate the cache, you can call invalidate_tools_cache() on the servers.</p> <p>End-to-end examples View complete working examples at examples/mcp.</p> <p>Tracing Tracing automatically captures MCP operations, including:</p> <p>Calls to the MCP server to list tools MCP-related info on function calls</p>"},{"location":"mcp%20docs/remote_mcp/","title":"Remote MCP","text":"<p>Allow models to use remote MCP servers to perform tasks.</p> <p>Model Context Protocol (MCP) is an open protocol that standardizes how applications provide tools and context to LLMs. The MCP tool in the Responses API allows developers to give the model access to tools hosted on Remote MCP servers. These are MCP servers maintained by developers and organizations across the internet that expose these tools to MCP clients, like the Responses API.</p> <p>Calling a remote MCP server with the Responses API is straightforward. For example, here's how you can use the DeepWiki MCP server to ask questions about nearly any public GitHub repository.</p> <p>A Responses API request with MCP tools enabled</p> <pre><code>curl https://api.openai.com/v1/responses \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n  \"model\": \"gpt-4.1\",\n  \"tools\": [\n    {\n      \"type\": \"mcp\",\n      \"server_label\": \"deepwiki\",\n      \"server_url\": \"https://mcp.deepwiki.com/mcp\",\n      \"require_approval\": \"never\"\n    }\n  ],\n  \"input\": \"What transport protocols are supported in the 2025-03-26 version of the MCP spec?\"\n}'\n</code></pre> <pre><code>import OpenAI from \"openai\";\nconst client = new OpenAI();\n\nconst resp = await client.responses.create({\n    model: \"gpt-4.1\",\n    tools: [\n        {\n            type: \"mcp\",\n            server_label: \"deepwiki\",\n            server_url: \"https://mcp.deepwiki.com/mcp\",\n            require_approval: \"never\",\n        },\n    ],\n    input: \"What transport protocols are supported in the 2025-03-26 version of the MCP spec?\",\n});\n\nconsole.log(resp.output_text);\n</code></pre> <pre><code>from openai import OpenAI\n\nclient = OpenAI()\n\nresp = client.responses.create(\n    model=\"gpt-4.1\",\n    tools=[\n        {\n            \"type\": \"mcp\",\n            \"server_label\": \"deepwiki\",\n            \"server_url\": \"https://mcp.deepwiki.com/mcp\",\n            \"require_approval\": \"never\",\n        },\n    ],\n    input=\"What transport protocols are supported in the 2025-03-26 version of the MCP spec?\",\n)\n\nprint(resp.output_text)\n</code></pre> <p>It is very important that developers trust any remote MCP server they use with the Responses API. A malicious server can exfiltrate sensitive data from anything that enters the model's context. Carefully review the Risks and Safety section below before using this tool.</p>"},{"location":"mcp%20docs/remote_mcp/#the-mcp-ecosystem","title":"The MCP ecosystem","text":"<p>We are still in the early days of the MCP ecosystem. Some popular remote MCP servers today include Cloudflare, Hubspot, Intercom, Paypal, Pipedream, Plaid, Shopify, Stripe, Square, Twilio and Zapier. We expect many more servers\u2014and registries making it easy to discover these servers\u2014to launch in the coming months. The MCP protocol itself is also early, and we expect to add many more updates to our MCP tool as the protocol evolves.</p>"},{"location":"mcp%20docs/remote_mcp/#how-it-works","title":"How it works","text":"<p>The MCP tool works only in the Responses API, and is available across all our new models (gpt-4o, gpt-4.1, and our reasoning models). When you're using the MCP tool, you only pay for tokens used when importing tool definitions or making tool calls\u2014there are no additional fees involved.</p>"},{"location":"mcp%20docs/remote_mcp/#step-1-getting-the-list-of-tools-from-the-mcp-server","title":"Step 1: Getting the list of tools from the MCP server","text":"<p>The first thing the Responses API does when you attach a remote MCP server to the <code>tools</code> array, is attempt to get a list of tools from the server. The Responses API supports remote MCP servers that support either the Streamable HTTP or the HTTP/SSE transport protocol.</p> <p>If successful in retrieving the list of tools, a new <code>mcp_list_tools</code> output item will be visible in the Response object that is created for each MCP server. The <code>tools</code> property of this object will show the tools that were successfully imported.</p> <pre><code>{\n  \"id\": \"mcpl_682d4379df088191886b70f4ec39f90403937d5f622d7a90\",\n  \"type\": \"mcp_list_tools\",\n  \"server_label\": \"deepwiki\",\n  \"tools\": [\n    {\n      \"name\": \"read_wiki_structure\",\n      \"input_schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"repoName\": {\n            \"type\": \"string\",\n            \"description\": \"GitHub repository: owner/repo (e.g. \\\"facebook/react\\\")\"\n          }\n        },\n        \"required\": [\n          \"repoName\"\n        ],\n        \"additionalProperties\": false,\n        \"annotations\": null,\n        \"description\": \"\",\n        \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n      }\n    },\n    // ... other tools\n  ]\n}\n</code></pre> <p>As long as the <code>mcp_list_tools</code> item is present in the context of the model, we will not attempt to pull a refreshed list of tools from an MCP server. We recommend you keep this item in the model's context as part of every conversation or workflow execution to optimize for latency.</p>"},{"location":"mcp%20docs/remote_mcp/#filtering-tools","title":"Filtering tools","text":"<p>Some MCP servers can have dozens of tools, and exposing many tools to the model can result in high cost and latency. If you're only interested in a subset of tools an MCP server exposes, you can use the <code>allowed_tools</code> parameter to only import those tools.</p> <p>Constrain allowed tools</p> <pre><code>curl https://api.openai.com/v1/responses \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n  \"model\": \"gpt-4.1\",\n  \"tools\": [\n    {\n      \"type\": \"mcp\",\n      \"server_label\": \"deepwiki\",\n      \"server_url\": \"https://mcp.deepwiki.com/mcp\",\n      \"require_approval\": \"never\",\n      \"allowed_tools\": [\"ask_question\"]\n    }\n  ],\n  \"input\": \"What transport protocols does the 2025-03-26 version of the MCP spec (modelcontextprotocol/modelcontextprotocol) support?\"\n}'\n</code></pre> <pre><code>import OpenAI from \"openai\";\nconst client = new OpenAI();\n\nconst resp = await client.responses.create({\n    model: \"gpt-4.1\",\n    tools: [{\n        type: \"mcp\",\n        server_label: \"deepwiki\",\n        server_url: \"https://mcp.deepwiki.com/mcp\",\n        require_approval: \"never\",\n        allowed_tools: [\"ask_question\"],\n    }],\n    input: \"What transport protocols does the 2025-03-26 version of the MCP spec (modelcontextprotocol/modelcontextprotocol) support?\",\n});\n\nconsole.log(resp.output_text);\n</code></pre> <pre><code>from openai import OpenAI\n\nclient = OpenAI()\n\nresp = client.responses.create(\n    model=\"gpt-4.1\",\n    tools=[{\n        \"type\": \"mcp\",\n        \"server_label\": \"deepwiki\",\n        \"server_url\": \"https://mcp.deepwiki.com/mcp\",\n        \"require_approval\": \"never\",\n        \"allowed_tools\": [\"ask_question\"],\n    }],\n    input=\"What transport protocols does the 2025-03-26 version of the MCP spec (modelcontextprotocol/modelcontextprotocol) support?\",\n)\n\nprint(resp.output_text)\n</code></pre>"},{"location":"mcp%20docs/remote_mcp/#step-2-calling-tools","title":"Step 2: Calling tools","text":"<p>Once the model has access to these tool definitions, it may choose to call them depending on what's in the model's context. When the model decides to call an MCP tool, we make an request to the remote MCP server to call the tool, take it's output and put that into the model's context. This creates an <code>mcp_call</code> item which looks like this:</p> <pre><code>{\n  \"id\": \"mcp_682d437d90a88191bf88cd03aae0c3e503937d5f622d7a90\",\n  \"type\": \"mcp_call\",\n  \"approval_request_id\": null,\n  \"arguments\": \"{\\\"repoName\\\":\\\"modelcontextprotocol/modelcontextprotocol\\\",\\\"question\\\":\\\"What transport protocols does the 2025-03-26 version of the MCP spec support?\\\"}\",\n  \"error\": null,\n  \"name\": \"ask_question\",\n  \"output\": \"The 2025-03-26 version of the Model Context Protocol (MCP) specification supports two standard transport mechanisms: `stdio` and `Streamable HTTP` ...\",\n  \"server_label\": \"deepwiki\"\n}\n</code></pre> <p>As you can see, this includes both the arguments the model decided to use for this tool call, and the <code>output</code> that the remote MCP server returned. All models can choose to make multiple (MCP) tool calls in the Responses API, and so, you may see several of these items generated in a single Response API request.</p> <p>Failed tool calls will populate the error field of this item with MCP protocol errors, MCP tool execution errors, or general connectivity errors. The MCP errors are documented in the MCP spec here.</p>"},{"location":"mcp%20docs/remote_mcp/#approvals","title":"Approvals","text":"<p>By default, OpenAI will request your approval before any data is shared with a remote MCP server. Approvals help you maintain control and visibility over what data is being sent to an MCP server. We highly recommend that you carefully review (and optionally, log) all data being shared with a remote MCP server. A request for an approval to make an MCP tool call creates a <code>mcp_approval_request</code> item in the Response's output that looks like this:</p> <pre><code>{\n  \"id\": \"mcpr_682d498e3bd4819196a0ce1664f8e77b04ad1e533afccbfa\",\n  \"type\": \"mcp_approval_request\",\n  \"arguments\": \"{\\\"repoName\\\":\\\"modelcontextprotocol/modelcontextprotocol\\\",\\\"question\\\":\\\"What transport protocols are supported in the 2025-03-26 version of the MCP spec?\\\"}\",\n  \"name\": \"ask_question\",\n  \"server_label\": \"deepwiki\"\n}\n</code></pre> <p>You can then respond to this by creating a new Response object and appending an <code>mcp_approval_response</code> item to it.</p> <p>Approving the use of tools in an API request</p> <pre><code>curl https://api.openai.com/v1/responses \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n  \"model\": \"gpt-4.1\",\n  \"tools\": [\n    {\n      \"type\": \"mcp\",\n      \"server_label\": \"deepwiki\",\n      \"server_url\": \"https://mcp.deepwiki.com/mcp\"\n    }\n  ],\n  \"previous_response_id\": \"resp_682d498bdefc81918b4a6aa477bfafd904ad1e533afccbfa\",\n  \"input\": [{\n    \"type\": \"mcp_approval_response\",\n    \"approve\": true,\n    \"approval_request_id\": \"mcpr_682d498e3bd4819196a0ce1664f8e77b04ad1e533afccbfa\"\n  }]\n}'\n</code></pre> <pre><code>import OpenAI from \"openai\";\nconst client = new OpenAI();\n\nconst resp = await client.responses.create({\n    model: \"gpt-4.1\",\n    tools: [{\n        type: \"mcp\",\n        server_label: \"deepwiki\",\n        server_url: \"https://mcp.deepwiki.com/mcp\",\n    }],\n    previous_response_id: \"resp_682d498bdefc81918b4a6aa477bfafd904ad1e533afccbfa\",\n    input: [{\n        type: \"mcp_approval_response\",\n        approve: true,\n        approval_request_id: \"mcpr_682d498e3bd4819196a0ce1664f8e77b04ad1e533afccbfa\"\n    }],\n});\n\nconsole.log(resp.output_text);\n</code></pre> <pre><code>from openai import OpenAI\n\nclient = OpenAI()\n\nresp = client.responses.create(\n    model=\"gpt-4.1\",\n    tools=[{\n        \"type\": \"mcp\",\n        \"server_label\": \"deepwiki\",\n        \"server_url\": \"https://mcp.deepwiki.com/mcp\",\n    }],\n    previous_response_id=\"resp_682d498bdefc81918b4a6aa477bfafd904ad1e533afccbfa\",\n    input=[{\n        \"type\": \"mcp_approval_response\",\n        \"approve\": True,\n        \"approval_request_id\": \"mcpr_682d498e3bd4819196a0ce1664f8e77b04ad1e533afccbfa\"\n    }],\n)\n\nprint(resp.output_text)\n</code></pre> <p>Here we're using the <code>previous_response_id</code> parameter to chain this new Response, with the previous Response that generated the approval request. But you can also pass back the outputs from one response, as inputs into another for maximum control over what enter's the model's context.</p> <p>If and when you feel comfortable trusting a remote MCP server, you can choose to skip the approvals for reduced latency. To do this, you can set the <code>require_approval</code> parameter of the MCP tool to an object listing just the tools you'd like to skip approvals for like shown below, or set it to the value <code>'never'</code> to skip approvals for all tools in that remote MCP server.</p> <p>Never require approval for some tools</p> <pre><code>curl https://api.openai.com/v1/responses \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n  \"model\": \"gpt-4.1\",\n  \"tools\": [\n    {\n      \"type\": \"mcp\",\n      \"server_label\": \"deepwiki\",\n      \"server_url\": \"https://mcp.deepwiki.com/mcp\",\n      \"require_approval\": {\n          \"never\": {\n            \"tool_names\": [\"ask_question\", \"read_wiki_structure\"]\n          }\n      }\n    }\n  ],\n  \"input\": \"What transport protocols does the 2025-03-26 version of the MCP spec (modelcontextprotocol/modelcontextprotocol) support?\"\n}'\n</code></pre> <pre><code>import OpenAI from \"openai\";\nconst client = new OpenAI();\n\nconst resp = await client.responses.create({\n    model: \"gpt-4.1\",\n    tools: [\n        {\n            type: \"mcp\",\n            server_label: \"deepwiki\",\n            server_url: \"https://mcp.deepwiki.com/mcp\",\n            require_approval: {\n                never: {\n                    tool_names: [\"ask_question\", \"read_wiki_structure\"]\n                }\n            }\n        },\n    ],\n    input: \"What transport protocols does the 2025-03-26 version of the MCP spec (modelcontextprotocol/modelcontextprotocol) support?\",\n});\n\nconsole.log(resp.output_text);\n</code></pre> <pre><code>from openai import OpenAI\n\nclient = OpenAI()\n\nresp = client.responses.create(\n    model=\"gpt-4.1\",\n    tools=[\n        {\n            \"type\": \"mcp\",\n            \"server_label\": \"deepwiki\",\n            \"server_url\": \"https://mcp.deepwiki.com/mcp\",\n            \"require_approval\": {\n                \"never\": {\n                    \"tool_names\": [\"ask_question\", \"read_wiki_structure\"]\n                }\n            }\n        },\n    ],\n    input=\"What transport protocols does the 2025-03-26 version of the MCP spec (modelcontextprotocol/modelcontextprotocol) support?\",\n)\n\nprint(resp.output_text)\n</code></pre>"},{"location":"mcp%20docs/remote_mcp/#authentication","title":"Authentication","text":"<p>Unlike the DeepWiki MCP server, most other MCP servers require authentication. The MCP tool in the Responses API gives you the ability to flexibly specify headers that should be included in any request made to a remote MCP server. These headers can be used to share API keys, oAuth access tokens, or any other authentication scheme the remote MCP server implements.</p> <p>The most common header used by remote MCP servers is the <code>Authorization</code> header. This is what passing this header looks like:</p> <p>Use Stripe MCP tool</p> <pre><code>curl https://api.openai.com/v1/responses \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n  \"model\": \"gpt-4.1\",\n  \"input\": \"Create a payment link for $20\",\n  \"tools\": [\n    {\n      \"type\": \"mcp\",\n      \"server_label\": \"stripe\",\n      \"server_url\": \"https://mcp.stripe.com\",\n      \"headers\": {\n        \"Authorization\": \"Bearer $STRIPE_API_KEY\"\n      }\n    }\n  ]\n}'\n</code></pre> <pre><code>import OpenAI from \"openai\";\nconst client = new OpenAI();\n\nconst resp = await client.responses.create({\n    model: \"gpt-4.1\",\n    input: \"Create a payment link for $20\",\n    tools: [\n        {\n            type: \"mcp\",\n            server_label: \"stripe\",\n            server_url: \"https://mcp.stripe.com\",\n            headers: {\n                Authorization: \"Bearer $STRIPE_API_KEY\"\n            }\n        }\n    ]\n});\n\nconsole.log(resp.output_text);\n</code></pre> <pre><code>from openai import OpenAI\n\nclient = OpenAI()\n\nresp = client.responses.create(\n    model=\"gpt-4.1\",\n    input=\"Create a payment link for $20\",\n    tools=[\n        {\n            \"type\": \"mcp\",\n            \"server_label\": \"stripe\",\n            \"server_url\": \"https://mcp.stripe.com\",\n            \"headers\": {\n                \"Authorization\": \"Bearer $STRIPE_API_KEY\"\n            }\n        }\n    ]\n)\n\nprint(resp.output_text)\n</code></pre> <p>To prevent the leakage of sensitive keys, the Responses API does not store the values of any string you provide in the <code>headers</code> object. These values will also not be visible in the Response object created. Additionally, because some remote MCP servers generate authenticated URLs, we also discard the path portion of the <code>server_url</code> in our responses (i.e. <code>example.com/mcp</code> becomes <code>example.com</code>). Because of this, you must send the full path of the MCP <code>server_url</code> and any relevant <code>headers</code> in every Responses API creation request you make.</p>"},{"location":"mcp%20docs/remote_mcp/#risks-and-safety","title":"Risks and safety","text":"<p>The MCP tool permits you to connect OpenAI to services that have not been verified by OpenAI and allows OpenAI to access, send and receive data, and take action in these services. All MCP servers are third-party services that are subject to their own terms and conditions.</p> <p>If you come across a malicious MCP server, please report it to <code>security@openai.com</code>.</p>"},{"location":"mcp%20docs/remote_mcp/#connecting-to-trusted-servers","title":"Connecting to trusted servers","text":"<p>Pick official servers hosted by the service providers themselves (e.g. we recommend connecting to the Stripe server hosted by Stripe themselves on mcp.stripe.com, instead of a Stripe MCP server hosted by a third party). Because there aren't too many official remote MCP servers today, you may be tempted to use a MCP server hosted by an organization that doesn't operate that server and simply proxies request to that service via your API. If you must do this, be extra careful in doing your due diligence on these \"aggregators\", and carefully review how they use your data.</p>"},{"location":"mcp%20docs/remote_mcp/#log-and-review-data-being-shared-with-third-party-mcp-servers","title":"Log and review data being shared with third party MCP servers.","text":"<p>Because MCP servers define their own tool definitions, they may request for data that you may not always be comfortable sharing with the host of that MCP server. Because of this, the MCP tool in the Responses API defaults to requiring approvals of each MCP tool call being made. When developing your application, review the type of data being shared with these MCP servers carefully and robustly. Once you gain confidence in your trust of this MCP server, you can skip these approvals for more performant execution.</p> <p>We also recommend logging any data sent to MCP servers. If you're using the Responses API with <code>store=true</code>, these data are already logged via the API for 30 days unless Zero Data Retention is enabled for your organization. You may also want to log these data in your own systems and perform periodic reviews on this to ensure data is being shared per your expectations.</p> <p>Malicious MCP servers may include hidden instructions (prompt injections) designed to make OpenAI models behave unexpectedly. While OpenAI has implemented built-in safeguards to help detect and block these threats, it's essential to carefully review inputs and outputs, and ensure connections are established only with trusted servers.</p> <p>MCP servers may update tool behavior unexpectedly, potentially leading to unintended or malicious behavior.</p>"},{"location":"mcp%20docs/remote_mcp/#implications-on-zero-data-retention-and-data-residency","title":"Implications on Zero Data Retention and Data Residency","text":"<p>The MCP tool is compatible with Zero Data Retention and Data Residency, but it's important to note that MCP servers are third-party services, and data sent to an MCP server is subject to their data retention and data residency policies.</p> <p>In other words, if you're an organization with Data Residency in Europe, OpenAI will limit inference and storage of Customer Content to take place in Europe up until the point communication or data is sent to the MCP server. It is your responsibility to ensure that the MCP server also adheres to any Zero Data Retention or Data Residency requirements you may have. Learn more about Zero Data Retention and Data Residency here.</p>"},{"location":"mcp%20docs/remote_mcp/#usage-notes","title":"Usage notes","text":"<p>|| |ResponsesChat CompletionsAssistants|Tier 1200 RPMTier 2 and 31000 RPMTier 4 and 52000 RPM|PricingZDR and data residency|</p>"}]}